A Statistical Framework of Watermarks for Large Language
Models: Pivot, Detection Efficiency and Optimal Rules
Xiang Liâˆ—
Feng Ruanâ€ 
Huiyuan Wangâ€¡
Qi LongÂ§
Weijie J. SuÂ¶
March 28, 2024
Abstract
Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical
signals into text generated by large language models (LLMs), also known as watermarking, has
been used as a principled approach to provable detection of LLM-generated text from its
human-written counterpart. In this paper, we introduce a general and flexible framework for
reasoning about the statistical efficiency of watermarks and designing powerful detection rules.
Inspired by the hypothesis testing formulation of watermark detection, our framework starts
by selecting a pivotal statistic of the text and a secret keyâ€”provided by the LLM to the
verifierâ€”to control the false positive rate (the error of mistakenly detecting human-written
text as LLM-generated). Next, this framework allows one to evaluate the power of watermark
detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the
error of incorrectly classifying LLM-generated text as human-written). Our framework further
reduces the problem of determining the optimal detection rule to solving a minimax optimization
program. We apply this framework to two representative watermarksâ€”one of which has been
internally implemented at OpenAIâ€”and obtain several findings that can be instrumental in
guiding the practice of implementing watermarks. In particular, we derive optimal detection
rules for these watermarks under our framework. These theoretically derived detection rules are
demonstrated to be competitive and sometimes enjoy a higher power than existing detection
approaches through numerical experiments.
âˆ—University of Pennsylvania; Email: lx10077@upenn.edu.
â€ Northwestern University; Email: fengruan@northwestern.edu.
â€¡University of Pennsylvania; Email: huiyuanw@upenn.edu.
Â§University of Pennsylvania; Email: qlong@upenn.edu.
Â¶University of Pennsylvania; Email: suw@wharton.upenn.edu.
1
arXiv:2404.01245v2  [math.ST]  28 Aug 2024

1
Introduction
Large language models (LLMs) have emerged in recent years as a disruptive technology to generate
human-like text and other media [62, 46, 2]. While this reality enhances productivity in many sectors,
the mismatch between ownership and generation of content could lead to several unwanted outcomes:
Exacerbating misinformation. The ability of LLMs to generate a large amount of text in
parallel can be easily leveraged to exacerbate the spread of misinformation [73, 69], which
can be enabled by deploying automated bots on social media platforms [58]. It may also
facilitate fraud and deception by pretending to be humans interacting with their relatives and
acquaintances.
Facade of AI-assisted education. LLMs impose challenges to education because students
may use powerful LLMs to write essays for themselves [60, 43]. This deprives students of
opportunities to practice their own skills and creates inequalities among students depending
on the capabilities of the LLMs they access.
Data pollution. The internet will soon consist of more LLM-generated text than human-written
text. If LLM-generated text is indiscriminately mixed with human-written text for training, it
becomes difficult to create high-quality data for developing next-generation LLMs [50, 56, 17].
An initial effort to mitigate these problems has been to leverage specific patterns of LLM-
generated text to distinguish it from human-generated text [25, 74, 44]. However, this approach has
become increasingly ineffective as models such as ChatGPT-4, Claude 3, Gemini 1.5 Pro, and many
others have reached a level that makes it significantly difficult, if not impossible, to distinguish their
generated text from human-written text [68].
A more viable approach is to watermark text by embedding a signal into the LLM-generated text
in a manner that allows the watermark to be provably detected [15, 1]. The property of provable
detection is crucial because it allows the verifier to identify LLM-generated text for malicious purposes
without relying on assumptions about the text, which may not always hold. Moreover, a reasonable
watermarking scheme should be approximately unbiased, meaning it does not significantly distort
the meaning or style of the original text.
The necessity of watermarking LLM-generated text was highlighted in the Biden administrationâ€™s
October 2023 executive order, which incorporated proposals for watermarking LLM-generated text
and other AI-generated content [10].
As part of the executive order, the U.S. Department of
Commerce will help develop standards to watermark LLM-generated content. Accordingly, OpenAI,
Google, Meta, and other tech giants have pledged to develop watermarking systems [9].
This reality has made it imperative for researchers to develop watermarking methods for LLM-
generated text. Within a year since 2023, numerous watermarks have been proposed [36, 22, 38,
30, 70, 78, 77, 41, 24]. A common feature of these methods is leveraging the probabilistic nature of
LLMs. In essence, these methods incorporate pseudorandomness into the text-generation process of
LLMs, and the coupling between the LLM-generated text and the pseudorandomness serves as a
signal that can be used for detecting the watermark. The constructed signal becomes pronounced
for detection only when the pseudorandom numbers are known, making it practically difficult for
one to remove the watermark without access to the pseudorandom numbers [15].
To understand how watermarks work for LLMs in more detail, we must first introduce the concept
of â€œtokenizationâ€ in LLM text processing. Tokenization involves breaking down the text into smaller
2

units called tokens, which are informally known as sub-words. These tokens can be words, parts of
words, or even punctuation marks. For example, the sentence â€œHello, world!â€ when tokenized, might
be split into four tokens: [â€œHelloâ€, â€œ,â€, â€œ worldâ€, â€œ!â€]1. An LLM generates each token sequentially by
sampling from a probability distribution conditioned on prior tokens, among other things. Typically,
the size of the token vocabulary is of the order of 104 and varies with language models. Letting
W denote the vocabulary of all tokens, for example, |W| = 50, 272 for the OPT-1.3B model [76],
|W| = 32, 000 for the LLaMA series models [62], and |W| = 50, 257 for GPT-2 and GPT-3.5 series
models [51, 11].
After generating text in the form of a token sequence, w1w2 Â· Â· Â· wtâˆ’1, the (unwatermarked)
LLM generates the next token wt according to a multinomial distribution Pt := (Pt,w)wâˆˆW on the
vocabulary W, satisfying P
wâˆˆW Pt,w = 1.2 We call Pt the next-token prediction (NTP) distribution
at step t, and it depends on all prior generated tokens, the user-supplied prompt, as well as system
prompts that are hidden from users [65, 51, 11].
In contrast, a watermarked LLM generates the next token that is jointly determined by a
pseudorandom variable and the NTP distribution. Let Î¶t denote the pseudorandom variable at step
t, which is available only to the verifier. Formally, the watermarked LLM samples a token according
to the rule [30, 70]
wt := S(Pt, Î¶t),
where S is a (deterministic) decoder function. To achieve approximate unbiasedness, we require
that the probability distribution of wt â‰¡S(Pt, Î¶t) over the randomness3 embodied in Î¶t is close to
Pt, conditional on all previous tokens, w1 Â· Â· Â· wtâˆ’1 [30, 38]. In this regard, an unbiased watermark
roughly corresponds to a sampling method from multinomial distributions.
With the generated text now indistinguishable from that of the unwatermarked LLM, it seems
at first glance hopeless to obtain provable detectability for watermarked text [54, 75]. Interestingly,
detection can be made possible by carefully designing the decoder S to impose a coupling relationship
between the token and pseudorandom variable, even without knowing the NTP predictions [15].4 In
the following, we elaborate on this point by considering perhaps the simplest example of a watermark
that achieves both unbiasedness and provable detectability.
A baby watermark
Envision an LLM that involves only two tokens, 0 and 1â€”that is, W = {0, 1}.
Let Pt = (Pt,0, Pt,1) denote the NTP distribution at step t, and let Î¶t be i.i.d. copies of the standard
uniform random variable U(0, 1). Set the decoder as follows:
S(P , Î¶) =
(
0
if Î¶ â‰¤P0
1
otherwise.
(1)
1Refer to the website https://platform.openai.com/tokenizer for user-customized examples of tokenization.
2As a convention, throughout this paper three subscripts appear: lettert, letterw, and lettert,w, which correspond
to step t, token w, and token w at step t, respectively.
3Strictly speaking, Î¶t has no randomness. However, modern cryptographic theories ensure that Î¶t behaves very
much like a random variable. See more details in Section 2.
4We cannot use the probabilities Ptâ€™s as the verifier in general does not have access to the LLM that generates the
tokens. Moreover, the prompt for generating the text is unavailable to the verifier, hence the verifier cannot obtain Pt
even having access to the LLM.
3

This watermark is unbiased. Intuitively, if Î¶t is large, then wt is more likely to be 1 instead of 0, and
vice versa. This intuition suggests using the following statistic for detecting the watermark:
n
X
t=1
(2wt âˆ’1)(2Î¶t âˆ’1),
(2)
which measures the correlation between the tokens and pseudorandom variables. When the watermark
is present, this statistic tends to be larger in distribution than when the watermark is absent. One
can conclude that a watermark is detected if this statistic is above a certain threshold. Despite
being intuitive, however, this statistic is ad-hoc, and one cannot rule out the possibility of a better
detection rule.
To better understand practical watermarking schemes, we turn to real-world LLMs where the
size of the token vocabulary is very large. As a recap, at the core of an unbiased watermark is a
sampling method for multinomial distributions. Perhaps the two most common sampling methods
are the Gumbel-max trick [42, 32] and the inverse transform [19]. Interestingly, these two sampling
methods correspond precisely to two important and representative watermarks [38, 1].
Gumbel-max watermark [1]
Let Î¶ = (Uw)wâˆˆW consist of |W| i.i.d. copies of U(0, 1). A version
of the Gumbel-max trick [26] states that arg max
wâˆˆW
log Uw
Pw
follows the NTP distribution P â‰¡(Pw)wâˆˆW.5
Recognizing this fact, Scott Aaronson proposed the following decoder [1]:
Sgum(Î¶, P ) := arg max
wâˆˆW
log Uw
Pw
,
(3)
which is, by definition, unbiased [22, 49, 78]. This watermark has recently been implemented
internally at OpenAI [1].
Aaronson suggested declaring the presence of the watermark if the
following statistic is above a certain threshold: T ars
n
= âˆ’Pn
t=1 log(1 âˆ’Ut,wt). The intuition is that,
when the watermark is employed, (3) implies that a token wt is more likely to be selected when its
associated pseudorandom number Î¶t,wt is large. In contrast, when the text is written by a human,
Î¶t,wt would not be larger than other entries in the distribution at step t. Despite being intuitive, it
is worth mentioning that there are countless detection rules capable of capturing this distribution
shift. In particular, it is not clear whether this detection rule is optimal in any sense.
Inverse Transform Watermark [38]
It is well-known that any univariate distribution can be
sampled by applying the inverse cumulative distribution function (CDF) to U(0, 1). Given an NTP
distribution P and Ï€ that maps all tokens in W to a permutation of 1, 2, . . . , |W|, consider the
multinomial distribution with probability mass PÏ€âˆ’1(i) at i for 1 â‰¤i â‰¤|W|. The CDF of this
distribution takes the form F(x; Ï€) = P
wâ€²âˆˆW Pwâ€² Â· 1{Ï€(wâ€²)â‰¤x}. Taking as input U âˆ¼U(0, 1), the
generalized inverse of this CDF is defined as F âˆ’1(U; Ï€) = min{i : P
wâ€²âˆˆW Pwâ€² Â· 1{Ï€(wâ€²)â‰¤i} â‰¥U},
which by construction follows the multinomial distribution P after applying the permutation Ï€.
Making use of this fact, [38] proposed the inverse transform watermark with the following decoder:
Sinv(P , Î¶) := Ï€âˆ’1(F âˆ’1(U; Ï€)),
5It is noteworthy that the Gumbel-max trick has a broad range of applications, including partition function
estimation [28], ranking under the random utility model [57], and computational sampling in machine learning
[48, 42, 32].
4

where the pseudorandom variable Î¶ := (Ï€, U) and the permutation Ï€ is uniformly at random. By
definition, this watermark is unbiased. To detect the watermark, [38] proposed detection rules that,
roughly speaking, examine the absolute difference between Ut and the rank of the generated token,
Ï€t(wt), and sum the differences across the token sequence (see details in Section 4). When the text
is watermarked, the difference turns out to be small because the involved two random variables are
highly dependent. Similar to the Gumbel-max watermark, the detection of this watermark could
potentially benefit from a more principled derivation to enhance its power.
1.1
This Paper
From a statistical viewpoint, a watermark schemeâ€™s performance at the detection phase is evaluated by
two competing criteria. The first is how likely it would mistakenly detect human-written text as LLM-
generated text, and the second is the probability that it would mistakenly classify LLM-generated
text as its human-written counterpart. This perspective relates watermarks to hypothesis testing,
which formally calls the two aforementioned errors Type I error and Type II error, respectively. In
this regard, the effectiveness of the Gumbel-max watermark and inverse transform watermark, even
including the baby watermark, is not clear yet, though [22, 49] compared their efficiency through
empirical experiments. For example, even though they come with detection rules that seem intuitive,
it is not clear if they are statistically optimal in the sense of having the optimal trade-off between
Type I and Type II errors. If they are not optimal, it is of interest to find a better detection rule.
In general, we wish to have a general and flexible framework that can guide the development
of watermarks through optimizing its detection phase and assessing watermarks in a principled
manner. Specifically, the challenge lies in how we can provably control the Type I error for any given
watermark, considering that the NTP distributions, which are not accessible to the verifier, vary
from token to token. Once Type I error control is achieved, the next question is how to evaluate the
Type II error, preferably through a closed-form expression, which also hinges on the unknown NTP
distributions. Having known both the Type I and Type II errors, the final step involves comparing
watermark detection rules to ultimately identify the optimal detection rule based on our knowledge
of the LLM.
In this paper, we address these questions and challenges in a unified way by making the following
contributions.
A Statistical framework for watermarks. A major contribution of this paper is a general
statistical framework for developing statistically sound watermark detection rules through
precisely evaluating the Type I and Type II errors from the hypothesis testing viewpoint.
Under this framework, Type I error is controlled by leveraging a pivotal statistic that is
distributionally independent of the NTP distributions under the null. This framework is
accompanied by a technique for evaluating the asymptotic Type II error using large deviation
theory, and moreover, relies on the notion of class-dependent efficiency to tackle the challenge
of unknown and varying NTP distributions. Finally, this framework formulates the problem of
finding the most powerful detection rule as a minimax optimization program.
This framework is formally developed in Section 2.
Application to the Gumbel-max watermark. We apply our framework to Aaronsonâ€™s
Gumbel-max watermark in Section 3. Our main finding is that Aaronsonâ€™s detection rule
is suboptimal in the sense that its class-dependent efficiency is relatively low. Moreover, by
5

maximizing the class-dependent efficiency, we obtain the optimal detection rule, which admits
a simple analytical expression. This optimal detection rule is shown to outperform existing
rules in numerical experiments. Underlying these results is a technique that can reduce the
optimality problem to a convex geometry problem for the Gumbel-max watermark, which is a
contribution of independent interest to future research on watermarks.
Application to the inverse transform watermark. Next, we apply our framework to the
inverse transform watermark in Section 4. Our main finding is twofold. First, we overcome
a significant challenge in applying our framework to analyze this watermark by deriving
an asymptotic distribution when the text is watermarked. Second, we obtain the optimal
detection rule for the inverse transform watermark in a closed-form expression by maximizing
its class-dependent efficiency. Our numerical experiments corroborate its efficiency.
1.2
Related Work
The most closely related work to ours is the Gumbel-max watermark [1] and the inverse transform
watermark [38]. In [78, 22], the authors introduced unbiased watermarks that are robust to probability
perturbations or multi-bit processing. Other unbiased watermarks in this fast-growing line of research
include [15, 70, 30]. In addition, a popular example is the red-green list watermark, which splits the
vocabulary into red-green lists based on hash values of previous n-grams and slightly increases the
probability of green tokens embedding the watermark [35, 36, 77, 41, 12]. In the detection phase,
a high frequency of green tokens suggests the text is LLM-generated. This type of watermark is
biased since the NTP distributions have been altered, thereby leading to performance degradation of
the LLM.
In contrast, there is much less work on the theoretical front. A notable exception is the work
of [31], which approached watermark detection from the perspective of composite independence
testing. Their framework requires model providers to offer random rejection regions. In contrast,
our approach employs a pivotal statistic to detect distributional shifts between human-written and
LLM-generated text, allowing verifiers to choose any rejection region. While both approaches aim to
minimize a form of worst-case Type II error, the optimal detection rule proposed by [31] is hindered
by a non-vanishing Type II error due to the consideration of many worst-case scenarios, and it is
computationally inefficient because of the exponentially large rejection regions required.6 Conversely,
our optimal rule generally achieves a vanishing Type II error by addressing fewer worst-case scenarios,
benefiting from the well-regularized null behavior of pivotal statistics, and is computationally efficient
due to its sum-based structure.
Research on watermarking text has been conducted well before the advent of modern language
models. This body of research focuses on watermarking text by modifying it to introduce specific
patterns that are unlikely to be noticeable to readers. This includes synonym substitution [61],
syntactic restructuring [5], and linguistic steganography [16]. A common weakness of these approaches
lies in their biasedness and vulnerability to attacks that aim to remove watermarks [13, 79].
In a different direction, many methods have been proposed to detect text generated by LLMsâ€”
often not watermarkedâ€”from human-written counterparts. A common feature of these methods
is to examine the complete context, linguistic patterns, and other potentially revealing markers
in the given text to assess whether it is likely LLM-generated. The simplest method is to build
a classifier using synthetic and human text data, which is adopted by some commercial detection
6See Theorem 3.10 of [31] and the subsequent discussion.
6

platforms [25, 74]. Another category is training-free and leverages the inherent stylistic differences
between human and machine writing without specific training data, using techniques such as log
probability curvature [44, 7], divergent n-gram analysis [72], and intrinsic dimension estimation
[63]. However, [68] find that most post-hoc detection methods are neither accurate nor reliable and
suffer from a significant bias towards classifying the output as human-written rather than detecting
LLM-generated text. Furthermore, these methods have proven fragile to adversarial attacks and
biased against non-native English writers [37, 54, 39].
2
A Statistical Framework for Watermark Detection
In this section, we formally introduce our framework that enables statistical analysis of watermarks.
Our focus is on the development of effective techniques for assessing the statistical efficiency of the
watermarks accompanying this framework. Henceforth, in this paper, we write w1:n := w1 Â· Â· Â· wn
and Î¶1:n := Î¶1 Â· Â· Â· Î¶n for the text and associated pseudorandom variables, respectively.
2.1
Working Hypotheses
The problem of determining whether a watermark is present or not in the text can be formulated as
a hypothesis testing problem [31, 14, 35, 70, 24]:
H0 : w1:n is written by a human
H1 : w1:n is written by a watermarked LLM.
(4)
In addition to the text, this hypothesis testing problem also uses the pseudorandom variables as
data. A unified way to represent Î¶t based on existing constructions [70, 35, 36, 49, 22] is to take the
form
Î¶t = A(w1:(tâˆ’1), Key),
(5)
where Key denotes a secret key that will be passed to the verifier. The (deterministic) hash function
A maps any token sequence and a key to a pseudorandom number that will be used to generate the
next token. More precisely, the watermarked LLM generates the next token according to the rule
wt := S(Pt, Î¶t),
(6)
for some (deterministic) decoder S. Recall that P denotes the probability distribution of the next
token generated by the (unwatermarked) LLM.
With these notations in place, a watermark can be formally presented by the tuple (A, S, Key). To
lay a solid footing of watermarks on statistical hypothesis testing, we need two working hypotheses.
Working Hypothesis 2.1 (Soundness of pseudorandomness). In the watermarked LLM, the
pseudorandom variables Î¶1:n constructed above are i.i.d. copies of a random variable. Furthermore,
Î¶t is (statistically) independent of w1:(tâˆ’1).
Working Hypothesis 2.1 is grounded purely in cryptographic considerations. In cryptography,
there are well-established approaches to efficiently constructing and computing the pseudorandom
number as a function of text and the secret key [8, 47, 59, 55, 33]. The pseudorandom number
is very sensitive to the key, making it computationally indistinguishable from its truly random
counterpart without knowledge of the key.7 Specifically, although Î¶t is completely determined by the
7The phrase â€œcomputationally indistinguishableâ€ means that no polynomial-time algorithm can distinguish the
pseudorandom number from its truly random counterpart without knowledge of the key.
7

prior text w1:(tâˆ’1) (and the secret key), a run of the hash function A could effectively introduce fresh
randomness, thereby making Î¶t statistically independent of w1:(tâˆ’1). Note that, during detection, the
communication cost is merely to pass the secret key to the verifier as the prior tokens are publicly
available.
Hereafter, we regard Î¶ as a random variable, which allows us to formally define the unbiasedness
of a watermark. We say a watermark is unbiased if, for any multinomial distribution P , S(P , Î¶)
follows P . That is, for any NTP distribution P and token w âˆˆW,
P (S(P , Î¶) = w) = Pw,
where the expectation is taken over the randomness embodied in Î¶.
Together with the joint
independence of Î¶tâ€™s across the sequence of tokens, unbiasedness holds at every step conditional on
prior tokens.
Our next working hypothesis concerns the joint distribution of w1:n and Î¶1:n when the text is
written by a human.
Working Hypothesis 2.2 (Intrinsic nature of human randomness). Let w1:n be a sequence of
tokens generated by a human who has no knowledge of the secret key. Then, the human-generated
token wt and Î¶t are (statistically) independent conditional on (w1:(tâˆ’1), Î¶1:(tâˆ’1)), for all 1 â‰¤t â‰¤n.
Remark 2.1. To clear up any confusion, we remark that the verifier uses the human-generated tokens
w1, . . . , wn to compute the random variables Î¶1:n in (5). In particular, Working Hypothesis 2.1
remains valid for human-generated text, due to the construction of the hash function A.
In particular, this working hypothesis shows that, for human-written text, the token is not
generated according to (6). The rationale of this working hypothesis is that how a human writes
text is intrinsically random and cannot be captured by pseudorandomness. Therefore, the human-
generated token wt has nothing to do with Î¶t. Moreover, one can also argue for this working hypothesis
by recognizing that it is practically impossible for a human to generate text such that Î¶t and wt are
dependent because the secret key is not available. This is even the case for a different LLM without
having the secret key because it is computationally infeasible to replicate the pseudorandomness
without the key.
The two working hypotheses have been adopted by the literature, albeit not as explicit as our
treatment. For example, [70, 78] assumed conditions on the pseudorandom hash function so that the
first working hypothesis is valid.8 Other works directly impose distributional assumptions on some
summary statistics to effectively satisfy the two working hypotheses [35, 36, 77, 22]. As a departure
from these works, [38] considered a hash function of the form Î¶t = A(t, Key). Consequently, the
independence between the pseudorandom number and prior tokens follows by construction. However,
it is worthwhile mentioning that this form of the hash function hampers computational efficiency in
detection due to the necessity of iterating through the entire text sequence multiple times.
2.2
Pivotal Statistics
Understanding the hypothesis testing problem (4) requires analyzing the differences between the
joint distribution of (w1:n, Î¶1:n) under Working Hypotheses 2.1 and 2.2. This can be seen by first
8In particular, they assumed that A(w1:n, Key) is i.i.d. for any w1:n.
8

decomposing the joint probability density of (w1:n, Î¶1:n) into a product of conditional probabilities:9
P(w1:n, Î¶1:n) =
n
Y
t=1
P(wt, Î¶t | w1:(tâˆ’1), Î¶1:(tâˆ’1)).
(7)
â€¢ Under H0, Working Hypotheses 2.1 and 2.2 show that wt and Î¶t are independent given
(w1:(tâˆ’1), Î¶1:(tâˆ’1)). Hence,10
P(wt, Î¶t | w1:(tâˆ’1), Î¶1:(tâˆ’1)) = P(wt | w1:(tâˆ’1), Î¶1:(tâˆ’1))P(Î¶t | w1:(tâˆ’1), Î¶1:(tâˆ’1)) = Pt,wt Â· PÎ¶(Î¶t).
For example, the display above is equal to Pt,wt for the baby watermark defined in (1).
â€¢ Under H1, wt and Î¶t are dependent given (w1:(tâˆ’1), Î¶1:(tâˆ’1)). By Working Hypothesis 2.1 and
the decoding construction (6), we have
P(wt, Î¶t | w1:(tâˆ’1), Î¶1:(tâˆ’1)) =
(
PÎ¶(Î¶t)
if S(Pt, Î¶t) = wt
0
if S(Pt, Î¶t) Ì¸= wt
because the decoder S is deterministic. For example, the baby watermark satisfies P(wt, Î¶t |
w1:(tâˆ’1), Î¶1:(tâˆ’1)) = 1 if Î¶t > Pt,0, wt = 1, or Î¶t â‰¤Pt,0, wt = 0. It is 0 otherwise.
By the Neymanâ€“Pearson lemma, the most powerful test is based on the likelihood ratio:
PH0(w1:n, Î¶1:n)
PH1(w1:n, Î¶1:n) =
Qn
t=1 Pt,wt Â· PÎ¶(Î¶t)
Qn
t=1 1S(Pt,Î¶t)=wt Â· PÎ¶(Î¶t) =
(
P1,w1 Â· Â· Â· Pn,wn
if S(Pt, Î¶t) = wt for all t
âˆ
otherwise.
(8)
Although the likelihood ratio appears simple, taking only two values, we encounter a significant
challenge in distinguishing between the two cases mentioned above. This challenge arises because
the NTP distribution Pt is unknown and, worse, can vary with t. This nuisance parameter remains
unknown even if the verifier has complete access to the LLM, as the prompt used for generating the
text is usually not available to the verifier.
To address this challenge, we seek a pivotal statistic Yt = Y (wt, Î¶t) such that its distribution
is the same for any NTP distribution Pt under the null. Such a pivot allows us to construct test
statistics for watermark detection with known distributions and consequently obtain detection rules
with provable Type I error control, though at the price of information loss compared to using the
full data, wt and Î¶t. Formally, the original testing problem (4) is reduced to the following:
H0 : Yt âˆ¼Âµ0 i.i.d. for 1 â‰¤t â‰¤n
H1 : Yt âˆ¼Âµ1,Pt for 1 â‰¤t â‰¤n,
(9)
where Âµ0 denotes the (known) distribution of Yt when the text is human-written, and Âµ1,Pt denotes
the (unknown) distribution of Yt conditional on (w1:(tâˆ’1), Î¶1:(tâˆ’1)) when the text is generated by the
watermarked LLM. As is clear, Âµ1,Pt is determined by the NTP distribution Pt.
As a caveat, the choice of Y (wt, Î¶t) â‰¡Î¶t satisfies pivotality but is useless since the alternative
distribution is the same as the null distribution. A useful choice of Y , while being pivotal, should
9As an abuse of notation, P(Â·) is considered the probability density function when applied to a continuous variable,
and probability mass function when applied to a discrete variable.
10Here, we conceptualize a human as an LLM, using a multinomial distribution to model the selection of the next
token based on prior tokens.
9

allow the selected token to â€œpullâ€ the alternative distribution toward the same direction for any NTP
distribution Pt. For example, Y (wt, Î¶t) = (2wt âˆ’1)(2Î¶t âˆ’1) is a good choice for the baby watermark
since the dependence between wt and Î¶t (see (1) in Section 1) tends to make Y (wt, Î¶t) larger. In
general, it is a case-by-case approach to find a reasonable pivot for a given watermark (see Sections 3
and 4).
To test (9), it is natural to use the sum of Ytâ€™s across the token sequence as a test statistic. To
enhance the flexibility, we consider a score function h that applies to the pivot Y . This leads to the
following rejection rule for the hypothesis testing problem:
Th(Y1:n) :=
 1
if Pn
t=1 h(Yt) â‰¥Î³n,Î±
0
if Pn
t=1 h(Yt) < Î³n,Î±.
(10)
That is, we reject that the text is written by a human if Th is above Î³n,Î±. The threshold Î³n,Î± is
chosen to ensure significance level at Î±: PH0(Th(Y1:n) = 1) = Î±. For certain score functions discussed
in this work, the distribution of the sum of Ytâ€™s under the null hypothesis admits a closed-form
expression, allowing for analytical evaluation of Î³n,Î±.
In general, as the null distribution of h â—¦Y is
known, an estimator of Î³n,Î± when the text is sufficiently long is
Ë†Î³n,Î± = n Â· E0 h(Y ) + Î¦âˆ’1(1 âˆ’Î±) Â·
p
n Â· Var0(h(Y )),
where E0 and Var0 indicate that Âµ0 is used to take the expectation and variance. As can be seen,
the underlying NTP distributions Ptâ€™s are not involved in this detection rule.
2.3
Class-Dependent Efficiency
Once the Type I error is controlled, we seek to evaluate the Type II error and use it as a measure
to ascertain which choice of score function is more desired than others. If the NTP distributions
Pt were known and remained the same with respect to varying t, the optimal score function would
simply be given by the log-likelihood ratio, according to the Neymanâ€“Pearson lemma. However,
this is not the case. We manage this challenge by assuming that the NTP distributions belong to
a distribution class, denoted as P. The flexibility of using a distribution class lies in that one can
choose a small class when much is known about the distributional properties of the LLM, and choose
a large class if little is known.
Given a distribution class P, we can evaluate the Type II error of the test statistic Th(Y1:n) over
the least-favorable NTP distributions in P. This gives rise to a notion of class-dependent efficiency.
For any score function h and NTP distribution P , we define the following moment-generating
function (MGF):
Ï•P ,h(Î¸) := E1,P eâˆ’Î¸h(Y ),
(11)
where E1,P indicates that the expectation is taken over the randomness embodied in Y âˆ¼Âµ1,P in
(9). Assuming that the NTP distributions are all in P, the following result delineates the Type II
error in the large sample limit. We defer its proof to Appendix A, which relies on techniques in large
deviation theory [18, 64].
Theorem 2.1. Assume Pt âˆˆP for all t. For any h satisfying E0 |h| < âˆ, the Type II error of the
detection rule Th defined in (10) obeys
lim sup
nâ†’âˆPH1(Th(Y1:n) = 0)1/n â‰¤eâˆ’RP(h),
(12)
10

where RP(h) is given by
RP(h) = âˆ’inf
Î¸â‰¥0 sup
P âˆˆP
(Î¸ E0 h(Y ) + log Ï•P ,h(Î¸)) = âˆ’inf
Î¸â‰¥0

Î¸ E0 h(Y ) + sup
P âˆˆP
log Ï•P ,h(Î¸)

.
(13)
Remark 2.2. RP(h) is the P-dependent efficiency rate of h, or simply the P-efficiency rate.
Remark 2.3. Under a certain regularity condition, (12) is tight in the sense that there exists P â‹†
in P such that, if Pt = P â‹†for all t, then PH1(Th(Y1:n) = 0) â‰¥eâˆ’(RP(h)+Îµ)n for any positive Îµ
and sufficiently large n. The regularity condition is satisfied by both the Gumbel-max and inverse
transform watermarks and is detailed in the remark following the proof of Theorem 2.1 in Appendix
A.
Proposition 2.1 shows the Type II error decays exponentially as long as the exponential component
RP(h) is positive. The larger the value of RP(h) is, the more efficient the detection rule Th is. Put
differently, this class-dependent measure of efficiency reduces comparing watermark detection rules
to the rate of class-dependent efficiency.
This efficiency rate, obtained from large deviation theory, does not depend on the specific value of
Î±. The same is true for Bahadur efficiency [6], which quantifies the rate at which the significance level
(p-value) of a test statistic approaches zero with increasing sample size. It emphasizes the control of
Type I errors for simple hypothesis testing with i.i.d. data. In contrast, our metric RP(h) centers on
quantifying the least decline of Type II errors to zero when the underlying NTP distribution P is
picked up from the distribution class P. In short, it focuses on composite hypothesis testing where
the observed data is not identically distributed.
Optimality via minimax optimization
More importantly, the notion of class-dependent ef-
ficiency serves as a concrete approach to identifying the optimal score function that achieves the
largest possible value of P-efficiency rate. Following from (13), formally, this amounts to solving the
following optimization problem:
sup
h
RP(h) = âˆ’inf
h,Î¸â‰¥0 sup
P âˆˆP

E0 Î¸h(Y ) + log E1,P eâˆ’Î¸h(Y )
.
By viewing Î¸h as a new score function, finding the optimal h is reduced to solving the following
minimax optimization program:11
min
h max
P âˆˆP L(h, P ) where L(h, P ) := E0 h(Y ) + log E1,P eâˆ’h(Y ).
(14)
The function L(h, P ) is convex in the score function h for any fixed P , but is generally not concave
in P when h is fixed. Therefore, this minimax optimization problem is generally not convex-concave,
making it challenging to solve (14) numerically [40, 53].
Interestingly, we will show in Sections 3 and 4 that, for the Gumbel-max and inverse transform
watermarks, the minimax optimization problem possesses certain structural properties that enable
us to analytically identify global solutions to (14).
11To highly the minimax nature of this formulation, we use max and min in place of sup and inf, respectively.
11

0.0
0.2
0.4
0.6
0.8
1.0
maxwâˆˆW Pw
102
103
Figure 1: Empirical frequency of maxwâˆˆW Pt,w using outputs from ChatGPT-3.5-turbo. There are a
total of 4,997 top-one probabilities recorded, 81.36% of which are less than 0.999 while 99.79% are
larger than 0.001. See Appendix D.3 for the experimental setup.
Choice of distribution classes
The choice of the distribution class P is crucial since it follows
from (13) that RP1(h) â‰¥RP2(h) if P1 âŠ‚P2. While it might be plausible to assume a fraction
of NTP distributions are in a â€œniceâ€ class, some might be outside. The following result extends
Proposition 2.1 to this practical scenario.
Proposition 2.1. Assume that at least Î³-fraction of P1, . . . , Pn is in P1 with the rest being in P2,12
where 0 < Î³ < 1 and P1 âŠ‚P2. Then, for any h, we have
lim sup
nâ†’âˆPH1(Th(Y1:n) = 0)1/n â‰¤eâˆ’{Î³Â·RP1(h)+(1âˆ’Î³)Â·RP2(h)}.
When there is no prior at all about the LLMs, the associated class is the (|W| âˆ’1)-dimensional
simplex, which contains all possible NTP distributions and we denote by Simp(W). This distribution
class includes singular distributionsâ€”a distribution P such that Pw = 1 for some token wâ€”for
which the pivotal statistic has the same distribution under the null and alternative in (9). Formally,
this gives RSimp(W)(h) = 0, that is, the class-dependent efficiency of Simp(W) is zero. Taking
P1 = P âŠ‚P2 = Simp(W), Proposition 2.1 shows lim sup
nâ†’âˆPH1(Th(Y1:n) = 0)1/n â‰¤eâˆ’Î³RP(h). This
result implies that when only a fraction of NTP distributions is known to some extent, the Type II
error is still governed by RP(h) up to a multiplicative constant Î³. The comparison between choices
of the score functions h can still be reduced to comparing the value of RP(h).
In addition, the discussion above reveals that any detection rule would become powerless if
nothing is known about the NTP distributions. This justifies the necessity of imposing structural
12We do not need to know which are in the Î³ fraction.
12

assumptions on the distribution class. For 0 < âˆ†â‰¤1 âˆ’
1
|W|, we call
Pâˆ†:=
n
P : max
wâˆˆW Pw â‰¤1 âˆ’âˆ†
o
the âˆ†-regular distribution class. Accordingly, an NTP distribution P is called âˆ†-regular if P âˆˆ
Pâˆ†.
This excludes singular distributions.
Interestingly, âˆ†-regularity is closely related to the
Shannon entropy. For any âˆ†-regular NTP distribution P , its Shannon entropy satisfies Ent(P ) =
P Pw log
1
Pw â‰¥P Pw(1âˆ’Pw) â‰¥P Pw Â·âˆ†= âˆ†, where the first inequality follows because log
1
1âˆ’x â‰¥x
for x < 1. This shows that âˆ†-regularity imposes a lower bound on how much information the NTP
distribution offers. In practice, most NTP distributions are âˆ†-regular for a proper value of âˆ†. See
Figure 1 for an empirical investigation where 88.65% of the NTP distributions are 0.0001-regular,
81.36% are 0.001-regular, and 70.28% are 0.01-regular.
This distribution class offers a way to measure the performance of a detection rule using the
class-dependent efficiency in (13). For example, the Pâˆ†-efficiency rate of the baby watermark using
the statistic (2) is
âˆ’inf
Î¸â‰¥0 log
1
Î¸
eÎ¸(1âˆ’2âˆ†) + eâˆ’Î¸(1âˆ’2âˆ†)
2
âˆ’eâˆ’Î¸

.
A proof of this fact is deferred to Appendix B.8.
In practice, âˆ†-regularity is closely related to the temperature parameter in LLMs [3], which is
used to divide the raw output of a modelâ€™s last layer by a temperature parameter before applying the
softmax function. A high temperature leads to more uniform probabilities (encouraging exploration),
while a low temperature makes the distribution sharper, emphasizing the most likely outcomes
(encouraging exploitation).
3
Application to the Gumbel-max Watermark
In this section, we apply the framework to the Gumbel-max watermark [1]. Recall that the Gumbel-
max decoder can also be written as
wt = Sgum(Pt, Î¶t) := arg max
wâˆˆW V gum(Pt,w, Ut,w) where V gum(p, u) := log u
p
.
(15)
Above, {Î¶t}n
t=1 = {(Ut,w)wâˆˆW}n
t=1 represents nÃ—|W| i.i.d. replicates of the standard uniform random
variable U(0, 1). As seen from (15), the Gumbel-max trick ensures that this decoder is unbiased
for sampling from the NTP distribution Pt.
In implementation, Î¶t can be computed using a
pseudorandom hash function A that depends only on, for example, the last five tokens wtâˆ’5, . . . , wtâˆ’1
in addition to the secrete key [1].
Uniqueness of the Gumbel-max decoder
As a deviation from the main focus, we are tempted
to ask whether other forms of Gumbel-max-style decoders remain unbiased. Consider an unbiased
decoder that takes the form
SV (P , Î¶) := arg max
wâˆˆW V (Pw, Uw)
13

for some function V . When the vocabulary size |W| = 2, writing P = (P0, P1), unbiasedness
requires13
P(SV (P , Î¶) = 0) = P(V (P0, U0) â‰¥V (P1, U1)) =
P0
P0 + P1
.
(16)
When there are more than two tokens in the vocabulary, a natural requirement is that, by â€œgluingâ€
two tokens into a new token, the decoder remains unbiased for the new vocabulary that has been
reduced by one in size. This new token is sampled whenever either of the two original tokens is
sampled, which occurs with probability P0 + P1. This would be true if we have
max{V (P0, U0), V (P1, U1)} d= V (P0 + P1, U),
(17)
where d= denotes equality in distribution and U âˆ¼U(0, 1).
Both (16) and (17) are satisfied by Aaronsonâ€™s choice V gum for any P0 and P1 such that P0+P1 â‰¤1.
Interestingly, our following theorem shows that this choice is essentially unique. Its proof is deferred
to Appendix B.2.
Theorem 3.1. Suppose V satisfies both (16) and (17). SV is unbiased if and only if
V (p, u) = g
log u
p

â‰¡g (V gum(p, u))
for a strictly increasing function g.
3.1
Main Results
Optimal score function
Our framework starts by identifying a pivot for the hypothesis testing
problem (4). The random number Ut,wt, which corresponds to the selected token wt at step t, is
a good candidate. This is because Ut,wt follows the standard uniform distribution under H0 while
being stochastically larger under H1, as ensured by Working Hypotheses 2.1 and 2.2. Indeed, Ut,wt
is used in [1] for its detection rule.
We write Y gum
t
â‰¡Y gum(wt, Î¶t) := Ut,wt. This choice of pivot can be seen as an approximation
of the most â€œoptimisticâ€ log-likelihood ratio used in an independent hypothesis test for watermark
detection (see Appendix B.1 for details). The distribution of this pivot under H1 is a mixture of
Beta distributions, which is a classic result in the literature.
Lemma 3.1 ([49, 22]). Under H1, the distribution of Y gum
t
given Pt obeys
PH1(Y gum
t
â‰¤r | Pt) =
X
wâˆˆW
Pt,wr1/Pt,w
for r âˆˆ[0, 1].
The fact that this distribution is stochastically larger than U(0, 1) can be gleaned from the
following inequality: P
wâˆˆW Pt,wr1/Pt,w = r P
wâˆˆW Pt,wr1/Pt,wâˆ’1 â‰¤r P
wâˆˆW Pt,w = r. Also, see
Figure 2 for an illustration: there is a higher accumulation of probability mass around one under
H1 compared to the distribution observed under H0. From this observation, any score function h
aware of the distributional differences could be used for detecting the Gumbel-max watermarks. For
example, [1] uses hars(r) = âˆ’log(1 âˆ’r), while [38, 22] propose hlog(r) = log r. All of these score
13In (16), it evaluates to P0 since P0 + P1 = 1. But we regard (16) as an identity for any P0 and P1 such that
P0 + P1 â‰¤1.
14

0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
CDF with |W| = 15
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
PDF with |W| = 15
Pow(2, 1)
Pow(1, 1)
Pow(0.5, 1)
Uniform
Figure 2: Illustration of the distribution of Y gum
t
. We plot the CDF F1,Pt and the PDF fPt by
assuming Pt,w âˆ(w + b)âˆ’a, w âˆˆW with |W| = 15 and different choices of (a, b). The resulting
distribution is denoted by Pow(a, b). F1,Pt deviates from the uniform distribution.
functions are increasing functions that assign large values to points near one. While all these choices
control the Type I error, a crucial question is on what basis we compare score functions and which is
the optimal one.
To proceed under the framework, we consider the class-dependent efficiency with distribution
class Pâˆ†for âˆ†âˆˆ[0, 1 âˆ’|W|âˆ’1]. As one of the main findings of this paper, we obtain the score
function with the highest Pâˆ†-efficiency rate. Its proof constitutes the subject of Section 3.2. Below,
âŒŠxâŒ‹denotes the greatest integer that is less than or equal to x.
Theorem 3.2. Let hâ‹†
gum,âˆ†: [0, 1] â†’R be defined as
hâ‹†
gum,âˆ†(r) = log

1
1 âˆ’âˆ†

r
âˆ†
1âˆ’âˆ†+ r
e
âˆ†
1âˆ’e
âˆ†

with
eâˆ†= (1 âˆ’âˆ†)

1
1 âˆ’âˆ†

.
(18)
This function gives the optimal Pâˆ†-efficiency rates in the sense that RPâˆ†(hâ‹†
gum,âˆ†) â‰¥RPâˆ†(h) for
any measurable function h. Moreover, RPâˆ†(hâ‹†
gum,âˆ†) is attained at the following least-favorable NTP
distribution in Pâˆ†:
P â‹†
âˆ†=

1 âˆ’âˆ†, . . . , 1 âˆ’âˆ†
|
{z
}
âŒŠ
1
1âˆ’âˆ†âŒ‹times
, 1 âˆ’(1 âˆ’âˆ†) Â·

1
1 âˆ’âˆ†

, 0, . . .

.
(19)
Remark 3.1. To clear up any confusion, the second conclusion amounts to saying that (hâ‹†
gum,âˆ†, P â‹†
âˆ†)
is a saddle point solution to the minimax problem (14). Explicitly, P maximizes L(hâ‹†
gum,âˆ†, P ) in
(14) if and only if P results from rearranging P â‹†
âˆ†â€™s coordinates.
The significance of Theorem 3.2 lies partly in the closed-form nature of the optimal score function,
thereby making it easy to use in detecting LLM-generated text. The left panel of Figure 3 shows
hâ‹†
gum,âˆ†for some values of âˆ†. By the Neymanâ€“Pearson lemma, the optimal score function and least-
favorable distribution are related via hâ‹†
gum,âˆ†= log
 dÂµ1,P /dÂµ0

P =P â‹†
âˆ†. The left panel of Figure 4
15

0.0
0.2
0.4
0.6
0.8
1.0
âˆ’0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
âˆ†=0.1
âˆ†=0.6
âˆ†=0.7
âˆ†=0.8
âˆ†=0.95
0.0
0.2
0.4
0.6
0.8
âˆ’2
âˆ’1
0
1
2
3
4
âˆ†=0.1
âˆ†=0.6
âˆ†=0.7
âˆ†=0.8
âˆ†=0.95
Figure 3: Illustration of hâ‹†
gum,âˆ†(left) and hâ‹†
dif,âˆ†(right) for different values of âˆ†.
shows the efficiency RPâˆ†(hâ‹†
gum,âˆ†) as a function of âˆ†. This function is not smooth when âˆ†=
k
k+1
for integer k â‰¥1 because the support of P â‹†
âˆ†jumps at these values.
As an interesting fact, the least-favorable distribution P â‹†
âˆ†and its permuted counterparts form
all the vertices of Pâˆ†(see Lemma 3.4 in Section 3.2). Moreover, the vertices are the closest NTP
distributions in Pâˆ†to singular distributions. Roughly speaking, the closer to singular distributions,
the more difficult it is to tease apart the two hypotheses in (9). To see this, note that the testing
problem becomes most difficult when the NTP distribution is singular, which makes the null and
alternative distributions of the pivot indistinguishable.
Comparison with other rules
While it is now known that hâ‹†
gum,âˆ†is optimal in the sense of
Pâˆ†-efficiency, it remains interesting to analyze the relative efficiency of other existing rules. In
addition to the aforementioned hars and hlog, we consider hind,Î´(r) = 1{râ‰¥Î´} for a threshold 0 < Î´ < 1,
say, Î´ = eâˆ’1.14 This indicator function is included as it is perhaps the simplest score function
detecting the distributional differences between H0 and H1.
By making use of Theorem 2.1 within our framework, we compare these four detection rules in
terms of Pâˆ†-efficiency in the theorem below.
Theorem 3.3. There exists an absolute constant âˆ†â‹†â‰ˆ0.17756 such that the following two statements
hold:
(a) When 0.001 < âˆ†< âˆ†â‹†, hars has higher Pâˆ†-efficiency than both hlog and hind,eâˆ’1, that is,
max

RPâˆ†(hlog), RPâˆ†(hind,eâˆ’1)
	
< RPâˆ†(hars) < RPâˆ†(hâ‹†
gum,âˆ†).
(b) When âˆ†â‹†< âˆ†< 0.99, hlog has higher Pâˆ†-efficiency than both hars and hind,eâˆ’1, that is,
max

RPâˆ†(hars), RPâˆ†(hind,eâˆ’1)
	
< RPâˆ†(hlog) < RPâˆ†(hâ‹†
gum,âˆ†).
14According to the Râˆ†-efficiency defined in (25), Î´ = eâˆ’1 achieves the largest Râˆ†-efficiency when âˆ†â†’1. See
Lemma C.7 for details.
16

0.001
0.01
0.1
1
âˆ†
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
101
RPâˆ†(h)
hâ‹†
gum,âˆ†
hars
hlog
hind,1/e
0.05
0.17756...
0.25
âˆ†
0.00
0.02
0.04
0.06
0.08
0.10
Figure 4: Pâˆ†-efficiency rates of detection methods for Gumbel-max watermarks. Note that RPâˆ†(h)
has non-smooth points when âˆ†= 1
2, 2
3, 3
4, . . ..
Remark 3.2. The constant âˆ†â‹†is the unique root of the equation RPâˆ†(hars) = RPâˆ†(hlog) for âˆ†> 0.001.
Numerical evaluation shows âˆ†â‹†= 0.17756080525 Â· Â· Â· . A proof of Theorem 3.3 can be found in
Appendix B.6.
Figure 4 illustrates Theorem 3.3 by presenting the comparisons under our statistical framework.
Notably, for small values of âˆ†, maxwâˆˆW Pt,w can be close to 1. In this regime, Âµ1,Pt is close to Âµ0 in
distribution, making the detection efficiency relatively low regardless of the rule used. Conversely,
a large value of âˆ†implies all the token probabilities tend to be small, leading to a significant
discrepancy between Âµ1,Pt and Âµ0. Accordingly, all detection rules have efficiency rates tending to
infinity.
While hâ‹†
gum,âˆ†achieves the highest Pâˆ†-efficiency rate across the entire range of âˆ†, the relative
performance of hars and hlog depends on the value of âˆ†. From an empirical perspective, hars is
observed to outperform hlog in common settings of LLMs [38, 22]. This is because the largest
token probability in NTP distributions is, by and large, close to 1. To appreciate this fact, we
experiment on ChatGPT-3.5-turbo with twenty prompts (see Appendix D.3 for details) and track
the largest token probabilities across the generation of token sequences. The results are presented in
Figure 1, which shows that, for example, 56.85% of the NTP distributions have the largest token
probability above 1 âˆ’âˆ†â‹†â‰ˆ0.82. This is the regime where hars is superior to hlog, as predicted by
our Theorem 3.3.
Nevertheless, âˆ†can be large when the LLM generation is highly stochastic. In this regime,
hind,eâˆ’1 slightly outperforms hars, but the gap soon diminishes to zero as âˆ†tends to 1, as seen from
Figure 4. Similarly, hlog has a diminishing suboptimality compared with the optimal hâ‹†
gum,âˆ†in
the sense that lim
âˆ†â†’1 RPâˆ†(hlog)/RPâˆ†(hâ‹†
gum,âˆ†) â†’1. We prove the limiting behaviors of these score
functions in Appendix B.7.
17

3.2
Proof of Theorem 3.2
The proof of Theorem 3.2 relies on the minimax formulation (14) in our framework to solve
for the optimal score function.
As mentioned earlier, however, this task is generally difficult
since L(h, P ) = E0 h(Y ) + log E1,P eâˆ’h(Y ) is not convex-concave. We circumvent this obstacle by
introducing a simple yet remarkable result that characterizes the MGF (11) associated with the
Gumbel-max watermark.
Lemma 3.2 (Convexity Lemma). For any non-decreasing function h, the functional15 P 7â†’Ï•h(P ) :=
E1,P eâˆ’h(Y gum) is convex in P âˆˆSimp(W).
This convexity lemma is the key to the proof of Theorem 3.2 and is a contribution of independent
interest to future work on LLM watermarks. Roughly speaking, it helps reduce the â€œmaxâ€ part in the
minimax problem (14) to the problem of identifying vertices of the distribution class. Technically
speaking, the role of the convexity lemma in the proof of Theorem 3.2 is through the following
lemma.
Lemma 3.3. For any non-decreasing function h, we have
sup
P âˆˆPâˆ†
Ï•h(P ) = Ï•h(P â‹†
âˆ†).
(20)
Remark 3.3. A direct consequence of Lemma 3.3 is that RPâˆ†(h) = âˆ’infÎ¸â‰¥0 L(Î¸h, P â‹†
âˆ†).
Now we prove Lemmas 3.2 and 3.3 in order.
Proof of Lemma 3.2. By Lemma 3.1, the CDF of Y gum with NTP distribution P takes the form:
F1,P (r) = P
wâˆˆW Pwr1/Pw for r âˆˆ[0, 1]. This function relates to Ï•h(P ) since
Ï•h(P ) = E1,P eâˆ’h(Y gum) =
Z 1
0
eâˆ’h(r)F1,P (dr).
First, we show that P 7â†’F1,P (r) is a convex function for any given r âˆˆ[0, 1]. This is demonstrated
by showing that the Hessian matrix of P 7â†’F1,P (r) is positive semidefinite for any P within the
interior of its domain Simp(W):
âˆ‡2
P F1,P (r) =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
r1/P1 log2 r
P 3
1
0
. . .
0
0
r1/P2 log2 r
P 3
2
. . .
0
. . .
. . .
. . .
. . .
0
0
. . .
r1/P|W| log2 r
P 3
|W|
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£»
âª°0.
Note the Hessian is diagonal with all non-negative entries, confirming the convexity of P 7â†’F1,P (r).
Second, we examine the functional form of Ï•h(P ) through integration by parts, which yields:
Ï•h(P ) = F1,P (r)eâˆ’h(r)

1
0
+
Z 1
0
F1,P (r)eâˆ’h(r)h(dr) = eâˆ’h(1) +
Z 1
0
F1,P (r)eâˆ’h(r)h(dr).
This equation expresses Ï•h(P ) as a nonnegative weighted sum of F1,P (r) evaluated over r âˆˆ[0, 1].
Given the established convexity of P 7â†’F1,P (r), and considering that a nonnegative weighted
sum of convex functions remains convex, it follows directly that P 7â†’Ï•h(P ) is convex. This
completes the proof of Lemma 3.2.
15We consider a slightly different version of the MGF by setting Î¸ = 1 and taking as input P in (11).
18

Proof of Lemma 3.3. Our proof is based on a fundamental principle in convex analysis: the supremum
of a convex function over a compact convex set in a Euclidean space is necessarily attained at an
extreme point of the convex set [29].
Fix a non-decreasing function h. The mapping P 7â†’Ï•h(P ) is shown to be convex in P by
Lemma 3.3. Additionally, Pâˆ†is convex. Thus, the supremum of the convex function P 7â†’Ï•h(P )
must be necessarily attained at an extreme point of the convex constraint set Pâˆ†. By Lemma 3.4,
any extreme point of the set Pâˆ†must be a permuted P â‹†
âˆ†.
Lemma 3.4. The set of extreme points of Pâˆ†, denoted by Ext(Pâˆ†), is given by
Ext(Pâˆ†) = {Ï€(P â‹†
âˆ†) : Ï€ is a permutation on {1, 2, . . . , |W|}} ,
where Ï€(P ) denote the permuted NTP distribution whose ith coordinate is PÏ€(i).
Note there is a permutation invariance in the objective: Ï•h(P ) = Ï•h(Ï€(P )) holds for every
permutation Ï€ on {1, 2, . . . , |W|} and every vector P . As a consequence, we can explicitly compute
the supremum of Ï•h(P ) on the convex set Pâˆ†as follows:
sup
P âˆˆPâˆ†
Ï•h(P ) =
sup
P âˆˆExt(Pâˆ†)
Ï•h(P ) = Ï•h(P â‹†
âˆ†).
This proves equation (20). The remaining statement in Lemma 3.3 then follows.
We conclude this section by proving Theorem 3.2.
Proof of Theorem 3.2. The second part of this theorem follows directly from the convexity lemma
(Lemma 3.2) and Lemma 3.4.
To prove the first part, we work on the minimax formulation (14) and begin by recognizing that
min
h
max
P âˆˆPâˆ†L(h, P ) = min
h
max
P âˆˆPâˆ†(E0 h(Y gum) + log Ï•h(P ))
â‰¥min
h (E0 h(Y gum) + log Ï•h(P â‹†
âˆ†)) = âˆ’DKL(Âµ0, Âµ1,P â‹†
âˆ†),
(21)
where the final equality is due to the Donskerâ€“Varadhan representation [20] and DKL denotes the
Kullbackâ€“Leibler divergence. Note that Lemma 3.3 can not be applied here because the minimization
function h is not necessarily non-decreasing.
Due to the uniqueness of the Donskerâ€“Varadhan representation, E0 h(Y gum) + log Ï•h(P â‹†
âˆ†) is
strictly larger than âˆ’DKL(Âµ0, Âµ1,P â‹†
âˆ†) unless we take the log-likelihood ratio
hâ‹†
gum,âˆ†(r) = log
dÂµ1,P â‹†
âˆ†
dÂµ0
(r) = log

1
1 âˆ’âˆ†

r
âˆ†
1âˆ’âˆ†+ r
e
âˆ†
1âˆ’e
âˆ†

,
which is non-decreasing in r. In this case, we get
max
P âˆˆPâˆ†L(hâ‹†
gum,âˆ†, P ) = max
P âˆˆPâˆ†

E0 hâ‹†
gum,âˆ†(Y gum) + log Ï•hâ‹†
gum,âˆ†(P )

= E0 hâ‹†
gum,âˆ†(Y gum) + log Ï•hâ‹†
gum,âˆ†(P â‹†
âˆ†) = âˆ’DKL(Âµ0, Âµ1,P â‹†
âˆ†),
(22)
where the final equality follows from Lemma 3.3 and the third equality is ensured by, again, the
Donskerâ€“Varadhan representation.
Taken together, (21) and (22) show that hâ‹†
gum,âˆ†is the unique score function that solves the
minimax problem (14) with P â‹†
âˆ†.
19

0.0
0.2
0.4
0.6
0.8
1.0
Ut
0.0
0.2
0.4
0.6
0.8
1.0
Î·(Ï€t(wt))
Scatter plot of (Ut, Î·(Ï€t(wt)))
H0
H1
0.0
0.1
0.2
0.3
0.4
0
1000
2000
3000
4000
Histogram of Y dif
t
0.0
0.1
0.2
0.3
0.4
0.5
0
1000
2000
3000
4000
Histogram of Y dif
t
Figure 5: Left: the scatter plot of 5 Ã— 104 i.i.d. observations from (Ut, Î·(Ï€t(wt))) when |W| = 100
and the top five probabilities in Pt are fixed as (0.2, 0.2, 0.1, 0.05, 0.05) and the rest probabilities
are uniformly distributed. Middle: histogram of 104 i.i.d. observations of Y dif
t
when the underlying
NTP distribution is Pt. Right: the same setting as the middle panel except that the probabilities
except for the top five of Pt follow normalized Pow(0.5, 1) values.
4
Application to the Inverse Transform Watermark
In this section, we apply the framework to the inverse transform watermark [38]. Without loss of
generality, below we take W = {1, . . . , |W|}. Recall that its decoder is defined as
wt = Sinv(Pt, Î¶t) := Ï€âˆ’1
t (F âˆ’1(Ut; Ï€t)),
where Î¶t = (Ï€t, Ut) with Ut âˆ¼U(0, 1) and Ï€t being sampled uniformly at random from all permutations
on W.
Following the strategy in [49, 38], (Ï€t, Ut)â€™s are jointly independent across the token
sequence. For a given permutation Ï€, F(Â·, Ï€) above denotes the CDF under permutation: F(x; Ï€) =
P
wâ€²âˆˆW Pwâ€² Â· 1{Ï€(wâ€²)â‰¤x}. By construction, the inverse transform watermark is unbiased for sampling
from the NTP distribution P .
To see the unbiasedness directly, for any token w, note that
Sinv(P , Î¶) = w if and only if
X
wâ€²âˆˆW
Pwâ€² Â· 1{Ï€(wâ€²)<Ï€(w)} â‰¤U â‰¤
X
wâ€²âˆˆW
Pwâ€² Â· 1{Ï€(wâ€²)â‰¤Ï€(w)},
(23)
which occurs with probability Pw since the interval above has length Pw.
4.1
Main Results
To utilize the framework in Section 2 for the inverse transform watermark, we need to construct a
pivotal statistic Yt = Y (Ut, Ï€t) such that its distribution is known under the null, and meanwhile
should capture the dependence between Ut and Ï€t under the alternative for any NTP distribution.
Under H0, Ut is independent of the human-written token wt, and Ï€t(wt) is uniformly distributed
over the vocabulary W. Under H1, in contrast, a larger Ut value suggests a larger value of Ï€t(wt) in
distribution for the watermarked text, which can be gleaned from (23). See the left panel of Figure
5 for a simulation that illustrates the distinct behaviors of (Ut, Î·(Ï€t(wt)) under H0 and H1.
20

This motivates the use of the following statistic for watermark detection [38, 49]:
Y dif
t
= |Ut âˆ’Î·(Ï€t(wt))|,
Î·(i) :=
i âˆ’1
|W| âˆ’1,
where Î· normalizes an integer token to [0, 1] for direct comparison with U âˆˆ[0, 1]. As is clear, this
statistic is a pivot for our framework.
Technical challenge
To continue under our framework, we are faced with a difficulty in charac-
terizing the distribution of the Y dif
t
under the alternative. Although it is clear from Figure 5 that
Y dif
t
tends to be small under the alternative, the complex dependence of Y dif
t
â€™s distribution on the
unknown NTP Pt under H1 renders the evaluation of class-dependent efficiency in the framework
generally intractable.
Formally, this technical challenge can be elucidated by Lemma 4.1 and its following remark.
Lemma 4.1. Under H0, the CDF of Y dif
t
is
F dif
0 (r) := PH0(Y dif
t
â‰¤r) =
1
|W|
|W|
X
i=1
[min{Î·(i) + r, 1} âˆ’max{Î·(i) âˆ’r, 0}] .
Under H1, the CDF of Y dif
t
is
F dif
1,Pt(r) := PH1(Y dif
t
â‰¤r|Pt) =
1
|W|!
X
Ï€âˆˆÎ 
|W|
X
i=1
|(aÏ€,iâˆ’1, aÏ€,i] âˆ©B(Î·(i), r)|,
where Î  collects all permutations on W, aÏ€,i = Pi
j=1 Pt,Ï€(j), B(v, r) = {x âˆˆ[0, 1] : |x âˆ’v| â‰¤r} and
| Â· | represents the length of an interval.
In Lemma 4.1, the first part in particular shows that Y dif
t
is a pivotal statistic under H0. The
second part provides the explicit CDF of Y dif
t
under H1 conditional on Pt.
Nonetheless, Lemma 4.1 also shows that the distribution formula for Y dif
t
is by no means simple
under the alternative hypothesis H1: the dependence on Pt is intricate. The intricacy arises due
to the nature of S, whose definition involves all permutations Î  of W, which also complicates the
distribution formula for Y dif
t
= |Ut âˆ’Î·(Ï€t(wt))| where wt = S(Pt, (Ut, Ï€t)). Such complexity in the
distribution formula poses significant technical challenges in evaluating the effectiveness of any test
procedure under the alternative hypotheses H1.
Considering these intricacies, our goal of deriving an optimal score function h based on Y dif
t
seems challenging at first glance.
Asymptotic distributions
Fortunately, we discover a surprising result that simplifies the under-
standing of the distribution of Y dif
t
under the alternative H1, which facilitates later derivation of
the optimal score function h. Roughly speaking, the conditional distribution of Y dif
t
given Pt under
H1 primarily depends on the largest values among Ptâ€™s coordinates, especially when the vocabulary
size |W| is large. For illustration, histograms in Figure 5 compare Y dif
t
under H1 for different Pt
and P â€²
t scenarios where |W| = 100. Despite differences in Pt and P â€²
t, each sharing the same top five
coordinates but differing elsewhere, the conditional distribution of Y dif
t
remains remarkably similar.
21

We now mathematically formalize this empirical observation under a special case where there
is one token that takes predominantly high probability in Pt. This scenario, being the most basic
case in theory, offers practical insights as empirically, many LLMs often concentrate most of Ptâ€™s
probability mass on a single token [23]. Through our analysis, we derive the limiting distribution
of Y dif
t
given Pt when the vocabulary size |W| â†’âˆ. Formally, this means that we are effectively
examining a sequence of Pt, which is indexed by |W|, and investigate the limiting conditional
distribution of Y dif
t
given Pt as we move along that sequence. Yet when the context makes it clear,
we suppress the dependence of Pt on the index |W| for notation simplicity.
In below, we always use Pt,(i) to refer to the i-th largest coordinates of Pt for every vector Pt
and integer i between 1 and |W|.
Theorem 4.1. Under H0,
lim
|W|â†’âˆPH0(Y dif
t
â‰¤r) = 1 âˆ’(1 âˆ’r)2 for any r âˆˆ[0, 1].
Under H1, assuming that
lim
|W|â†’âˆPt,(1) = 1 âˆ’âˆ†and
lim
|W|â†’âˆlog |W| Â· Pt,(2) = 0 hold, then
lim
|W|â†’âˆPH1(Y dif
t
â‰¤r | Pt) = 1 âˆ’

1 âˆ’
r
1 âˆ’âˆ†
2
for any r âˆˆ[0, 1 âˆ’âˆ†].
Theorem 4.1 formalizes this observed phenomenon, providing a simplified distribution characteri-
zation of the statistics Y dif
t
under H1, assuming that the token probability Pt is highly concentrated
at a single token, under the large vocabulary size limit |W| â†’âˆ. The limiting distribution is
determined by a single scalar 1 âˆ’âˆ†that corresponds to the top token probability.
Notably, in this limit |W| â†’âˆ, the statistics Y dif
t
has a different support under H0 and under
H1. By exploiting this distinction, we proceed to identify the optimal score h under our framework,
which achieves infinite power in the limit |W| â†’âˆin distinguishing the hypotheses H0 and H1.
Optimal score function
Following our previous discussions, we focus on scenarios where an
LLMâ€™s probability distribution, Pt, is primarily concentrated on a single token.
We model Pt using a belief class Pâˆ†, a subset of the âˆ†-regular class Pâˆ†. Unlike Pâˆ†, which only
restricts the highest token probability to 1 âˆ’âˆ†, the belief class Pâˆ†also sets a limit on the second
highest token probability, using a threshold Îµ|W|:16
Pâˆ†=

P : P(1) â‰¤1 âˆ’âˆ†, P(2) â‰¤Îµ|W|
	
.
(24)
In our subsequent asymptotic analysis |W| â†’âˆ, we assume that log |W| Â· Îµ|W| â†’0, from which we
can utilize Theorem 4.1 to simplify our distributional characterization of Y dif
t
.
This leads to the definition of a limit efficiency measure, Râˆ†, whose definition is based on our
efficiency measure RPâˆ†under our framework in Section 2:
Râˆ†(h) = lim inf RPâˆ†(h).
(25)
where lim inf denotes the large vocabulary size limit where |W| â†’âˆand log |W| Â· Îµ|W| â†’0. In
plain words, Râˆ†(h) quantifies the efficiency of any score function h in the limit of large vocabulary
16For simplicity, we omit the notation indicating the dependence of Pâˆ†on Îµ|W| when the context makes this clear.
22

size |W| â†’âˆ, assuming that the distribution of the underlying LLM predominantly focuses on a
single token.
We are ready to describe the optimal score function h that maximizes this efficiency measure
Râˆ†(h). This optimal procedure is formally stated in Theorem 4.2.
Theorem 4.2. Fix âˆ†âˆˆ(0, 1]. Let hâ‹†
dif,âˆ†: [0, 1] â†’R denote
hâ‹†
dif,âˆ†(r) = log fdif,âˆ†(r)
fdif,0(r)
where fdif,âˆ†(r) =
2
1 âˆ’âˆ†Â· max

1 âˆ’
r
1 âˆ’âˆ†, 0

Then, limMâ†’âˆRâˆ†

[hâ‹†
dif,âˆ†][âˆ’M,M]

= âˆ.
Theorem 4.2 identifies the score function hâ‹†
dif,âˆ†that achieves infinite power in distinguishing H1
from H0 in the large vocabulary limit |W| â†’âˆ, assuming that the NTP distribution Pt focuses on
a single token, formally described by Pâˆ†with log |W| Â· Îµ|W| â†’0. The right panel of Figure 3 shows
hâ‹†
dif,âˆ†for different values of âˆ†.
Our derivation of hâ‹†
dif,âˆ†is based on calculating a log-likelihood ratio between the hardest alter-
native within Pâˆ†and the null, using the asymptotic distribution formula described in Theorem 4.1.
The log-likelihood ratio hâ‹†
dif,âˆ†achieves infinite power due to Y dif having distinct supports under H0
and H1 in the limit, as demonstrated in Theorem 4.1.
Finally, the truncation in [hâ‹†
dif,âˆ†][âˆ’M,M] is mainly for technical reasons, stemming from a proof
artifact.
4.2
Proof of Theorem 4.2
Fix âˆ†âˆˆ(0, 1]. To prove Theorem 4.2, our goal is to establish the limit: limMâ†’âˆRâˆ†

[hâ‹†
dif,âˆ†][âˆ’M,M]

=
âˆ. Our strategy involves establishing an effective lower bound for Râˆ†(h), as detailed in Lemma 4.2.
This involves introducing a function Fdif,âˆ†for each âˆ†âˆˆ[0, 1], defined as:
Fdif,âˆ†(r) = 1 âˆ’
max
n
1 âˆ’
r
1âˆ’âˆ†, 0
o2
for any r âˆˆ[0, 1].
By Theorem 4.1, this corresponds to the limiting CDF
of Y dif given Pt, assuming that Pt âˆˆPâˆ†.
Lemma 4.2. For any function h that is non-decreasing, Lipschitz-continuous on [0, 1], there is the
lower bound:
Râˆ†(h) â‰¥âˆ’
Z 1
0
h(r)Fdif,0(dr) + log
Z 1
0
eâˆ’h(r)Fdif,âˆ†(dr)

.
(26)
The proof of Lemma 4.2 is deferred to Section 4.2.1.
Now we finish the proof of Theorem 4.2. Let G(h) denote the RHS of inequality (26). Also,
let Âµdif,âˆ†denote the probability measure associated with the CDF Fdif,âˆ†. By Donskerâ€“Varadhan
representation [20], we deduce:
sup
h
G(h) = sup
h

âˆ’
Z 1
0
h(r)Fdif,0(dr) âˆ’log
Z 1
0
eâˆ’h(r)Fdif,âˆ†(dr)

= DKL(Âµdif,0, Âµdif,âˆ†) = âˆ
where the last identity holds because the two probability measures Âµdif,0 and Âµdif,âˆ†have different
support, with Âµdif,0 supported on [0, 1] and Âµdif,âˆ†on [0, 1 âˆ’âˆ†] for âˆ†> 0. This supremum suph G(h)
is obtained for h as the log-likelihood ratio:
hâ‹†
dif,âˆ†= log dÂµdif,âˆ†
dÂµdif,0
= log fdif,âˆ†
fdif,0
.
23

Despite G(hâ‹†
dif,âˆ†) = âˆ, we are not able to use Lemma 4.2 to conclude that Râˆ†(hâ‹†
dif,âˆ†) = âˆ.
This is because the function hâ‹†
dif,âˆ†, despite being non-increasing, is neither uniformly bounded nor
Lipschitz-continuous on R, thus not meeting the requirements of Lemma 4.2.
Nonetheless, after truncating hâ‹†
dif,âˆ†to hM = [hâ‹†
dif,âˆ†][âˆ’M,M], the function hM is non-decreasing,
Lipschitz-continuous, and uniformly bounded on R for any M < âˆ. This allows us to use Lemma 4.2
to conclude that for every M < âˆ: Râˆ†(hM) â‰¥G(hM). Applying the limit M â†’âˆon both sides,
and leveraging Lebesgueâ€™s dominated convergence theorem along with Fatouâ€™s lemma, we deduce:
lim
Mâ†’âˆRâˆ†
 [hâ‹†
dif,âˆ†][âˆ’M,M]

= lim
Mâ†’âˆRâˆ†(hM) â‰¥lim
Mâ†’âˆG(hM) â‰¥G(hâ‹†
dif,âˆ†) = âˆ.
This yields Theorem 4.2 as desired.
4.2.1
Proof of Lemma 4.2
We are interested in lower bounding Râˆ†(h) = lim inf RPâˆ†(h). Recall that its definition is given by:
Râˆ†(h) = âˆ’lim sup inf
Î¸â‰¥0 sup
P âˆˆPâˆ†
h
Î¸ E0 h(Y dif) + log E1,P [eâˆ’Î¸h(Y dif)]
i
.
(27)
Our lower bound strategy relies on first connecting Râˆ†(h) to an intermediate quantity that
L(h, âˆ†â€²) := lim sup sup
P âˆˆQâˆ†â€²
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
where Qâˆ†â€² :=

P : P(1) = 1 âˆ’âˆ†â€², log |W| Â· P(2) â‰¤Îµ|W|
	
. This connection is given in Lemma 4.3.
Lemma 4.3. For every function h Lipschitz-continuous on [0, 1], we have
Râˆ†(h) â‰¥âˆ’sup
âˆ†â€²â‰¥âˆ†
L(h, âˆ†â€²).
Lemma 4.3 implies that maximizing Râˆ†(h) is related to the minimax optimization: infh supâˆ†â€²â‰¥âˆ†L(h, âˆ†â€²).
This minimax formulation is closely related to our original minimax formulation (14), yet it replaces
the |W|-dimensional variable P with a scalar âˆ†â€² after taking asymptotics (Theorem 4.1).
Proof of Lemma 4.3. By definition, we have
inf
Î¸â‰¥0 sup
P âˆˆPâˆ†
h
Î¸ E0 h(Y dif) + log E1,P [eâˆ’Î¸h(Y dif)]
i
â‰¤sup
P âˆˆPâˆ†
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
.
After we substitute this inequality into (27), we obtain a lower bound
Râˆ†(h) â‰¥âˆ’lim sup sup
P âˆˆPâˆ†
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
= âˆ’lim sup sup
âˆ†â€²â‰¥âˆ†
sup
P âˆˆQâˆ†â€²
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
,
(28)
where the second identity holds because Pâˆ†= âˆªâˆ†â€²â‰¥âˆ†Qâˆ†â€² by definition.
We wish to swap the order of supâˆ†â€²â‰¥âˆ†and lim sup in (28) to reach the conclusion of Lemma 4.3.
Lemma 4.4 justifies such a swap as valid.
24

Lemma 4.4. For any Lipschitz-continuous function h on [0, 1], we have
lim sup sup
âˆ†â€²â‰¥âˆ†
sup
P âˆˆQâˆ†â€²
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
= sup
âˆ†â€²â‰¥âˆ†
lim sup sup
P âˆˆQâˆ†â€²
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
.
By swapping supâˆ†â€²â‰¥âˆ†and lim sup in (28), we obtain
Râˆ†(h) â‰¥âˆ’sup
âˆ†â€²â‰¥âˆ†
lim sup sup
P âˆˆQâˆ†â€²
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
= âˆ’sup
âˆ†â€²â‰¥âˆ†
L(h, âˆ†â€²).
This completes the proof of Lemma 4.3.
Lemma 4.3 reduces the problem of finding a lower bound on Râˆ†(h) to evaluating supâˆ†â€²â‰¥âˆ†L(h, âˆ†â€²).
To do so, we first derive an explicit expression for L(h, âˆ†â€²).
Lemma 4.5. For any Lipschitz-continuous h on [0, 1], we have
L(h, âˆ†â€²) =
Z 1
0
h(r)Fdif,0(dr) + log
Z 1
0
eâˆ’h(r)Fdif,âˆ†â€²(dr).
The proof of Lemma 4.5 is deferred to Appendix C.4. Here we give some intuition behind its
proof. Theorem 4.1 states that as |W| â†’âˆ, for any sequence of P âˆˆQâˆ†â€² indexed by W, the CDF
of Y dif under H0 converges to Fdif,0, while its CDF under H1 given P converges to Fdif,âˆ†â€². This
results in a convergence statement that holds for any sequence of P âˆˆQâˆ†â€², and for any continuous
function h uniformly bounded on R:
lim
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
=
Z 1
0
h(r)Fdif,0(dr) + log
Z 1
0
eâˆ’h(r)Fdif,âˆ†â€²(dr).
Lemma 4.5 strengthens this by ensuring such convergence can be made uniform across the sequence:
L(h, âˆ†â€²) = lim sup sup
P âˆˆQâˆ†â€²
h
E0 h(Y dif) + log E1,P [eâˆ’h(Y dif)]
i
=
Z 1
0
h(r)Fdif,0(dr) + log
Z 1
0
eâˆ’h(r)Fdif,âˆ†â€²(dr).
The caveat is that the above uniform limit applies only to functions h that are Lipschitz-continuous
and uniformly bounded on R, as required in Lemma 4.5.
Finally, with Lemma 4.5 we are able to derive an explicit expression of the supremum supâˆ†â€²â‰¥âˆ†L(h, âˆ†â€²).
Lemma 4.6. For any non-increasing and Lipschitz-continuous h on [0, 1],
sup
âˆ†â€²â‰¥âˆ†
L(h, âˆ†â€²) = L(h, âˆ†) =
Z 1
0
h(r)Fdif,0(dr) + log
Z 1
0
eâˆ’h(r)Fdif,âˆ†(dr).
Proof of Lemma 4.6. Using integration by parts, we first obtain the equation:
Z 1
0
eâˆ’h(r)Fdif,âˆ†â€²(dr) = eâˆ’h(1) âˆ’
Z 1
0
Fdif,âˆ†â€²(r)eâˆ’h(r)(âˆ’h)(dr).
25

As a consequence, the above integralâ€™s value is non-increasing as âˆ†â€² increases because Fdif,âˆ†â€²(r) is
non-decreasing in âˆ†â€² for any r within the interval [0, 1].
By Lemma 4.5, this further implies that L(h, âˆ†â€²) is non-decreasing in âˆ†â€². Thus, supâˆ†â€²â‰¥âˆ†L(h, âˆ†â€²) =
L(h, âˆ†). The conclusion of Lemma 4.6 then follows.
We now finish the proof of Lemma 4.2. By Lemma 4.3 and Lemma 4.6, we obtain
Râˆ†(h) â‰¥âˆ’sup
âˆ†â€²â‰¥âˆ†
L(h, âˆ†â€²) = âˆ’
Z 1
0
h(r)Fdif,0(dr) âˆ’log
Z 1
0
eâˆ’h(r)Fdif,âˆ†(dr).
This yields the lower bound as desired.
5
Experiments
This section highlights our frameworkâ€™s effectiveness through synthetic and real-data experiments,
showcasing the practical utility of our proposed methods for detecting watermarks.17
5.1
Synthetic Studies
In our simulation, we generate a vocabulary W of size 1, 000 and assess Type I and II errors in
watermark detection methods for generated token sequences. These methods include hars(Y gum),
hlog(Y gum), hind,eâˆ’1(Y gum), hâ‹†
gum,âˆ†(Y gum) for Gumbel-max watermark and hneg(Y dif), hâ‹†
dif,âˆ†(Y dif)
for the inverse transform watermark.
Our initial investigation is on Type I error control for finite token sequences, recognizing that the
guarantees for Type I error control follow from the central limit theorem and hold asymptotically
when the sequence length T â†’âˆ. For a given text length T, we generate 5, 000 samples of
unwatermarked word tokens sequences. We uniformly sample each unwatermarked token from the
vocabulary W in our experiments. Although different sampling strategies could be employed, due to
the pivotal property of our test statistics, we anticipate that they would lead to similar Type I error
rates.
Throughout 5, 000 repeated experiments, we compute the average Type I error, with the findings
presented in the leftmost column of Figure 6. These results reveal that empirically, the Type I errors
generally align with the nominal level of 0.05, fluctuating between 0.04 and 0.06. This performance
confirms our theoretical expectations regarding Type I error control across most scenarios. The only
exception is the score function hind,Î´(Y gum), achieving close to the nominal 0.05 level only for token
sequences over 300 in length, where asymptotic effects emerge.
Next, we examine the effectiveness of these methods in controlling Type II errors on watermarked
sequences. Performing this examination involves specifying how each Pt is generated under H1. In
our simulation, we model LLMâ€™s NTP (Pt) distributions as spike distributions: we set its largest
probability as Pt,(1) = 1âˆ’âˆ†t, where âˆ†t > 0 is i.i.d. sampled from [a, b] with a = 10âˆ’3,18 and b = 0.5,
and uniformly distribute the remaining probabilities so that Pt,(k) = âˆ†t/(|W| âˆ’1) for k > 1. The
choice of this setup is based on our theoretical development. We also tested alternative b values of
0.1, 0.3, and 0.7, but given the results are similar, we will include those details in the appendix.
17The experiment code is collected in the GitHub repository https://github.com/lx10077/WatermarkFramework.
18This small number 10âˆ’3 is used to avoid H1 merging with H0.
26

0
200
400
600
Unwatermarked text length
4 Ã— 10âˆ’2
5 Ã— 10âˆ’2
6 Ã— 10âˆ’2
7 Ã— 10âˆ’2
8 Ã— 10âˆ’2
9 Ã— 10âˆ’2
Type I error
H0
0
200
400
600
Watermarked text length
10âˆ’2
10âˆ’1
Type II error
H1, âˆ†âˆ¼U(0.001, 0.5)
hars
hlog
hind,1/e
hâ‹†
gum,0.01
hâ‹†
gum,0.005
0
200
400
600
Unwatermarked text length
4 Ã— 10âˆ’2
5 Ã— 10âˆ’2
6 Ã— 10âˆ’2
7 Ã— 10âˆ’2
Type I error
H0
0
200
400
600
Watermarked text length
10âˆ’1
100
Type II error
H1, âˆ†âˆ¼U(0.001, 0.5)
hneg
hâ‹†
dif,0.1
hâ‹†
dif,0.01
hâ‹†
dif,0.001
Figure 6: Average Type I and II errors (on a log scale) vs. text length on synthetic datasets for
Gumbel-max (top) and inverse transform watermarks (bottom).
Throughout 5, 000 repeated experiments, we compute the average Type II error and present the
results in the rightmost column of Figure 6. Designed using the optimality criterion through our
framework, both hâ‹†
gum,âˆ†(Y gum) for Gumbel-max and hâ‹†
dif,âˆ†(Y dif) for inverse transform outperform
other watermark detection techniques in reducing Type II errors. Additionally, empirical performance
rankings are consistent with our theoretical prediction in Theorem 3.3: hars(Y gum) has lower Type
II error, followed by hlog(Y gum), with hind,eâˆ’1(Y gum) having the highest Type II error.
5.2
Real-World Examples
In this section, we conduct an empirical analysis of watermark detection methods for text sequences
from large token generation models such as OPT-1.3B [76] and Sheared-LLaMA-2.7B [71]. Specifically,
we evaluate Type I errors on the unwatermarked texts directly from large token generation models
and Type II errors with texts from the same models, but with watermarks incorporated. This
experimental setup essentially follows the approach described in the existing literature [35] and is
further detailed in Appendix D.2 for completeness.
We present our numerical results with the OPT-1.3B models in Figure 7 (similar results for
Sheared-LLaMA-2.7B are documented in Appendix D.2). Notably, both the average Type I error
(leftmost column of Figure 7) and Type II error (rightmost column of Figure 7) curves are computed
based on 500 repeated experiments. Though the setup (mainly the way each Pt is generated) is
slightly different from our simulation studies, the conclusions are quite similar.
First, in most scenarios, all the detection methods maintain Type I errors within 0.04 to 0.06,
27

0
50
100
150
200
Unwatermarked text length
10âˆ’1
4 Ã— 10âˆ’2
6 Ã— 10âˆ’2
2 Ã— 10âˆ’1
Type I error
0
50
100
150
200
Watermarked text length
5 Ã— 10âˆ’1
6 Ã— 10âˆ’1
7 Ã— 10âˆ’1
8 Ã— 10âˆ’1
9 Ã— 10âˆ’1
Type II error
hars
hlog
hind,eâˆ’1
hâ‹†
gum,0.1
0
50
100
150
200
Unwatermarked text length
3 Ã— 10âˆ’2
4 Ã— 10âˆ’2
6 Ã— 10âˆ’2
Type I error
0
50
100
150
200
Watermarked text length
100
6 Ã— 10âˆ’1
7 Ã— 10âˆ’1
8 Ã— 10âˆ’1
9 Ã— 10âˆ’1
Type II error
hneg
hâ‹†
dif,0.1
hâ‹†
dif,0.01
hâ‹†
dif,0.001
Figure 7: Average Type I and II errors (on a log scale) vs. text length on C4 datasets [52] and the
OPT-1.3B model for Gumbel-max (top) and inverse transform watermarks (bottom).
aligning well with the expected 0.05 level, particularly in texts over 100 tokens long, as observed in
our numerical experiments. Second, our methods, hâ‹†
gum,âˆ†(Y gum) for Gumbel-max and hâ‹†
dif,âˆ†(Y dif)
for inverse transform, excel in minimizing Type II errors over other score functions. For Gumbel-max
watermarks, the performance rankings follow our Theorem 3.3â€™s predictions: hars(Y gum) records a
lower Type II error, then hlog(Y gum), with hind,eâˆ’1(Y gum) the highest type II error.
6
Discussion
In this paper, we have introduced a statistical framework for reasoning about watermarks for LLMs
from a hypothesis testing viewpoint. Our framework offers techniques and concepts for analyzing
the asymptotic test efficiency of watermark detection by evaluating Type I and Type II errors of the
problem of watermark detection. Specifically, to provide a formal approach to comparing different
detection rules, we have introduced class-dependent efficiency, which further reduces the comparison
problem to a minimax optimization problem. We have applied this framework to two representative
unbiased watermarks, namely, the Gumbel-max watermark and the inverse transform watermark.
Our results include the derivation of optimal detection rules, which are shown to have competitive
or superior performance compared to existing detection rules.
Our work opens up several research directions in this emerging line of statistical research on
watermarks for LLMs. Perhaps the most immediate question is how to choose âˆ†for the distribution
class Pâˆ†when the knowledge of the underlying token distributions is limited. For instance, it would
28

be valuable to explore methods for enhancing detection power when âˆ†is known in a distributional
sense (see the discussion in Appendix D.4).
Another approach is to extend Proposition 2.1 to
multiple distribution classes in recognition of the heterogeneity of NTP distributions across the
token sequence. This can be done, for example, by adaptively clustering the empirical distributions
of Pt into several clusters and using these to represent the distribution classes. Moreover, it would
be a welcome advance to offer guidelines on how to choose pivotal statistics, which perhaps would
allow the framework to maximize its utility. In this paper, we fix the pivotal statistics from the
beginning. However, we show that it is fundamentally difficult to identify optimal pivotal statistics
that maximize our efficiency notion. The challenge lies in the nonconvex nature of the set of pivotal
statistics, making the selection problem a nonconvex optimization problem that is currently beyond
our reach (see Appendix A.3).
Finally, given that an unbiased watermark is probabilistically similar
to sampling from a multinomial distribution, it would be of interest to consider other sampling
schemes such as the alias method [66, 67] to develop new categories of watermarks.
More broadly, we wish to remark on several possible extensions of our framework for improving
watermark detection and design. Our framework would be enhanced if different watermarks with
their own detection rules could be compared on a fair and comprehensive basis. Different watermarks
might work best in different regimes and, therefore, directly comparing their class-dependent efficiency
rates would be very sensitive to the choice of the distribution class. Another direction to extend our
framework is to take distribution classes that are adaptive to NTP distributions in the real-world
deployment of LLMs. This adaptivity has the potential to make the minimax formulation (14) better
capture the empirical performance. A possible approach is to model the empirical distribution of the
NTP distributions Pt using Zipfâ€™s law [80] or other laws. A challenge here, however, might arise
from the fact that the distribution class is no longer permutation invariant. Finally, an interesting
extension is to have varying score functions across the token sequence, as opposed to having the
same h in the test statistic (10). This flexibility might enhance the power of watermark detection by
taking into account the heterogeneity of the NTP distributions from token to token in generation.
Acknowledgments
This work was supported in part by NIH grants, RF1AG063481 and U01CA274576, NSF DMS-2310679, a
Meta Faculty Research Award, and Wharton AI for Business. The content is solely the responsibility of the
authors and does not necessarily represent the official views of the NIH.
References
[1] S. Aaronson. Watermarking of large language models. https://simons.berkeley.edu/talks/scott-
aaronson-ut-austin-openai-2023-08-17, August 2023.
[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[3] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive
science, 9(1):147â€“169, 1985.
[4] M. Albert. Concentration inequalities for randomly permuted sums. In High Dimensional Probability
VIII: The Oaxaca Volume, pages 341â€“383. Springer, 2019.
[5] M. J. Atallah, V. Raskin, M. Crogan, C. Hempelmann, F. Kerschbaum, D. Mohamed, and S. Naik.
Natural language watermarking: Design, analysis, and a proof-of-concept implementation. In Information
29

Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April 25â€“27, 2001 Proceedings 4,
pages 185â€“200. Springer, 2001.
[6] R. R. Bahadur. On the asymptotic efficiency of tests and estimates. SankhyÂ¯a: The Indian Journal of
Statistics, pages 229â€“252, 1960.
[7] G. Bao, Y. Zhao, Z. Teng, L. Yang, and Y. Zhang. Fast-detectgpt: Efficient zero-shot detection of
machine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130, 2023.
[8] B. Barak. An intensive introduction to cryptography, lectures notes for Harvard CS 127. https:
//intensecrypto.org/public/index.html, Fall 2021.
[9] D. Bartz and K. Hu.
OpenAI, Google, others pledge to watermark AI content for safety, White
house says. https://www.reuters.com/technology/openai-google-others-pledge-watermark-ai-
content-safety-white-house-2023-07-21/, 2023. Accessed: 2023-10-03.
[10] J. Biden.
Fact sheet:
President Biden issues executive order on safe,
secure,
and trust-
worthy
artificial
intelligence.
https://www.whitehouse.gov/briefing-room/statements-
releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-
and-trustworthy-artificial-intelligence/, October 2023. Accessed: 2023-11-01.
[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. In Advances in neural information
processing systems, volume 33, pages 1877â€“1901, 2020.
[12] Z. Cai, S. Liu, H. Wang, H. Zhong, and X. Li. Towards better statistical understanding of watermarking
LLMs. arXiv preprint arXiv:2403.13027, 2024.
[13] F. Cayre, C. Fontaine, and T. Furon. Watermarking security: Theory and practice. IEEE Transactions
on signal processing, 53(10):3976â€“3987, 2005.
[14] S. Chakraborty, A. S. Bedi, S. Zhu, B. An, D. Manocha, and F. Huang. On the possibilities of ai-generated
text detection. arXiv preprint arXiv:2304.04736, 2023.
[15] M. Christ, S. Gunn, and O. Zamir. Undetectable watermarks for language models. arXiv preprint
arXiv:2306.09194, 2023.
[16] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker. Digital watermarking and steganography. Morgan
kaufmann, 2007.
[17] D. Das, K. De Langis, A. Martin, J. Kim, M. Lee, Z. M. Kim, S. Hayati, R. Owan, B. Hu, R. Parkar, et al.
Under the surface: Tracking the artifactuality of LLM-generated data. arXiv preprint arXiv:2401.14698,
2024.
[18] A. Dembo and O. Zeitouni. Large deviations techniques and applications, volume 38. Springer Science &
Business Media, 2009.
[19] L. Devroye. Nonuniform random variate generation. Handbooks in operations research and management
science, 13:83â€“121, 2006.
[20] M. D. Donsker and S. S. Varadhan. Asymptotic evaluation of certain Markov process expectations for
large time. IV. Communications on pure and applied mathematics, 36(2):183â€“212, 1983.
[21] R. Durrett. Probability: Theory and Examples (Edition 4.1). Cambridge University Press, 2013.
[22] P. Fernandez, A. Chaffin, K. Tit, V. Chappelier, and T. Furon. Three bricks to consolidate watermarks
for large language models. arXiv preprint arXiv:2308.00113, 2023.
[23] S. Gehrmann, H. Strobelt, and A. Rush. GLTR: Statistical detection and visualization of generated text.
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System
Demonstrations. Association for Computational Linguistics, 2019.
30

[24] E. Giboulot and F. Teddy. WaterMax: breaking the LLM watermark detectability-robustness-quality
trade-off. arXiv preprint arXiv:2403.04808, 2024.
[25] GPTZero. GPTZero: More than an AI detector preserve whatâ€™s human. https://gptzero.me/, 2023.
[26] E. J. Gumbel. Statistical theory of extreme values and some practical applications: A series of lectures,
volume 33. US Government Printing Office, 1948.
[27] C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser,
J. Taylor, S. Berg, N. J. Smith, et al. Array programming with NumPy. Nature, 585(7825):357â€“362,
2020.
[28] T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori perturbations.
In International Conference on Machine Learning, pages 1667â€“1674, 2012.
[29] J.-B. Hiriart-Urruty and C. LemarÃ©chal. Convex analysis and minimization algorithms I: Fundamentals,
volume 305. Springer science & business media, 1996.
[30] Z. Hu, L. Chen, X. Wu, Y. Wu, H. Zhang, and H. Huang. Unbiased watermark for large language
models. arXiv preprint arXiv:2310.10669, 2023.
[31] B. Huang, H. Zhu, B. Zhu, K. Ramchandran, M. I. Jordan, J. D. Lee, and J. Jiao. Towards optimal
statistical watermarking. arXiv preprint arXiv:2312.07930, 2023.
[32] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-Softmax. In International
Conference on Learning Representations, 2016.
[33] J. Katz and Y. Lindell. Introduction to Modern Cryptography. Chapman & Hall CRC, 2008.
[34] J. Kim, M. Tawarmalani, and J.-P. P. Richar. Convexification of Permutation-invariant Sets. Krannert
School of Management, Purdue University, Institute for Research in the Behavioral, Economic, and
Management Sciences, 2019.
[35] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein. A watermark for large language
models. In International Conference on Machine Learning, volume 202, pages 17061â€“17084, 2023.
[36] J. Kirchenbauer, J. Geiping, Y. Wen, M. Shu, K. Saifullah, K. Kong, K. Fernando, A. Saha, M. Gold-
blum, and T. Goldstein. On the reliability of watermarks for large language models. arXiv preprint
arXiv:2306.04634, 2023.
[37] K. Krishna, Y. Song, M. Karpinska, J. Wieting, and M. Iyyer. Paraphrasing evades detectors of
AI-generated text, but retrieval is an effective defense. In Advances in Neural Information Processing
Systems, volume 36, 2024.
[38] R. Kuditipudi, J. Thickstun, T. Hashimoto, and P. Liang. Robust distortion-free watermarks for language
models. arXiv preprint arXiv:2307.15593, 2023.
[39] W. Liang, M. Yuksekgonul, Y. Mao, E. Wu, and J. Zou. GPT detectors are biased against non-native
english writers. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning
Models, 2023.
[40] T. Lin, C. Jin, and M. I. Jordan. Near-optimal algorithms for minimax optimization. In Conference on
Learning Theory, pages 2738â€“2779. PMLR, 2020.
[41] Y. Liu and Y. Bu. Adaptive text watermark for large language models. arXiv preprint arXiv:2401.13927,
2024.
[42] C. J. Maddison, D. Tarlow, and T. Minka. A* sampling. In Advances in neural information processing
systems, volume 27, 2014.
[43] S. Milano, J. A. McGrane, and S. Leonelli. Large language models challenge the future of higher
education. Nature Machine Intelligence, 5(4):333â€“334, 2023.
31

[44] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn. Detectgpt: Zero-shot machine-generated
text detection using probability curvature. arXiv preprint arXiv:2301.11305, 2023.
[45] M. E. Oâ€™neill. PCG: A family of simple fast space-efficient statistically good algorithms for random
number generation. ACM Transactions on Mathematical Software, 2014.
[46] OpenAI.
ChatGPT: Optimizing language models for dialogue.
http://web.archive.org/web/
20230109000707/https://openai.com/blog/chatgpt/, Jan 2023.
[47] C. Paar and J. Pelzl. Understanding cryptography: A textbook for students and practitioners. Springer
Science & Business Media, 2009.
[48] G. Papandreou and A. L. Yuille. Perturb-and-map random fields: Using discrete optimization to learn
and sample from energy models. In 2011 International Conference on Computer Vision, pages 193â€“200.
IEEE, 2011.
[49] J. Piet, C. Sitawarin, V. Fang, N. Mu, and D. Wagner. Mark my words: Analyzing and evaluating
language model watermarks. arXiv preprint arXiv:2312.00273, 2023.
[50] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition
via large-scale weak supervision. In International Conference on Machine Learning, pages 28492â€“28518.
PMLR, 2023.
[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised
multitask learners. OpenAI blog, 1(8):9, 2019.
[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.
Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485â€“5551, 2020.
[53] H. Rahimian and S. Mehrotra. Frameworks and results in distributionally robust optimization. Open
Journal of Mathematical Optimization, 3:1â€“85, 2022.
[54] V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and S. Feizi. Can AI-generated text be
reliably detected? arXiv preprint arXiv:2303.11156, 2023.
[55] B. Schneier. Applied Cryptography. John Wiley & Sons, 1996.
[56] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. The curse of recursion:
Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.
[57] H. A. Soufiani, D. C. Parkes, and L. Xia. Random utility theory for social choice. In Neural Information
Processing Systems, pages 126â€“134, 2012.
[58] K. Starbird. Disinformationâ€™s spread: Bots, trolls and all of us. Nature, 571(7766):449â€“450, 2019.
[59] D. R. Stinson. Cryptography: Theory and practice. Chapman & Hall CRC, 2005.
[60] C. Stokel-Walker. AI bot ChatGPT writes smart essaysâ€”Should professors worry? Nature News, 2022.
[61] U. Topkara, M. Topkara, and M. J. Atallah. The hiding virtues of ambiguity: Quantifiably resilient
watermarking of natural language text through synonym substitutions. In Proceedings of the 8th workshop
on Multimedia and security, pages 164â€“174, 2006.
[62] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal,
E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971, 2023.
[63] E. Tulchinskii, K. Kuznetsov, L. Kushnareva, D. Cherniavskii, S. Nikolenko, E. Burnaev, S. Barannikov,
and I. Piontkovskaya. Intrinsic dimension estimation for robust detection of AI-generated texts. In
Advances in Neural Information Processing Systems, volume 36, 2024.
32

[64] A. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
[65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin.
Attention is all you need. In Advances in neural information processing systems, volume 30, 2017.
[66] A. J. Walker. New fast method for generating discrete random numbers with arbitrary frequency
distributions. Electronics Letters, 8(10):127â€“128, 1974.
[67] A. J. Walker. An efficient method for generating discrete random variables with general distributions.
ACM Transactions on Mathematical Software (TOMS), 3(3):253â€“256, 1977.
[68] D. Weber-Wulff, A. Anohina-Naumeca, S. Bjelobaba, T. Folt`ynek, J. Guerrero-Dib, O. Popoola, P. vSigut,
and L. Waddington. Testing of detection tools for AI-generated text. International Journal for Educational
Integrity, 19(1):26, 2023.
[69] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle,
A. Kasirzadeh, et al.
Ethical and social risks of harm from language models.
arXiv preprint
arXiv:2112.04359, 2021.
[70] Y. Wu, Z. Hu, H. Zhang, and H. Huang. DiPmark: A stealthy, efficient and resilient watermark for large
language models. arXiv preprint arXiv:2310.07710, 2023.
[71] M. Xia, T. Gao, Z. Zeng, and D. Chen. Sheared LLaMA: Accelerating language model pre-training via
structured pruning. In International Conference on Learning Representations, 2023.
[72] X. Yang, W. Cheng, L. Petzold, W. Y. Wang, and H. Chen. DNA-GPT: Divergent n-gram analysis for
training-free detection of GPT-generated text. arXiv preprint arXiv:2305.17359, 2023.
[73] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi. Defending against
neural fake news. In Advances in neural information processing systems, volume 32, 2019.
[74] ZeroGPT. ZeroGPT: Trusted GPT-4, ChatGPT and AI detector tool by ZeroGPT. https://www.
zerogpt.com/, 2023.
[75] H. Zhang, B. L. Edelman, D. Francati, D. Venturi, G. Ateniese, and B. Barak. Watermarks in the sand:
Impossibility of strong watermarking for generative models. arXiv preprint arXiv:2311.04378, 2023.
[76] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,
et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[77] X. Zhao, P. V. Ananth, L. Li, and Y.-X. Wang. Provable robust watermarking for AI-generated text. In
International Conference on Learning Representations, 2024.
[78] X. Zhao, L. Li, and Y.-X. Wang. Permute-and-Flip: An optimally robust and watermarkable decoder
for LLMs. arXiv preprint arXiv:2402.05864, 2024.
[79] X. Zhou, W. Zhao, Z. Wang, and L. Pan. Security theory and attack analysis for text watermarking. In
International Conference on E-Business and Information System Security, pages 1â€“6. IEEE, 2009.
[80] G. K. Zipf. Human behavior and the principle of least effort: An introduction to human ecology. Ravenio
books, 2016.
33

This Supplementary Material contains the remaining proofs and technical details. The proof
that supports the general framework is collected in Section A. The proofs about the Gumbel-max
watermark are presented in Section B. Section C includes the proofs of results for the inverse
transform watermark. Section D contains experiment details.
A
Proof Supporting the General Framework
A.1
Proof of Theorem 2.1
Proof of Theorem 2.1. We will use the Chernoff bound to analyze the type II error of Th. Recall
that the test introduced by h is
Th(Y1:n) =
 1
if Pn
t=1 h(Yt) â‰¥Î³n,Î±
0
if Pn
t=1 h(Yt) < Î³n,Î±,
By the strong law of large numbers, we know that as long as Î± âˆˆ(0, 1),
lim sup
nâ†’âˆ
Î³n,Î±
n
= E0 h(Y ).
(29)
We will prove this equation (29) at the end of the proof.
Let Pt denote the probability outputted by M at iteration t. By the condition that h is regular,
we have that for any feasible Î¸ â‰¥0,
E1,Pt exp(âˆ’Î¸h(Yt)) = exp(log Ï•Pt,h(Î¸)) â‰¤exp(log Ï•P,h(Î¸))
Using the last inequality, we have that for any feasible Î¸ â‰¥0,
1 âˆ’E1 Th(Y1:n) = P1
 n
X
t=1
âˆ’h(Yt) â‰¥âˆ’Î³n,Î±
!
â‰¤exp(Î³n,Î±Î¸) Â· EH1 exp
 n
X
t=1
âˆ’Î¸h(Yt)
!
(30)
â‰¤exp(Î³n,Î±Î¸) Â· exp(n Â· log Ï•P,h(Î¸)).
Therefore,
lim sup
nâ†’âˆ[1 âˆ’E1 Th(Y1:n)]1/n â‰¤lim sup
nâ†’âˆinf
Î¸â‰¥0 exp(Î¸Î³n,Î±/n) Â· exp(log Ï•P,h(Î¸))
â‰¤inf
Î¸â‰¥0 lim sup
nâ†’âˆexp(Î¸Î³n,Î±/n) Â· exp(log Ï•P,h(Î¸))
= inf
Î¸â‰¥0 exp(Î¸ E0 h(Y ) + log Ï•P,h(Î¸)).
Proof of (29). Under the null hypothesis, Yt are i.i.d. copies of Âµ0. By the strong law of large
numbers, it follows that
1
nk
nk
X
t=1
h(Yt) â†’E0 h(Y )
34

for any diverging subsequence {nk}kâ‰¥1 satisfying limkâ†’âˆnk = âˆ.
By definition, Î³n,Î± is the theoretical critical value that makes the detection rule Th have a given
Type I error of Î±, i.e.,
PH0
 n
X
t=1
h(Yt) â‰¥Î³n,Î±
!
= Î±.
Assume, for contradiction, that lim supnâ†’âˆ
Î³n,Î±
n
> E0[h(Y )]. Then we could find a small positive
number Ïµ > 0 and a diverging subsequence {nk}kâ‰¥1 such that
Î³nk,Î±
nk
â‰¥E0[h(Y )] + Îµ for sufficiently
large k. This leads to a contradiction, as follows:
Î± = lim
kâ†’âˆPH0
 nk
X
t=1
h(Yt) â‰¥Î³nk,Î±
!
= lim
kâ†’âˆPH0
 
1
nk
nk
X
t=1
h(Yt) â‰¥Î³nk,Î±
nk
!
â‰¤lim
kâ†’âˆPH0
 
1
nk
nk
X
t=1
h(Yt) â‰¥E0 h(Y ) + Îµ
!
= PH0 (E0 h(Y ) â‰¥E0 h(Y ) + Îµ) = 0.
Similarly, if lim infnâ†’âˆ
Î³n,Î±
n
< E0[h(Y )], we could find a small positive number Ïµ > 0 and a
diverging subsequence {nk}kâ‰¥1 such that
Î³nk,Î±
nk
â‰¤E0 h(Y ) âˆ’Îµ for sufficiently large k. This also leads
to a contradiction:
Î± = lim
kâ†’âˆPH0
 nk
X
t=1
h(Yt) â‰¥Î³nk,Î±
!
= lim
kâ†’âˆPH0
 
1
nk
nk
X
t=1
h(Yt) â‰¥Î³nk,Î±
nk
!
â‰¥lim
kâ†’âˆPH0
 
1
nk
nk
X
t=1
h(Yt) â‰¥E0 h(Y ) âˆ’Îµ
!
= PH0 (E0 h(Y ) â‰¥E0 h(Y ) âˆ’Îµ) = 1.
Remark A.1. We detail the tightness mentioned in Remark 2.3 in the following lemma.
Lemma A.1. If there exists some P in the closure of P which maximizes Ï•P ,h(Î¸) simultaneously for
all Î¸ â‰¥0, then (12) is tight in the sense that there exists P â‹†in P such that, if Pt = P â‹†for all t,
then
PH1(Th(Y1:n) = 0) â‰¥eâˆ’(RP(h)+Îµ)n
for any positive Îµ and sufficiently large n.
This condition regularizes P, h and Âµ1,P simultaneously. As we already see, it is satisfied by
both the Gumbel-max and inverse transform watermarks (see Lemma 3.3 and 4.6 for the details). It
implies our rates are tight in the least-favorable case.
35

Proof of Lemma A.1. To demonstrate the tightness of our results, we resort to the following lemma.
Lemma A.2 (Exchangeability). Let cl(P) denote the closure of P. For any Î¸ â‰¥0 and P âˆˆcl(P), we
define
Q(Î¸, P ) = Î¸ E0 h(Y ) + log E1,P eâˆ’Î¸h(Y ).
If there exists some P âˆ—âˆˆcl(P) such that Q(Î¸, P â‹†) = supP âˆˆcl(P) Q(Î¸, P ) for any Î¸ â‰¥0, then
sup
P âˆˆcl(P)
inf
Î¸â‰¥0 Q(Î¸, P ) = inf
Î¸â‰¥0
sup
P âˆˆcl(P)
Q(Î¸, P ).
Proof of Lemma A.2. First of all, by definition, we have
sup
P âˆˆcl(P)
inf
Î¸â‰¥0 Q(Î¸, P ) â‰¤inf
Î¸â‰¥0
sup
P âˆˆcl(P)
Q(Î¸, P ).
We then focus on the other direction. Because there exists some P âˆ—âˆˆcl(P) such that Q(Î¸, P â‹†) =
supP âˆˆcl(P) Q(Î¸, P ) for any Î¸ â‰¥0,
sup
P âˆˆcl(P)
inf
Î¸â‰¥0 Q(Î¸, P ) â‰¥inf
Î¸â‰¥0 Q(Î¸, P â‹†) = inf
Î¸â‰¥0
sup
P âˆˆcl(P)
Q(Î¸, P ).
By Lemma A.2, given that
RP(h) = âˆ’inf
Î¸â‰¥0
sup
P âˆˆcl(P)
Q(Î¸, P ),
we know that RP(h) can also be written as
RP(h) = âˆ’
sup
P âˆˆcl(P)
inf
Î¸â‰¥0 Q(Î¸, P ).
As a result, we can find a sequence of Pk âŠ‚P such that
inf
Î¸â‰¥0 Q(Î¸, Pk) â†’inf
Î¸â‰¥0
sup
P âˆˆcl(P)
Q(Î¸, P )
as k â†’âˆ; that is, RPk(h) â†’RP(h) as k â†’âˆ.
By standard results from the large deviation theory [18, 64], if we set all the NTP distribution
as Pk, then
lim
nâ†’âˆPH1(Th(Y1:n) = 0)1/n = eâˆ’RPk(h) â‰¥eâˆ’(RP(h)+Îµ(1)
k ),
(31)
where Îµ(1)
k
â†’0 as k â†’âˆ. It then follows that
PH1(Th(Y1:n) = 0) â‰¥eâˆ’n(RP(h)+Îµ(1)
k +Îµ(2)
n ),
where Îµ(2)
n
denotes a sequence of positive numbers approaching to zero as n â†’âˆto ensure (31) to
hold. We conclude that the lower bound holds by choosing a sufficiently large k.
36

A.2
Proof of Proposition 2.1
In this subsection, we present the proof of Proposition 2.1 and establish the equation that RSimp(W)(h) =
0.
Proof of Proposition 2.1. We start from the inequality (30). Since Î³-fraction of P1, . . . , Pn belongs
to P1 while the rest belongs to P2, it follows that
1 âˆ’E1 Th(Y1:n) â‰¤exp(Î³n,Î±Î¸) Â· exp(Î³n Â· log Ï•P1,h(Î¸)) + (1 âˆ’Î³)n Â· log Ï•P2,h(Î¸)).
Hence,
lim sup
nâ†’âˆ[1 âˆ’E1 Th(Y1:n)]1/n â‰¤
inf
Î¸âˆˆ[0,Î¸P,h) exp(Î¸ E0 h(Y ) + Î³ log Ï•P1,h(Î¸) + (1 âˆ’Î³) log Ï•P2,h(Î¸))
â‰¤exp(âˆ’[Î³RP1,h + (1 âˆ’Î³)RP2,h])
Lemma A.3. If Âµ0 âˆˆ{Âµ1,P : P âˆˆSimp(W)},
RSimp(W)(h) = 0.
Proof of Lemma A.3. On one hand, we always have RP(h) â‰¥0 for any P âŠ‚Simp(W). The reason
is given as follows. Recall that RP(h) = âˆ’inf
Î¸â‰¥0 f(Î¸) where f(Î¸) := Î¸ E0 h(Y ) + log Ï•P,h(Î¸). Clearly,
f(Î¸) is convex in Î¸ with f(0) = 0. Hence, we always have
RP,h â‰¥âˆ’f(0) = 0.
On the other hand, we have RSimp(W),h â‰¤0. The reason is given as follows. Note that RP,h is
monotone in P: if P1 âŠ‚P2, then RP2,h â‰¤RP1,h. If Âµ0 âˆˆ{Âµ1,P : P âˆˆSimp(W)}, without loss of
generality, we assume Âµ0 = Âµ1,P0. Since we must have {P0} âŠ‚Simp(W), it follows that
RSimp(W),h â‰¤RP0,h = 0.
A.3
Difficulties of Finding the Optimal Pivot Statistic
At the end of this section, we illustrate that searching for the optimal pivotal statistic is not generally
a convex problem. Overcoming these challenges would be significant and could be addressed in
future research endeavors.
First and foremost, from an optimization perspective, the problem of finding the optimal pivotal
statistic is in general non-convex. We will illustrate this non-convexity through concrete examples.
To set up the stage, we introduce some notation. For a given pivotal statistic Y , we first denote
J(Y ) = suph RP(h) to be the P efficiency for that pivotal statistic. We deduce
J(Y ) = sup
h
RP(h) = âˆ’inf
h sup
P âˆˆP

E0 h(Y ) + log E1,P eâˆ’h(Y )
= âˆ’sup
P âˆˆP
inf
h

Î¸ E0 h(Y ) + log E1,P eâˆ’h(Y )
37

= inf
P âˆˆP sup
h

âˆ’Î¸ E0 h(Y ) âˆ’log E1,P eâˆ’h(Y )
= inf
P âˆˆP DKL(Âµ0(Y )âˆ¥Âµ1,P (Y )).
The problem of finding the optimal pivotal statistic can be formally written down as:
maximize J(Y )
subject to
Y = Y (w, Î¶) is a pivotal statistics.
(32)
Here we recall the notation that w refers to the token, and Î¶ refers to the pseudorandom numbers.
We give examples to demonstrate that (i) the functional J is not convex nor concave, and (ii) the
constraint set Y being a pivotal statistic is not a convex set.
Consider the Gumbel-max watermark with two tokens, say, W = {1, 2} (our examples extend to
the general case |W| â‰¥2 by restricting the support of P to two). We denote the pseudorandom
number Î¶ = (U1, U2) where U1, U2 âˆ¼U(0, 1). Our examples take place in the simplest setting where
P = {P } is a singleton. In this setting, J(Y ) satisfies:
J(Y ) = DKL(Âµ0(Y )âˆ¥Âµ1,P (Y )).
Our first proposition states that J(Y ) is not necessarily convex or concave.
Proposition A.1 (The objective function is neither convex nor concave). There is a singleton set
{P} = {P } with Y1, Y2, Y3, Y4, each of which is a function of (w, Î¶), such that
J
Y1 + Y2
2

< J(Y1) + J(Y2)
2
and J
Y3 + Y4
2

> J(Y3) + J(Y4)
2
.
Our second proposition demonstrates that the set of pivot statistics is not a convex set.
Proposition A.2 (The constraint is not convex). There exist two pivotal statistics, Y1 and Y2, such
that Y1+Y2
2
is not a pivotal statistic.
These two factors contribute to the significant challenges in solving the underlying optimization
problem. We provide the proofs for the above two propositions in the following.
Proof of Proposition A.1. We introduce the following functions and parameters:
Y1 = Uw, Y2 = U1, Y3 = Uw + U1, Y4 = |Uw âˆ’U1|, and P1 = P2 = 1/2.
Table 1 presents some important calculation results which complete the proof.
Proof of Proposition A.2. Define Y1 = Uw and Y2 = U1. It is clear that Y1 and Y2 are pivotal
statistics, as their distributions are independent of the distribution of w under the null hypothesis H0.
In the following, we aim to compute the null CDF for Y1+Y2
2
. Under H0, we have w âˆ¼P = (P1, P2)
and w âŠ¥(U1, U2). As a result, it follows that for any r âˆˆ[0, 1],
PH0
Y1 + Y2
2
â‰¤r

= PH0
Uw + U1
2
â‰¤r

= P1 Â· P(U1 â‰¤r) + P2 Â· P(U1 + U2 â‰¤2r)
= P1 Â· r + P2 Â·
(
2r2 if r âˆˆ[0, 1/2],
1 âˆ’2(1 âˆ’r)2 if r âˆˆ[1/2, 1].
We conclude that Y1+Y2
2
is not a pivotal statistic because its null CDF depends on the distribution
of w, i.e., P = (P1, P2).
38

Function Y
F0(r)
F1(r)
J(Y ) = DKL(F0âˆ¥F1)
Y1 = Uw
r
r2
1 âˆ’log 2 â‰ˆ0.306
Y2 = U1
r
r
0
Y1+Y2
2
= Uw+U1
2
(
r
2 + r2 if r âˆˆ[0, 1/2]
5râˆ’1
2
âˆ’r2 if r âˆˆ[1/2, 1]
(
3r if r âˆˆ[0, 1/2]
2 âˆ’r if r âˆˆ[1/2, 1]
â‰ˆ0.0450295
or Y3 = Uw + U1
Y4 = |Uw âˆ’U1|
1 âˆ’(1âˆ’r)2
2
1 âˆ’(1âˆ’r)2
2
0
Y3+Y4
2
= max{Uw, U1}
r+r2
2
r2
â‰ˆ0.0996444
Table 1: CDFs under different hypotheses and the efficiency rates of the considered functions. Fi
represents the CDF under hypothesis Hi. The domain for all the CDFs is r âˆˆ[0, 1]. Here w = 1 if
and only if log U1
P1
â‰¥log U2
P2 . Recall that we use P1 = P2 = 1/2 in the proof of Proposition A.1.
B
Proof for Gumbel-max Watermarks
B.1
Analysis for the pivotal statistic Y gum
The distribution of Y gum
t
under the alternative hypothesis H1 has been studied by some existing
works [49, 22]. We include a proof for completeness.
Proof of Lemma 3.1. We assume Î¶ = (Uw)wâˆˆW with each Uw i.i.d. from U(0, 1). We split the
probability P(Uwt â‰¤r) into the sum of probability of |W| disjoint events as follows:
P(Uwt â‰¤r) =
X
wâˆˆW
P(Uwt â‰¤r, wt = w) =
X
wâˆˆW
P(Uw â‰¤r, wt = w).
By definition in (3), we have that
P(Uw â‰¤r, wt = w) = P(Uw â‰¤r, w = arg max
jâˆˆW U 1/Pt,j
j
)
= P(Uw â‰¤r, Uj â‰¤U Pt,j/Pt,w
w
for any j Ì¸= w)
=
Z r
0
duw
Z
[0,1]|W|âˆ’1 1
{ujâ‰¤u
Pt,j/Pt,w
w
for any jÌ¸=w}du1 . . . duwâˆ’1duw+1 . . . du|W|
=
Z r
0
Y
jÌ¸=w
uPt,j/Pt,w
w
duw
=
Z r
0
u1/Pt,wâˆ’1
w
duk = Pt,w Â· r1/Pt,w,
where the third equality follows from the independence of U1, . . . , U|W|.
At the end of this subsection, we argue that the pivotal statistic Y gum
t
= Y (wt, Î¶t) = Ut,wt in
our manuscript is a good pivotal statistic. Our justification is that it approximates well the most
optimistic likelihood ratio test statistic for an independent hypothesis test designed for watermark
detection.
Suppose we have one generated token (n = 1). Fix t (and we drop the notation dependence on t).
Consider the independence test regarding the token w and the pseudorandom variable Î¶. Under H0,
39

w and Î¶ are independent. Under H1, w and Î¶ are related by the decoder function S. More precisely,
we have the hypothesis testing problem:
H0 : w âŠ¥Î¶ versus H1 : w = S(P , Î¶) for some P .
This independent test is designed for watermark detection. We now compute the likelihood ratio
test statistic. Let Ï denote the probability density of Î¶.
â€¢ Under H0, the joint probability density of (w, Î¶) is given by Pw Â· Ï(Î¶).
â€¢ Under H1, the joint probability density of (w, Î¶) is given by 1w=S(P ,Î¶)Ï(Î¶).
The most optimistic log-likelihood ratio test statistic for this independent test is given by:
Î›(w, Î¶) = sup
P
log 1w=S(P ,Î¶)Ï(Î¶)
Pw Â· Ï(Î¶)
= sup
P
log 1w=S(P ,Î¶)
Pw
.
We refer to Î›(w, Î¶) as the most optimistic log-likelihood ratio test statistic because we take the
supremum over all possible P to eliminate its dependence. Generally, we would reject H0 if the
log-likelihood ratio is sufficiently large. By using this most optimistic log-likelihood ratio, we adopt
a more liberal approach when deciding to reject the null hypothesis.
We compute Î›(w, Î¶) for the Gumbel-max watermark. Recall the notation Î¶ = (Uw)wâˆˆW. We
note that w = Sgum(P , Î¶) holds if and only if:
log
1
Uw
Pw
â‰¤
log 1
Uk
Pk
for any
k âˆˆW.
As a result, if w = Sgum(P , Î¶) holds, then:
1 =
X
kâˆˆW
Pk â‰¤
X
kâˆˆW
log 1
Uk
log
1
Uw
Pw =â‡’Pw â‰¥
log
1
Uw
P
kâˆˆW log 1
Uk
.
Using this relation, we deduce that
Î›(w, Î¶) = log
P
kâˆˆW log 1
Uk
log
1
Uw
.
We argue that in the regime where |W| is large, this likelihood ratio statistic can be effectively
approximated by
Î›(w, Î¶) â‰ˆlog
|W|
log
1
Uw
as there is the law of large numbers where
1
|W|
P
kâˆˆW log 1
Uk â†’1 for i.i.d. Ui âˆ¼U(0, 1). Note, on
the RHS, log
|W|
log
1
Uw
is an increasing function of Uw, which is exactly the current pivotal statistic for
the Gumbel-max watermark.
To conclude, we have shown that the pivotal statistic Y gum
t
in our manuscript, up to a monotone
transform, approximates well the most optimistic likelihood ratio test statistic for the independent
hypothesis test. This supports the effectiveness of Y gum
t
as a pivotal statistic for watermark detection.
Thus, one contribution of our work is identifying the optimal transform h based on Y gum
t
that
achieves the highest possible statistical efficiency.
40

B.2
Proof of Theorem 3.1
Proof of Theorem 3.1. For every P âˆˆ[0, 1], let us define
GP (t) = P(V (P, U) â‰¤t).
Since max{V (P1, U1), V (P2, U2)} d= V (P1 + P2, U) holds, this then implies for every t âˆˆR:
GP1+P2(t) = P(V (P1 + P2, U) â‰¤t)
= P(max{V (P1, U1), V (P2, U2)} â‰¤t) = GP1(t)GP2(t)
(33)
must hold for every non-negative P1, P2 with P1 + P2 â‰¤1.
Hence, if we define ht(P) = âˆ’log GP (t), then ht is a solution to the following variant of Cauchyâ€™s
functional equation
f(P1 + P2) = f(P1) + f(P2) âˆ€P1, P2 â‰¥0, P1 + P2 â‰¤1.
Furthermore, by (33), for any 0 â‰¤P1 < P2 â‰¤1,
GP2(t) = GP1(t)GP2âˆ’P1(t) â‰¥GP1(t),
and thus ht(P) is monotone in P. Fix t > 0. By Lemma B.1 and using the fact that ht is measurable,
we obtain that for every P > 0
ht(P) = Î½(t)P
must hold for some Î½(t) âˆˆR. This means for every P âˆˆ(0, 1] and t âˆˆR
P(V (P, U) â‰¤t) = GP (t) = eÎ½(t)P .
Since t 7â†’GP (t) is monotonically increasing and right continuous, t 7â†’Î½(t) must be increasing and
right continuous. Let us introduce
Î½âˆ’1(t) = sup{y : Î½(y) < t}.
Then Î½âˆ’1 is increasing. Using the fact that Î½âˆ’1(t) â‰¤x is equivalent to t â‰¤Î½(x), the random variable
Î½âˆ’1(log U/P) has its distribution function given by GP , meaning that for P âˆˆ(0, 1]
V (P, U) d= Î½âˆ’1(log U/P).
Lemma B.2 shows that the function Î½âˆ’1 must be strictly increasing in order for
P(Î½âˆ’1(log U1/P1) â‰¤Î½âˆ’1(log U2/P2)) = P1/(P1 + P2)
to hold. Note also V (0, U) = âˆ’âˆmust hold almost surely. The conclusion is, by picking g with
g(z) = Î½âˆ’1(z) when z > âˆ’âˆand g(âˆ’âˆ) = âˆ’âˆ, V (P, U) d= g(log U/P) holds for every P âˆˆ[0, 1].
Lemma B.1. If a monotone function f : [0, 1] â†’R satisfies that f(P1 + P2) = f(P1) + f(P2) for
any P1, P2 â‰¥0, P1 + P2 â‰¤1, then for any P âˆˆ[0, 1],
f(P) = Pf(1).
41

Proof of Lemma B.1. For any P1, P2 â‰¥0, P1 + P2 â‰¤1, f(P1 + P2) = f(P1) + f(P2) implies that
f(0) = 0,
f
 j
k

= j
kf(1)
for any j, k âˆˆN and j â‰¤k. This gives that f(p) = pf(1) for any rational number p âˆˆ[0, 1].
Suppose that f(r0) = r0f(1) is not true for some irrational r0 âˆˆ[0, 1].
Then we have either
f(r0) > Î·f(1) or f(r0) < r0f(1). In the former case we can find rational numbers r1, r2 such that
r1 < r0 < r2 < f(r0)/f(1) and consequently
f(r1) = r1f(1) < r0f(1) < f(r0)
and
f(r2) = r2f(1) < f(r0).
It immediately follows that {f(r0) âˆ’f(r1)}{f(r0) âˆ’f(r2)} > 0, which is, however, contradictory to
the monotonicity of f. The latter case of f(r0) < r0f(1) can be proved using similar arguments. We
then conclude that f(r0) = r0f(1) holds for all irrational number r0 âˆˆ[0, 1], and hence f(p) = pf(1)
for all p âˆˆ[0, 1].
Lemma B.2. Assume Î½ : R â†’R is non-decreasing, right continuous, and satisfies P(Î½âˆ’1(log U1/P1) â‰¤
Î½âˆ’1(log U2/P2)) = P1/(P1 + P2) for U1, U2
i.i.d.
âˆ¼U(0, 1) and any P1, P2 > 2. Then Î½âˆ’1 is strictly
increasing, i.e., we have Î½âˆ’1(t1) < Î½âˆ’1(t2) for any t1 < t2.
Proof of Lemma B.2. If Î½âˆ’1 is not strictly increasing, then there exists some t1 < t2 such that
Î½âˆ’1(t1) = Î½âˆ’1(t2). Hence, we have that
P

Î½âˆ’1
log U1
P1

â‰¤Î½âˆ’1
log U2
P2

= P
log U1
P1
â‰¤log U2
P2

+ P

Î½âˆ’1
log U1
P1

â‰¤Î½âˆ’1
log U2
P2

, log U1
P1
> log U2
P2

â‰¥P
log U1
P1
â‰¤log U2
P2

+ P

t2 â‰¥log U1
P1
> log U2
P2
â‰¥t1

>
P1
P1 + P2
,
which is contradictory to the condition that P

Î½âˆ’1 
log U1
P1

â‰¤Î½âˆ’1 
log U2
P2

=
P1
P1+P2 .
B.3
Analysis of Different Score Functions
In this subsection, we analyze different score functions and provide all the necessary information for
computing the vanilla efficiency rate RP (h). We consider four score functions: the log-likelihood
ratio hllr, Aaronsonâ€™s score function hars, log function hlog, and the indicator function (Î´ = 0.5).
Recall that from Lemma 3.1, fP is the PDF of Y gum
t
if the underlying NTP distribution is Pt = P .
Remark B.1. hllr is the log-likelihood ratio and hlog can be viewed as a relaxed log-likelihood ratio.
This is because hlog can be derived by applying Jensenâ€™s inequality to lower bound hllr. Indeed, it
follows that for any r âˆˆ[0, 1],
log fP (r) = log
X
wâˆˆW
r1/Pwâˆ’1 â‰¥
X
wâˆˆW
Pw log
 
r1/Pwâˆ’1
Pw
!
= (|W| âˆ’1) log r + Ent(P ).
If we replace log f1,P (r) with (|W| âˆ’1) log r + Ent(P ) and ensure the resulting test is of Î±-level, we
obtain the test introduced by hlog. Hence, hlog is the probability-agnostic relaxation of hllr.
42

0.00
0.25
0.50
0.75
1.00
Y1
0.0
0.2
0.4
0.6
0.8
1.0
Y2
Rejection region (Î± = 0.05, p1 = 0.2)
hllr
hars
hlog
hind,0.5
0.0
0.2
0.4
0.6
0.8
1.0
p1
0.00
0.05
0.10
0.15
0.20
0.25
0.30
RP(h)
Eï¬ƒciency
hllr
hars
hlog
hind,0.5
0.0
0.2
0.4
0.6
0.8
1.0
Type I error Î±
0.0
0.2
0.4
0.6
0.8
1.0
Type II error Î²
Trade-oï¬€function (p1=0.4)
hllr
hars
hlog
hind,0.5
Figure 8: The rejection region (left), test efficiency (middle), and trade-off function (right) of different
score functions when |W| = 2 and n = 2.
Score function h
E0 h(Y )
Ï•P ,h(Î¸)
RP (h)
hllr(y) = log fP (y)
âˆ’DKL(Âµ0, Âµ1,P )
R 1
0 f1âˆ’Î¸
P
(y)dy
DKL(Âµ0, Âµ1,P )
hars(y) = âˆ’log(1 âˆ’y)
1
P
wâˆˆW B(1/Pw, Î¸ + 1)
âˆ’inf
Î¸â‰¥0 [Î¸ + log Ï•P ,hars(Î¸)]
hlog(y) = log y
âˆ’1
P
wâˆˆW
Pw
1âˆ’PwÎ¸
sup
Î¸âˆˆ[0,
1
P(1) )
[Î¸ âˆ’log Ï•P ,hlog(Î¸)]
hind,Î´(y) = 1{yâ‰¥Î´}
1 âˆ’Î´
F(Î´) + eâˆ’Î¸(1 âˆ’F(Î´))
Î´ log
Î´
F(Î´) + (1 âˆ’Î´) log
1âˆ’Î´
1âˆ’F(Î´)
Table 2: The efficiency component RP (h) of different function hâ€™s. Here B(x, y) :=
R 1
0 rxâˆ’1(1 âˆ’
r)yâˆ’1dr is the Beta function.
A simple case study
To visualize the differences among the tests induced by these hâ€™s, we
examine a two-sample two-token case (i.e., n = 2 and |W| = 2). The left panel in Figure 8 depicts
the Î±-level rejection region for different hâ€™s. The rejection region defined by hllr, hlog, hind,Î´ exhibits
a more concentrated pattern around the corner (1, 1), in contrast to the elongated and narrow shape
defined by hars. Furthermore, the rejection regions of hllr and hlog share almost the same shape.
Formula to compute MGFs
The following proposition provides all necessary information for
computing RP (h) which we summarize in Table 2. Note that they are all non-decreasing, so Lemma
3.3 could apply.
Proposition B.1.
â€¢ For hllr(y) = log fP (y), âˆ’E0 hllr(Y ) = RP (hllr) = DKL(Âµ0, Âµ1,P ).
â€¢ For hars(y) = âˆ’log(1 âˆ’y), E0 hars(Y ) = 1 and Ï•P ,hars(Î¸) = P
wâˆˆW B(1/Pw, Î¸ + 1) where
B(a, b) =
R 1
0 yaâˆ’1(1 âˆ’y)bâˆ’1dy is the beta function. Ï•P ,hars(Î¸) is well-defined on [0, âˆ) and
is strictly decreasing with Ï•P ,hars(0) = âˆand Ï•P (hars)(âˆ) = 0. Hence, RP (hars) = âˆ’Î¸â‹†
1 âˆ’
log Ï•hars(Î¸â‹†
1) where Î¸â‹†
1 is the unique solution of the equation Ï•P ,hars(Î¸) + Ï•â€²
hars(Î¸) = 0.
â€¢ For hlog(y) = log y, E0 hlog(Y ) = âˆ’1 and Ï•P ,hlog(Î¸) = P
wâˆˆW
Pw
1âˆ’PwÎ¸ which is well-defined
on [0, 1/P(1)). Hence, RP (hlog) = Î¸â‹†
2 âˆ’log Ï•P ,hlog(Î¸â‹†
2) where Î¸â‹†
2 is the unique solution of the
43

equation Ï•P ,hlog(Î¸) = Ï•â€²
P ,hlog(Î¸).
â€¢ For hind,Î´(y) = 1{yâ‰¥Î´} for some Î´ âˆˆ(0, 1), E0 hind,Î´(Y ) = 1 âˆ’Î´ and Ï•P ,hind,Î´(Î¸) = F(Î´) +
eâˆ’Î¸(1âˆ’F(Î´)) which is well-defined on [0, âˆ). Hence, RP (hind,Î´) = Î´ log
Î´
F(Î´) +(1âˆ’Î´) log
1âˆ’Î´
1âˆ’F(Î´).
B.4
Proof of Lemma 3.4
Proof of Lemma 3.4. The set of extremal points of Pâˆ†is denoted by Ext(Pâˆ†) where, by definition,
an element does not lie between any two distinct points in Pâˆ†. Recall that P â‹†
âˆ†is defined as:
P â‹†
âˆ†=

1 âˆ’âˆ†, . . . , 1 âˆ’âˆ†
|
{z
}
âŒŠ
1
1âˆ’âˆ†âŒ‹times
, 1 âˆ’(1 âˆ’âˆ†) Â·

1
1 âˆ’âˆ†

, 0, . . .

.
We define Î (P â‹†
âˆ†) as the set of all NTP distributions that can be obtained by permuting P â‹†
âˆ†, where:
Î (P â‹†
âˆ†) := {Ï€(P â‹†
âˆ†) : Ï€ is a permutation on {1, 2, . . . , |W|}} ,
and Ï€(P ) denotes the permuted NTP distribution with the ith coordinate given by PÏ€(i). Our goal
is to prove that
Ext(Pâˆ†) = Î (P â‹†
âˆ†).
To start, we show that Î (P â‹†
âˆ†) âŠ†Ext(Pâˆ†), meaning every permuted NTP distribution of P â‹†
âˆ†is
an extremal point. Suppose the contrary, that there exists P â‹†
âˆ†= Î¸P1 + (1 âˆ’Î¸)P2 for some distinct
P1, P2 âˆˆPâˆ†and Î¸ âˆˆ(0, 1). Given the first âŒŠ
1
1âˆ’âˆ†âŒ‹coordinates of P â‹†
âˆ†are maximal, both P1 and P2
must reach the maximum in these coordinates, leading to a contradiction with the assumption that
P1 Ì¸= P2.
Next, we prove Ext(Pâˆ†) âŠ†Î (P â‹†
âˆ†), indicating every extremal point is obtained by permuting
P â‹†
âˆ†. It suffices to demonstrate that any point in Pâˆ†is in the convex hull of Î (P â‹†
âˆ†), denoted
by Conv(Î (P â‹†
âˆ†)). Without loss of generality, we pick up any P âˆˆPâˆ†with non-increasing order
P1 â‰¥P2 â‰¥Â· Â· Â· â‰¥P|W| and m coordinates equal to 1 âˆ’âˆ†.
In the following, we use induction to show:
Any NTP distribution in Pâˆ†with exactly m coordinates equal to 1 âˆ’âˆ†is in Conv(Î (P â‹†
âˆ†)).
â€¢ If m = âŒŠ
1
1âˆ’âˆ†âŒ‹, then P must assign the remaining probability mass (which is 1 âˆ’m(1 âˆ’âˆ†)) on
some of the rest coordinates. We assume these coordinates are contained in the set I satisfying
P
iâˆˆI Pi = 1 âˆ’m(1 âˆ’âˆ†). It is easy to find that P is the convex interpolation of |I| points in
Î (P â‹†
âˆ†) whose first N coordinates are the same as 1 âˆ’âˆ†and the ith coordinate is 1 âˆ’N(1 âˆ’âˆ†).
â€¢ Suppose that we already show the statement holds for m â‰¤âŒŠ
1
1âˆ’âˆ†âŒ‹, we now need to show
that it also holds for m âˆ’1. For notation simplicity, we let Pm âŠ‚Pâˆ†denote the set of NTP
distributions which having exact m coordinates equal to 1 âˆ’âˆ†. Let P âˆˆPmâˆ’1 be the NTP
distribution under consideration. Given its coordinates are decreasingly ranked, we must have
1 âˆ’âˆ†> Pm â‰¥Pm+1 â‰¥Â· Â· Â· â‰¥P|W|.
To proceed with the proof, we discuss two cases.
44

(a) If Pm + Pm+1 > 1 âˆ’âˆ†, we assert we must have P âˆˆConv(Pm). To see this, we construct
two NTP distributions P1 Ì¸= P2 âˆˆPm such that P lies in the segment between the two
points. These two points differ from P in exactly two coordinates:
P1,i = P2,i = Pi for i /âˆˆ{m, m + 1},
P1,m = 1 âˆ’âˆ†and P1,m+1 = Pm + Pm+1 âˆ’(1 âˆ’âˆ†),
P2,m = P1,mâˆ’1, P2,m+1 = P1,m.
By the hypothesis, we already have Pm âŠ‚Conv(Î (P â‹†
âˆ†)) so that P âˆˆConv(Î (P â‹†
âˆ†)).
(b) If Pm + Pm+1 â‰¤1 âˆ’âˆ†, we similarly construct two NTP distributions P1 Ì¸= P2 âˆˆPmâˆ’1
such that P lies in the segment between the two points. These two points differ from P
in exactly two coordinates:
P1,i = P2,i = Pi for i /âˆˆ{m, m + 1},
P1,m = Pm + Pm+1 and P1,m+1 = 0,
P2,m = 0, P2,m+1 = Pm + Pm+1.
In this case, though P1 and P2 are still in Pmâˆ’1, their support decreases by one when
compared with P . We then discuss the remaining coordinates of P1 and P2. Take P1
for example. We want to determine whether the sum of the largest two non-(1 âˆ’âˆ†)
probabilities in P1 is larger than 1 âˆ’âˆ†or not. If this is true, we arrive at Case (a) again
but with a new instance, namely P1, which has already been addressed. If this is not
true, we can further decrease the support of P1 by treating it as a new instance in Case
(b) and combining these two largest non-(1 âˆ’âˆ†) coordinates. Because P1 has a finite
number of non-(1 âˆ’âˆ†) coordinates, there is only a finite number of cases where we have
to deal with Case (b). As a result, by tracing the way we decompose P1, we always have
that P1 âˆˆConv(Î (P â‹†
âˆ†)) and similarly P2 âˆˆConv(Î (P â‹†
âˆ†)).
Combining Case (a) and Case (b), we prove the correctness of the statement.
B.5
Results for a General Probability Set
In the following, we consider a new, general distribution class Pâˆ†1,âˆ†2.
For two real numbers
0 â‰¤âˆ†2 < âˆ†1 < 1, we define the following probability class:
Pâˆ†1,âˆ†2 =

P âˆˆSimp(W) : P(1) â‰¤1 âˆ’âˆ†1, P(2) â‰¤1 âˆ’âˆ†2
	
,
(34)
where P(i) is the i-th largest probability in the NTP distribution P = (P1, Â· Â· Â· , P|W|). Notably,
Pâˆ†1,âˆ†2 is invariant under permutation, meaning that for any permutation Ï€, if P âˆˆPâˆ†1,âˆ†2, then
Ï€(P ) := (PÏ€(1), . . . , PÏ€(|W|)) also belongs to Pâˆ†1,âˆ†2. Furthermore, Pâˆ†1,âˆ†2 is a convex polytope,
defined as a compact convex set with a finite number of extreme points. The permutation invariance
allows us to precisely capture its set of extreme points.
Specifically, we establish the following lemma:
45

Lemma B.3. For 0 â‰¤âˆ†2 < âˆ†1 < 1, the set of extreme points of Pâˆ†1,âˆ†2 is
Ext(Pâˆ†1,âˆ†2) = Î (P â‹†
âˆ†1,âˆ†2)
where Î (P â‹†
âˆ†1,âˆ†2) :=
n
Ï€(P â‹†
âˆ†1,âˆ†2) : Ï€ is a permutation on {1, 2, . . . , |W|}
o
and P â‹†
âˆ†1,âˆ†2 is the least-
favorable NTP distribution in Pâˆ†1,âˆ†2 defined by
P â‹†
âˆ†1,âˆ†2 =

1 âˆ’âˆ†1, 1 âˆ’âˆ†2, . . . , 1 âˆ’âˆ†2
|
{z
}
âŒŠ
âˆ†1
1âˆ’âˆ†2 âŒ‹times
, âˆ†1 âˆ’(1 âˆ’âˆ†2) Â·

âˆ†1
1 âˆ’âˆ†2

, 0, . . .

.
Following a similar line of reasoning as in our main analysis, we could show that
hgum,âˆ†1,âˆ†2(r) = log
dÂµ1,P â‹†
âˆ†1,âˆ†2
dÂµ0
(r)
is the optimal score function for the Gumbel-max watermark when using Pâˆ†1,âˆ†2 as the prior
probability set.
However, this probability set is less practical because it requires specifying two real numbers
âˆ†1, âˆ†2. Therefore, we include this discussion as an extension, illustrating how our analysis and
method can be extended to other permutation-invariant sets P. In the main text, we focus on the
âˆ†-regular prior set Pâˆ†, which represents a special case of Pâˆ†1,âˆ†2. This case occurs when we set
âˆ†1 = âˆ†and âˆ†2 = 0, making it perhaps the simplest form of this class.
Proof of Lemma B.3. The proof strategy closely follows the approach used in Lemma 3.4, with a
few key distinctions that we detail here.
First, it is straightforward to see that P â‹†
âˆ†1,âˆ†2 and its permutations are all extreme points since
they reach the boundary conditions P(1) â‰¤1 âˆ’âˆ†1 and P(2) â‰¤1 âˆ’âˆ†2. This result can be easily
verified by using the definition of extreme points so we omit them for simplicity. Consequently, we
have Î (P â‹†
âˆ†1,âˆ†2) âŠ†Ext(Pâˆ†1,âˆ†2).
To prove the reverse inclusion, Ext(Pâˆ†1,âˆ†2) âŠ†Î (P â‹†
âˆ†1,âˆ†2), it suffices to demonstrate that any
point in Pâˆ†1,âˆ†2 lies within the convex hull of Î (P â‹†
âˆ†1,âˆ†2). This can be shown by applying the
following lemma. By the terminology introduced in Lemma B.4, we know that P â‹†
âˆ†1,âˆ†2 majorizes
any point in Pâˆ†1,âˆ†2, implying that any point in Pâˆ†1,âˆ†2 can be expressed as a convex combination
of P â‹†
âˆ†1,âˆ†2 and its permutations. Thus, the proof is complete.
Lemma B.4 (Lemma 2.2 in [34]). Given two vectors x, y âˆˆRd, we say x majorizes y, a property
we denote by x â‰¥m y, if Pj
i=1 x(i) â‰¥Pj
i=1 y(i) for all j = 1, . . . , d and Pd
i=1 x(i) = Pd
i=1 y(i). Here
x(i) is the i-th largest component of x. If x â‰¥m y, then y is a convex combination of x and its
permutations.
B.6
Proof of Theorem 3.3
Theorem B.1 (Formal version of Theorem 3.3). There exist two positive constants 0.001 < âˆ†gumb
1
â‰¤
âˆ†gumb
2
< 1 such that
46

â€¢ When 10âˆ’3 < âˆ†< âˆ†gumb
1
,
max

RPâˆ†(hlog), RPâˆ†(hind,eâˆ’1)
	
< RPâˆ†(hars).
â€¢ When âˆ†gumb
2
< âˆ†< 1,
max

RPâˆ†(hars), RPâˆ†(hind,eâˆ’1)
	
< RPâˆ†(hlog).
â€¢ From Figure 4, we observe that âˆ†gumb
1
= âˆ†gumb
2
â‰ˆ0.17756080525215662.
Proof of Theorem B.1. By Lemma 3.3, for any feasible âˆ†âˆˆ[0, |W|âˆ’1
|W| ], the efficiency exponents
RPâˆ†(h) are functions of âˆ†, with the functional forms being
RPâˆ†(hars) = âˆ’inf
Î¸â‰¥0
h
Î¸ + log Ï•P â‹†
âˆ†,hars(Î¸)
i
:= Rhars(âˆ†),
RPâˆ†(hlog) =
sup
Î¸âˆˆ[0,
1
1âˆ’âˆ†)
[Î¸ âˆ’log Ï•P â‹†
âˆ†,hlog(Î¸)] := Rhlog(âˆ†),
RPâˆ†(hind,Î´) = Î´ log
Î´
F1,P â‹†
âˆ†(Î´) + (1 âˆ’Î´) log
1 âˆ’Î´
1 âˆ’F1,P â‹†
âˆ†(Î´) := Rhind,Î´(âˆ†),
where definitions of Ï•P â‹†
âˆ†,hars, Ï•P â‹†
âˆ†,hlog, and Ï•P â‹†
âˆ†,hind,Î´ can be found in Table 2 and P â‹†
âˆ†is the least-
favorable NTP distribution defined in (19). Here, we slightly abuse the notion and rewrite each
RPâˆ†(h) as a function of âˆ†for simplicity.
We take the comparison between Rhars(âˆ†) and Rhlog(âˆ†) as an example since the other comparisons
follow similarly. When setting âˆ†= |W|âˆ’1
|W| , we note that P â‹†
âˆ†is the uniform distribution over W. In
this case,
Rhars
|W| âˆ’1
|W|

< DKL(Âµ0, Âµ1,P â‹†
âˆ†) = Rhlog
|W| âˆ’1
|W|

< âˆ,
where the inequality follows from the Neyman-Pearson lemma and the equation uses the observation
that hâ‹†
âˆ†is reduced to hlog if âˆ†= |W|âˆ’1
|W| . Note that Rhars and Rhlog are continuous with respect
to âˆ†. By the continuity, we conclude that there exists a constant 0 < âˆ†gumb
2
â‰¤|W|âˆ’1
|W|
such that
Rhars(âˆ†) < Rhlog(âˆ†) < âˆholds for all |W|âˆ’1
|W|
â‰¥âˆ†â‰¥âˆ†gumb
2
.
For the other direction, when setting âˆ†= 0.01, we numerically find that Rhlog(0.001) â‰ˆ3âˆ’6 and
Rhars(0.001) â‰ˆ2.5 Ã— 10âˆ’5. By the continuity, there exists a positive constant 10âˆ’3 < âˆ†gumb
1
such
that Rhlog(âˆ†) < Rhars(âˆ†) for all 10âˆ’3 < âˆ†< âˆ†gumb
1
.
Remark B.2. The constant 10âˆ’3 used in the above proof is not essential. For a more accurate
characterization, we use the python SciPy package to solve the root of RPâˆ†(hars) = RPâˆ†(hlog), the
numerical result suggests that there is only one root whose value is 0.17756080525215662 when
âˆ†â‰¥0.001.
47

B.7
Analysis of Efficiency Gap
Lemma B.5. For the probability vector P , let |supp(P )| denote the size of the support of P where
supp(P ) := {w âˆˆW : Pw Ì¸= 0}. If P(1) < 1, then
1 âˆ’min
(
P(1)
1 âˆ’P(1)
log
1
P(1)
,
|supp(P )| âˆ’
1
P(1)
|supp(P )| âˆ’1 âˆ’log |supp(P )|
)
â‰¤RP (hlog)
RP (hllr) â‰¤1.
Corollary B.1 (Ignorable suboptimality).
lim
âˆ†â†’1
RPâˆ†(hlog)
RPâˆ†(hâ‹†
gum,âˆ†) â†’1.
Proof of Corollary B.1. By Lemma 3.3, we have that for non-decreasing function h,
RPâˆ†(h) = RP â‹†
âˆ†(h)
where P â‹†
âˆ†is the least-favorable NTP distribution defined in (19). This corollary directly follows from
Lemma B.5 by noting the largest probability in P â‹†
âˆ†is 1 âˆ’âˆ†which converges to zero if âˆ†â†’1.
At the end, we provide the proof of Lemma B.5.
Proof of Lemma B.5. Let P(1) denote the largest probability in P . By definition, we have RP (h) â‰¤
DKL(Âµ0, Âµ1,P ) for any score function h. We then focus on the other direction. On one hand, it
follows that
RP (hlog) =
sup
Î¸âˆˆ[0,1/P(1))
 
Î¸ âˆ’log
X
wâˆˆW
Pw
1 âˆ’PwÎ¸
!
â‰¥
sup
Î¸âˆˆ[0,1/P(1))
 Î¸ + log(1 âˆ’P(1)Î¸)

=
1
P(1)
âˆ’1 âˆ’log
1
P(1)
.
The last equality uses the condition P(1) < 1 so that the supreme is obtained within [0, 1/P(1)). On
the other hand, note that lim
pâ†’0 r1/pâˆ’1 = 0 for any r âˆˆ(0, 1) and thus
DKL(Âµ0, Âµ1,P ) = âˆ’
Z 1
0
log
X
wâˆˆW
r1/Pwâˆ’1dr
â‰¤min

âˆ’
Z 1
0
log r1/P(1)âˆ’1dr, âˆ’
Z 1
0
(|supp(P )| âˆ’1) log rdr âˆ’log |supp(P )|

= min

1/P(1) âˆ’1, |supp(P )| âˆ’1 âˆ’log |supp(P )|
	
.
The particular inequality uses the following lemma. This lemma can be proven by computing the
derivative of DKL(Âµ0, Âµ1) with respect to the probability vector P = (p1, Â· Â· Â· , Pw).
48

Lemma B.6. Let |supp(P )| denote the cardinality of the support of P , then
0 â‰¤DKL(Âµ0, Âµ1,P ) = âˆ’
Z 1
0
log
 X
wâˆˆW
r1/Pwâˆ’1
!
dr â‰¤|supp(P )| âˆ’1 âˆ’log |supp(P )|.
The left inequality is achieved when Pw = 1 for some w âˆˆW and the right inequality is achieved
when Pw â‰¡1/|supp(P )| for w âˆˆsupp(P ).
Proof of Lemma B.6. The left inequality is obvious due to the non-negativeness of KL divergence.
The right inequality follows from Jensenâ€™s inequality. See Remark B.1 for the details.
We complete the proof by noting
1
P(1) âˆ’1 âˆ’log
1
P(1)
|supp(P )| âˆ’1 âˆ’log |supp(P )| â‰¥1 âˆ’
|supp(P )| âˆ’
1
P(1)
|supp(P )| âˆ’1 âˆ’log |supp(P )|.
B.8
Pâˆ†-Efficiency for the Baby Watermark
Lemma B.7. For the baby watermark in (1), the summary function Y (wt, Î¶t) = (2wt âˆ’1)(2Î¶t âˆ’1)
and the score function hid(y) = y, the efficiency exponent for the belief class Pâˆ†= {P = (P0, P1) :
âˆ†â‰¤min(P0, P1) â‰¤1/2} is
RPâˆ†(hid) = âˆ’inf
Î¸â‰¥0 log
1
Î¸
eÎ¸(1âˆ’2âˆ†) + eâˆ’Î¸(1âˆ’2âˆ†)
2
âˆ’eâˆ’Î¸

.
Proof of Lemma B.7. With the choice of (Y, h), we obtain the detection rule as in (10). By Proposi-
tion 2.1, the efficiency of this detection rule is measured by RP,h defined in (13). Note that by the
independence of wt and Î¶t under the null, we have E0 h(Y ) = 0. Thus, the efficiency exponent can
be written as
RP(hid) = âˆ’inf
Î¸â‰¥0 sup
P âˆˆP
log E1,P

eâˆ’Î¸Y (w,Î¶)
.
To calculate RP,h, we need first to obtain the distribution of Y (w, Î¶).
Lemma B.8. Under H1, the distribution of Y (w, Î¶) is
PH1(Y â‰¤y) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
0
y â‰¤2Pmin âˆ’1,
(y + 1)/2 âˆ’Pmin
2Pmin âˆ’1 < y â‰¤1 âˆ’2Pmin,
y
1 âˆ’2Pmin < y â‰¤1,
1
y > 1,
where Pmin = min{P0, P1}, P1 = P(w = 1) and P0 = 1 âˆ’P1.
Proof of Lemma B.8. By the Bayes rule, we have
P(Y â‰¤y) = P(Y â‰¤y | w = 1)P(w = 1) + P(Y â‰¤y | w = 0)P(w = 1)
= P

Î¶ â‰¤y + 1
2
w = 1

(1 âˆ’P0) + P

Î¶ â‰¥1 âˆ’y
2
w = 0

P0.
49

Note that for y < âˆ’1, (y +1)/2 < 0 and (1âˆ’y)/2 > 1 and for y > 1, (y +1)/2 > 1 and (1âˆ’y)/2 < 0.
Thus, P(Y â‰¤y) = 0 for y < âˆ’1 and P(Y â‰¤y) = 1 for y > 1. For âˆ’1 â‰¤y â‰¤1, it suffices to calculate
P(Î¶ â‰¤a | w = 1) and P(Î¶ â‰¤a | w = 0) for any âˆ’1 â‰¤a â‰¤1. By the Bayes rule,
P(Î¶ â‰¤a | w = 1) = P(w = 1 | Î¶ â‰¤a)P(Î¶ â‰¤a)
P(w = 1)
= max(0, a âˆ’P0)
a
a
1 âˆ’P0
= max

0, a âˆ’P0
1 âˆ’P0

.
Similarly,
P(Î¶ â‰¤a | w = 0) = min

1, a
P0

.
Combining these pieces, we obtain
PH1(Y â‰¤y) = max

0, y + 1
2
âˆ’P0

+ max

0, y âˆ’1
2
+ P0

.
If P0 â‰¤1/2, then the distribution can be further simplified as
PH1(Y â‰¤y) = y Â· 1{1â‰¥y>1âˆ’2P0}y +
y + 1
2
âˆ’P0

Â· 1{1âˆ’2P0â‰¥yâ‰¥2P0âˆ’1]}
(35)
for âˆ’1 â‰¤y â‰¤1. If P0 â‰¥1/2, (35) holds with P0 being replaced by P1. We then complete the
proof.
By Lemma B.8, when âˆ†â‰¤0.5, the MGF is given by
Ï•P ,h(Î¸) = E1,P eâˆ’Î¸Y = 1
Î¸
eÎ¸(1âˆ’2Pmin) + eâˆ’Î¸(1âˆ’2Pmin)
2
âˆ’eâˆ’Î¸

.
Note that Ï•P ,h(Î¸) is convex with respect to P for any Î¸ â‰¥0. Moreover, for Pâˆ†= {P = (P0, P1) :
âˆ†â‰¤min(P0, P1) â‰¤1/2}, it can be easily verified that Ï•P ,h(Î¸) attains its maximum as a function of
Pmin when Pmin = âˆ†, in which case
RPâˆ†(hid) = âˆ’inf
Î¸â‰¥0 log
1
Î¸
eÎ¸(1âˆ’2âˆ†) + eâˆ’Î¸(1âˆ’2âˆ†)
2
âˆ’eâˆ’Î¸

.
C
Proof for Inverse Transform Watermarks
C.1
Distribution Characterization
The first step in applying the framework of class-dependent efficiency is to characterize the distri-
butions of Y dif
t
under both null and alternative hypotheses. Note that it is a bivariate function of
(Ut, Ï€t(wt)). We first focus on the joint distribution of (Ut, Ï€t(wt)) instead.
Lemma C.1. Let r âˆˆ[0, 1] and w âˆˆW. Under H0, the joint CDF of (Ut, Ï€t(wt)) is:
P(Ut â‰¤r, Ï€t(wt) = w|H0) =
r
|W|.
50

Under H1, the joint CDF of (Ut, Î·(Ï€t(wt))) conditioning on Pt is:
P(Ut â‰¤r, Ï€t(wt) = w|Pt, H1) =
1
|W|!
X
Ï€âˆˆÎ 
P (Ut âˆˆ(aÏ€,wâˆ’1, aÏ€,w] âˆ©[0, r]) ,
where aÏ€,w = Pw
j=1 Pt,Ï€(j) is the sum of the first w probabilities of Pt under the permutation Ï€.
Lemma C.1 provides the explicit formulation for the distribution of (Ut, Î·(Ï€t(wt))). Using Lemma
C.1, one can apply the change of variables to compute the exact distribution for Y dif
t
and thus prove
Lemma 4.1.
Proof of Lemma 4.1. It follows from Lemma C.1 that
P(Y dif
t
â‰¤r|H0) =
X
wâˆˆW
P(|Ut âˆ’Î·(w)| â‰¤r, Ï€t(wt) = w|H0)
=
X
wâˆˆW
P(Ut âˆˆ[Î·(w) âˆ’r, Î·(w) + r]
\
[0, 1], Ï€t(wt) = w|H0)
=
1
|W|
X
wâˆˆW

[Î·(w) + r][0,1] âˆ’[Î·(w) âˆ’r][0,1]

.
On the other hand,
P(Y dif
t
â‰¤r|Pt, H1) =
X
wâˆˆW
P(|Ut âˆ’Î·(w)| â‰¤r, Ï€t(wt) = w|Pt, H1)
=
1
|W|!
X
Ï€âˆˆÎ 
X
wâˆˆW
Î»((aÏ€,wâˆ’1, aÏ€,w] âˆ©B(Î·(w), r)),
where aÏ€,w = Pw
j=1 Pt,Ï€(j), B(v, r) = {x âˆˆ[0, 1] : |x âˆ’v| â‰¤r} and Î» is uniform measure on [0, 1].
At the end, we provide the proof of Lemma C.1.
Proof of Lemma C.1. In the following, we denote the probability measure conditioning on Pt under
H1 by P1(Â·) = P(Â·|Pt, H1) for simplicity and P0(Â·) denotes the probability under H0. For any r âˆˆR
and w âˆˆW, by the law of total probability we have that for i âˆˆ{0, 1},
Pi(Ut â‰¤r, Ï€t(wt) = w) = Pi(Ut â‰¤r | Ï€t(wt) = w)Pi(Ï€t(wt) = w).
(36)
Under H0, the construction of the watermark implies that the random variables Ï€t, wt, and Ut
are independent of each other. The independence result has two consequences.
â€¢ First, Ut is independent with Ï€t(wt) so that P0(Ut â‰¤r | Ï€t(wt) = w) = P0(Ut â‰¤r) = r1.
â€¢ Second, Ï€t(wt) is uniformly distributed on W due to
P0(Ï€t(wt) = w) =
X
â„“âˆˆW
P0(Ï€t(wt) = w, wt = â„“) =
X
â„“âˆˆW
P0(Ï€t(â„“) = w)P0(wt = â„“) =
1
|W|.
51

As a result,
P0(Ut â‰¤r, Ï€t(wt) = w) =
r
|W|.
Under H1, imagine that we fix Ï€t to be a given permutation Ï€ (so that Ï€t is not random at
all). In this case, wt is also generated by the inverse transform sampling and thus its distribution is
still Pt. This remains true when Ï€t is allowed to be uniformly random; wt continues to follow the
distribution Pt. Consequently, one can show, using Bayesâ€™ formula, that Ï€t is independent of wt. In
particular, we have
P1(Ï€t = Ï€ | wt = w) = P1(wt = w | Ï€t = Ï€)P1(Ï€t = Ï€)
P1(wt = w)
= P1(Ï€t = Ï€).
Thus, Ï€t is independent of wt under H1. Using this independence, we obtain that P1(Ï€t(wt) = w) =
|W|âˆ’1. It remains to evaluate the distribution of Ut conditional on Ï€t(wt) = w. First, by the law of
total probability and Lemma C.2, we have for any measurable set S,
P1(Ut âˆˆS | wt = â„“, Ï€t(â„“) = w) =
X
Ï€âˆˆÎ :Ï€(â„“)=k
P1(Ut âˆˆS, Ï€t = Ï€ | wt = â„“, Ï€t(â„“) = w)
=
X
Ï€âˆˆÎ :Ï€(â„“)=k
P1(Ut âˆˆS | wt = â„“, Ï€t = Ï€)P(Ï€t = Ï€ | wt = â„“, Ï€t(â„“) = w)
=
1
(|W| âˆ’1)!
X
Ï€âˆˆÎ :Ï€(â„“)=k
1
Pt,â„“
P(Ut âˆˆSt(â„“, Ï€) âˆ©S),
where St(â„“, Ï€) is defined by
St(â„“, Ï€) =
 X
wâˆˆW
Pt,w1{Ï€(w)<Ï€(â„“)},
X
wâˆˆW
Pt,w1{Ï€(w)â‰¤Ï€(â„“)}

.
(37)
Again by the law of total probability and the fact that wt and Ï€t are independent, we have for any
measurable S âŠ‚[0, 1],
P1(Ut âˆˆS | Ï€t(wt) = w) =
X
â„“âˆˆW
P1(Ut âˆˆS, wt = â„“| Ï€t(wt) = w)
=
X
â„“âˆˆW
P1(Ut âˆˆS | wt = â„“, Ï€t(â„“) = w)P1(wt = â„“| Ï€t(wt) = w)
=
X
â„“âˆˆW
P1(Ut âˆˆS | wt = â„“, Ï€t(â„“) = w)Pt,â„“
=
1
(|W| âˆ’1)!
X
â„“âˆˆW
X
Ï€âˆˆÎ :Ï€(â„“)=w
P(Ut âˆˆSt(â„“, Ï€) âˆ©S)
=
1
(|W| âˆ’1)!
X
â„“âˆˆW
X
Ï€âˆˆÎ 
P(Ut âˆˆSt(â„“, Ï€) âˆ©S) Â· 1{Ï€(â„“)=w}
=
1
(|W| âˆ’1)!
X
Ï€âˆˆÎ 
P(Ut âˆˆSt(Ï€âˆ’1(w), Ï€) âˆ©S)
=
1
(|W| âˆ’1)!
X
Ï€âˆˆÎ 
P
 Ut âˆˆ
 aÏ€âˆ’1,wâˆ’1, aÏ€âˆ’1,w

âˆ©S

52

=
1
(|W| âˆ’1)!
X
Ï€âˆˆÎ 
P (Ut âˆˆ(aÏ€,wâˆ’1, aÏ€,w] âˆ©S) ,
(38)
where aÏ€âˆ’1,w = P
jâˆˆW pt,Ï€âˆ’1(j) is the endpoint of St(Ï€âˆ’1(w), Ï€) by definition. The last equation
uses Ï€âˆ’1 âˆˆÎ  is equivalent to Ï€ âˆˆÎ .
Plugging (38) into (36), we have
P1(Ut â‰¤r, Ï€t(wt) = w) =
1
|W|P1(Ut â‰¤r | Ï€t(wt) = w)
=
1
|W|!
X
Ï€âˆˆÎ 
P (Ut âˆˆ(aÏ€,wâˆ’1, aÏ€,w] âˆ©[0, r]) .
Lemma C.2. Under H1, the probability distribution of Ut | wt = w, Ï€t = Ï€ is
P1(Ut âˆˆS | wt = w, Ï€t = Ï€) =
1
Pt,w
P(Ut âˆˆSt(w, Ï€) âˆ©S),
where S is any measurable set in the range of Ut and wt is defined in (37).
Proof of Lemma C.2. For any measurable set S âŠ‚[0, 1],
P1(Ut âˆˆS | Ï€t = Ï€, wt = w) = P1(Ï€t = Ï€, wt = w | Ut âˆˆS)P(Ut âˆˆS)
P1(Ï€t = Ï€, wt = w)
.
By the definition of wt, wt = w if and only if
Ut âˆˆ
X
jâˆˆW
Pt,j1{Ï€t(j)<Ï€t(w)},
X
jâˆˆW
Pt,j1{Ï€t(j)â‰¤Ï€t(w)}

= St(w, Ï€t).
Thus, using the independence between Ut and Ï€t,
P1(Ï€t = Ï€, wt = w | Ut âˆˆS) = P1(wt = w | Ï€t = Ï€, Ut âˆˆS)P1(Ï€t = Ï€)
= P1(Ut âˆˆSt(w, Ï€t) | Ï€t = Ï€, Ut âˆˆS)P1(Ï€t = Ï€)
= P1(Ut âˆˆSt(w, Ï€) | Ï€t = Ï€, Ut âˆˆS)P1(Ï€t = Ï€)
= P1(Ut âˆˆSt(w, Ï€) | Ut âˆˆS)P1(Ï€t = Ï€)
= P(Ut âˆˆSt(w, Ï€) âˆ©S)/P(Ut âˆˆS) Â· P1(Ï€t = Ï€).
Moreover, note that Ï€t is independent with wt under H1 so that P1(Ï€t = Ï€, wt = w) = P1(Ï€t =
Ï€)P1(wt = w) = Pt,wP1(Ï€t = Ï€). Combining these pieces completes the proof.
C.2
Key Lemma for Asymptotic Analysis
At the center of our asymptotic analysis is Lemma C.3.
53

Lemma C.3. Let J collect any 1-Lipschitz-continuous function on [0, 1]2.
For any J âˆˆJ ,
J(Ut, Î·(Ï€t(wt))) is a bounded random variable where Î·(w) =
wâˆ’1
|W|âˆ’1. Let Pt be the NTP distri-
bution that wt follows. We introduce a new belief class, denoted by Qâˆ†with the following definition
Qâˆ†=

P : P(1) = 1 âˆ’âˆ†, log |W| Â· P(2) â‰¤Îµ|W|
	
.
The following equation holds uniformly for Pt âˆˆQâˆ†and âˆ†âˆˆ[0, 1 âˆ’
1
|W|]:
lim
|W|â†’âˆE1,Pt J(Ut, Î·(Ï€t(wt))) = âˆ†
Z 1
0
(1 âˆ’x)J(âˆ†x, x)dx + âˆ†
Z 1
0
xJ(âˆ†x + 1 âˆ’âˆ†, x)dx
+
Z 1
0
dy
Z âˆ†y+1âˆ’âˆ†
âˆ†y
J(x, y)dx.
(39)
The uniform convergence holds in the sense that
lim
|W|â†’âˆsup
JâˆˆJ
sup
âˆ†âˆˆ[0,1âˆ’
1
|W|]
sup
PtâˆˆQâˆ†
|E1,Pt J(Ut, Î·(Ï€t(wt))) âˆ’right hand side of (39)| = 0.
Given the flexibility in the choice of the Lipschitz-continuous function J, Lemma C.3 demonstrates
that this random vector (Ut, Î·(Ï€t(wt))) weakly converges to a joint bivariate random vector, which
we denoted by (Z1, Z2). This convergence is substantiated by Theorem 3.9.1 in [21], which provides
the theoretical foundation. Moreover, the expected value of J(Z1, Z2) is explicitly provided by (39),
showcasing a straightforward formulation.
By relaxing the requirement for uniform convergence, we can alleviate the condition of the
Lipschitz continuity imposed on J. This relaxation leads to the derivation of a corollary that
broadens the applicability of the expectation formula in (39).
Corollary C.1. For any bounded and measurable J : [0, 1]2 â†’R, (39) also holds.
Proof of Corollary C.1. It suffices to show (39) holds for any indicator functions, i.e., J(x, y) =
1{xâ‰¤r1,yâ‰¤r2} for a given pair (r1, r2) âˆˆ[0, 1]2. For the indicator function x 7â†’1{xâ‰¤r}, we introduce
its Lipschitz variant fÎµ,r(x) for a small number Îµ > 0:
fÎµ,r(x) :=
ï£±
ï£´
ï£²
ï£´
ï£³
1
if 0 â‰¤x â‰¤r,
1 âˆ’xâˆ’r
Îµ
if r â‰¤x â‰¤r + Îµ,
0
if r + Îµ â‰¤x â‰¤1.
Utilizing Lemma C.3 with the specific function JÎµ(x, y) = fÎµ,r1(x)fÎµ,r2(y), we establish the validity
of (39) for JÎµ. By progressively reducing Îµ to zero and invoking the monotone convergence theorem,
we affirm that (39) remains applicable for this particular J.
Remark C.1. This above proof yields an insightful result. Employing J(x, y) = 1{xâ‰¤r1,yâ‰¤r2} within
(39) reveals the asymptotic CDF of (Ut, Î·(Ï€t(wt))) to be free of discontinuities across the square
[0, 1]2. This continuity is a direct consequence of the right-hand side of (39) (substituting J(x, y) =
1{xâ‰¤r1,yâ‰¤r2}) being continuous in (r1, r2).
54

C.2.1
Proof of Lemma C.3
At the end of this section, we provide the proof of Lemma C.3.
Proof of Lemma C.3. Since all the following probability is given conditioning on Pt, for simplicity,
we omit the dependence on Pt and use P1(Â·) to denote the conditional probability. For any J âˆˆJ ,
we know that J is 1-Lipschitz-continuous and bounded. For simplicity, we use O(1) to hide universal
constants such as the bound of J, and its Lipschitz-continuous constant.
Lemma C.1 shows that the joint distribution of (Ut, Ï€t(wt)) under H1 is
P1(Ut â‰¤r, Ï€t(wt) = w) =
1
|W|!
X
Ï€âˆˆÎ 
P (Ut âˆˆ(aÏ€,wâˆ’1, aÏ€,w] âˆ©[0, r1]) ,
where aÏ€,w = Pw
j=1 Pt,Ï€(j) is the sum of the first w probabilities of Pt under the permutation Ï€.
Hence,
E1,Pt J(Ut, Î·(Ï€t(wt))) =
1
|W|!
X
wâˆˆW
X
Ï€âˆˆÎ 
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr.
Since we fix t âˆˆ[n], we drop the dependence of Pt on t for simplicity. We let P(1) = maxwâˆˆW Pw
denote the largest probability in P so that P(1) = 1 âˆ’âˆ†. Now, we discuss the position, denoted by
â„“, that is mapped to 1 according to the permutation Ï€t, i.e., Ï€t(â„“) = 1 and divide the summation
P
Ï€âˆˆÎ  into three parts. We use Î â„“to represent the set of permutations that map â„“to 1. Then
Î  = S
â„“âˆˆW Î â„“and thus
E1,PtJ(Ut, Î·(Ï€t(wt))) =
1
|W|!
X
wâˆˆW
X
â„“âˆˆW
X
Ï€âˆˆÎ â„“
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr
=
1
|W|!
X
wâˆˆW
wâˆ’1
X
â„“=1
X
Ï€âˆˆÎ â„“
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr +
1
|W|!
X
wâˆˆW
|W|
X
â„“=w+1
X
Ï€âˆˆÎ â„“
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr
+
1
|W|!
X
wâˆˆW
X
Ï€âˆˆÎ k
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr
=: T1 + T2 + T3.
In the following, we analyze the three terms T1, T2 and T3 respectively.
For the term T1
For simplicity, we let P(2) =
max
k:PwÌ¸=P(1)
Pw is the second largest probability in P .
The Taylor expansion together with the boundedness of J implies that
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr = J(aÏ€,w, Î·(w))PÏ€(w) + O(P 2
Ï€(w)).
Note that for any â„“Ì¸= w, Ï€ âˆˆÎ â„“mush satisfy Ï€(w) Ì¸= 1 and thus PÏ€(w) â‰¤P(2). As a result, it follows
that
T1 =
1
|W|!
X
wâˆˆW
|W|
X
â„“=w+1
X
Ï€âˆˆÎ â„“
h
J(aÏ€,w, Î·(w))PÏ€(w) + O(P 2
Ï€(w))
i
55

=
1
|W|!
X
wâˆˆW
|W|
X
â„“=w+1
X
Ï€âˆˆÎ â„“
J(aÏ€,w, Î·(w))PÏ€(w) + O(P(2))
=
1
|W|!
X
wâˆˆW
X
Ï€âˆˆÎ 
J(aÏ€,w, Î·(w))PÏ€(w)IÏ€,w + O(P(2))
= EÂ¯Ï€
X
wâˆˆW
J(aÂ¯Ï€,w, Î·(w))PÂ¯Ï€(w)IÂ¯Ï€,w + O(P(2)),
(40)
where Â¯Ï€ âˆ¼U(Î ) is a uniformly distributed permutation and IÏ€,w = 1{Ï€âˆ’1(1)>k} is the indicator
function. We then focus on the main term in (40). By Lipschitz-continuity of J(Â·, Â·), we have

X
wâˆˆW

J(aÂ¯Ï€,w, Î·(w))PÂ¯Ï€(w)IÂ¯Ï€,w âˆ’J

wâˆ†
|W| âˆ’1, Î·(w)
 âˆ†IÂ¯Ï€,w
|W| âˆ’1

â‰¤

X
wâˆˆW
J(aÂ¯Ï€,w, Î·(w))

PÂ¯Ï€(w) âˆ’
âˆ†
|W| âˆ’1

IÂ¯Ï€,w
 + O(1) Â·
X
wâˆˆW
âˆ†IÂ¯Ï€,w
|W| âˆ’1 Â·
aÏ€,w âˆ’
wâˆ†
|W| âˆ’1
 . (41)
Here the inequality uses the Lipschitz continuity of J(Â·, Â·).
We turn to focus on the right-hand side of (41). We analyze the first term in (41) using summation
by parts. We define
AÂ¯Ï€,w =
X
jâˆˆW

PÂ¯Ï€(j) âˆ’
âˆ†
|W| âˆ’1

IÂ¯Ï€,j
and make a convention that AÏ€,0 = 0 for any Ï€ âˆˆÎ . We comment that AÂ¯Ï€,w can be viewed as a
centered version of aÂ¯Ï€,w. It then follows that

X
wâˆˆW
J(aÂ¯Ï€,w, Î·(w))(AÂ¯Ï€,w âˆ’AÂ¯Ï€,wâˆ’1)

=

|W|âˆ’1
X
w=1
[J(aÂ¯Ï€,w, Î·(w)) âˆ’J(aÂ¯Ï€,w+1, Î·(w + 1))] AÂ¯Ï€,w + J(aÂ¯Ï€,w, Î·(w))AÂ¯Ï€,w

â‰¤
ï£®
ï£°
|W|âˆ’1
X
w=1
|J(aÂ¯Ï€,w, Î·(w)) âˆ’J(aÂ¯Ï€,w+1, Î·(w + 1))| + |J(aÂ¯Ï€,w, Î·(w))|
ï£¹
ï£»Â· sup
wâˆˆW
|AÂ¯Ï€,w|
= O(1) Â· sup
wâˆˆW
|AÂ¯Ï€,w|.
The last equation uses the boundedness and Lipschitz-continuity of J(Â·, Â·) which implies
|W|âˆ’1
X
w=1
|J(aÂ¯Ï€,w, Î·(w)) âˆ’J(aÂ¯Ï€,w+1, Î·(w + 1))| + |J(aÂ¯Ï€,w, Î·(w))|
â‰¤O(1)
ï£®
ï£°
|W|âˆ’1
X
w=1
|aÂ¯Ï€,w âˆ’aÂ¯Ï€,w+1| +
|W|âˆ’1
X
w=1
|Î·(w) âˆ’Î·(w + 1)| + 1
ï£¹
ï£»= O(1).
56

The second term in (41) is smaller than O(1) Â· P
wâˆˆW IÂ¯Ï€
aÏ€,w âˆ’
wâˆ†
|W|âˆ’1
. Note that EÂ¯Ï€ IÂ¯Ï€,w = |W|âˆ’w
|W|
for any w âˆˆW. Summarizing the above analysis for T1, we have that
EÂ¯Ï€
X
wâˆˆW
J(aÂ¯Ï€,w, Î·(w))PÂ¯Ï€(w)IÂ¯Ï€,w âˆ’
X
wâˆˆW
J

wâˆ†
|W| âˆ’1, Î·(w)
 |W| âˆ’w
|W| âˆ’1
âˆ†
|W|

â‰¤O(1) Â·

EÂ¯Ï€ sup
wâˆˆW
|aÂ¯Ï€,w| + EÂ¯Ï€ sup
wâˆˆW
IÂ¯Ï€,w
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1


â‰¤O(1) Â·
 1
|W| +
q
P(2) log |W|

,
(42)
where the last inequality uses the following lemma.
Lemma C.4. Define IÏ€,w = 1{Ï€âˆ’1(1)>w} and AÂ¯Ï€,w = P
jâˆˆW

PÂ¯Ï€(j) âˆ’
âˆ†
|W|âˆ’1

IÂ¯Ï€,j. Then
max

EÂ¯Ï€ sup
wâˆˆW
IÂ¯Ï€,w
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1
 , EÂ¯Ï€ sup
wâˆˆW
|AÂ¯Ï€,w|

â‰¤sup
â„“âˆˆW
EÂ¯Ï€
"
sup
wâ‰¤â„“âˆ’1
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1
#
,
sup
â„“âˆˆW
EÂ¯Ï€
"
sup
wâ‰¤â„“âˆ’1
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1
#
â‰¤
1
|W| +
v
u
u
t
|W|
X
i=2
P 2
(i) Â· c0 log(c0|W|) + P(2) Â· c0 log(c0|W|),
where P(i) is the ith largest probability in P and c0 is universal positive constant.
Finally, we observe that

1
|W|
X
wâˆˆW
J

wâˆ†
|W| âˆ’1, Î·(w)
 |W| âˆ’w
|W| âˆ’1 âˆ’
Z 1
0
(1 âˆ’x)J(âˆ†x, x)dx

â‰¤

1
|W|
X
wâˆˆW
J (Î·(w)âˆ†, Î·(w)) (1 âˆ’Î·(w)) âˆ’
Z 1
0
(1 âˆ’x)J(âˆ†x, x)dx
 + O(1)
|W|
â‰¤O(1)
|W|
(43)
where the first inequality uses the Lipschitz continuity of J(Â·, Â·) and the second inequality uses the
Taylor expansion and boundedness of J(Â·, Â·).
Combining (40), (42), and (43), we know that the convergence that
lim
|W|â†’âˆT1 = âˆ†Â·
Z 1
0
(1 âˆ’x)J(âˆ†x, x)dx
holds uniformly over P âˆˆQâˆ†and âˆ†âˆˆ[0, 1 âˆ’
1
|W|] in the sense that
lim
|W|â†’âˆsup
JâˆˆJ
sup
âˆ†âˆˆ[0,1âˆ’
1
|W|]
sup
P âˆˆQâˆ†
T1 âˆ’âˆ†Â·
Z 1
0
(1 âˆ’x)J(âˆ†x, x)dx
 = 0.
57

For the term T2
The analysis for T1 is essentially the same as previously discussed for itself. We
will directly present the result without providing the proof here:
lim
|W|â†’âˆT2 = âˆ†
Z 1
0
xJ(âˆ†x + 1 âˆ’âˆ†, x)dx
holds uniformly over P âˆˆQâˆ†and âˆ†âˆˆ[0, 1 âˆ’
1
|W|] in the sense that
lim
|W|â†’âˆsup
JâˆˆJ
sup
âˆ†âˆˆ[0,1âˆ’
1
|W|]
sup
P âˆˆQâˆ†
T2 âˆ’âˆ†
Z 1
0
xJ(âˆ†x + 1 âˆ’âˆ†, x)dx
 = 0.
For the term T3
The analysis for T3 is much simpler than T1 and T2. It follows that
T3 =
1
|W|!
X
wâˆˆW
X
Ï€âˆˆÎ k
Z aÏ€,w
aÏ€,wâˆ’1
J(r, Î·(w))dr
= EÂ¯Ï€
X
wâˆˆW
Z aÂ¯Ï€,w
aÂ¯Ï€,wâˆ’1
J(r, Î·(w))1{Â¯Ï€(w)=1}dr
=
1
|W|
X
wâˆˆW
EÂ¯Ï€
"Z aÏ€,wâˆ’1+1âˆ’âˆ†
aÏ€,wâˆ’1
J(r, Î·(w))dr
Â¯Ï€(w) = 1
#
It then follows that
T3 âˆ’
1
|W|
X
wâˆˆW
Z
wâˆ’1
|W|âˆ’1 âˆ†+1âˆ’âˆ†
wâˆ’1
|W|âˆ’1 âˆ†
J(r, Î·(w))dr

â‰¤O(1) Â· sup
wâˆˆW
EÂ¯Ï€
aÏ€,wâˆ’1 âˆ’w âˆ’1
|W| âˆ’1âˆ†

Â¯Ï€(w) = 1

â‰¤O(1) Â·
 1
|W| +
q
P(2) log |W|

.
(44)
where the first inequality uses the boundedness of J(Â·, Â·) and the second inequality uses Lemma C.4.
On the other hand, using the same analysis in (43), we have

1
|W|
X
wâˆˆW
Z
wâˆ’1
|W|âˆ’1 âˆ†+1âˆ’âˆ†
wâˆ’1
|W|âˆ’1 âˆ†
J(r, Î·(w))dr âˆ’
Z 1
0
dx
Z âˆ†x+1âˆ’âˆ†
âˆ†x
J(r, x)dr
 = O(1)
|W| .
(45)
Combining (44) and (45), we know that the convergence that
lim
|W|â†’âˆT3 =
Z 1
0
dx
Z âˆ†x+1âˆ’âˆ†
âˆ†x
J(r, x)dr
holds uniformly over P âˆˆQâˆ†and âˆ†âˆˆ[0, 1 âˆ’
1
|W|] in the sense that
lim
|W|â†’âˆsup
JâˆˆJ
sup
âˆ†âˆˆ[0,1âˆ’
1
|W|]
sup
P âˆˆQâˆ†
T3 âˆ’
Z 1
0
dx
Z âˆ†x+1âˆ’âˆ†
âˆ†x
J(r, x)dr
 = 0.
58

In the end, we provide the missing proofs for Lemma C.4.
Proof of Lemma C.4. We first simplify the target expectations. We note that
EÂ¯Ï€ sup
wâˆˆW
IÂ¯Ï€,w
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1
 =
1
|W|
|W|
X
â„“=1
EÂ¯Ï€

sup
wâˆˆW
IÂ¯Ï€,w
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1

=
1
|W|
|W|
X
â„“=1
EÂ¯Ï€
"
sup
wâ‰¤â„“âˆ’1
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1
#
â‰¤sup
â„“âˆˆW
EÂ¯Ï€
"
sup
wâ‰¤â„“âˆ’1
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1
#
,
and
EÂ¯Ï€ sup
wâˆˆW

X
jâˆˆW

PÂ¯Ï€(j) âˆ’
âˆ†
|W| âˆ’1

IÂ¯Ï€,j

=
1
|W|
|W|
X
â„“=1
EÂ¯Ï€
ï£®
ï£°sup
wâˆˆW

X
jâˆˆW

PÂ¯Ï€(j) âˆ’
âˆ†
|W| âˆ’1

IÂ¯Ï€,j

Â¯Ï€(â„“) = 1
ï£¹
ï£»
=
1
|W|
|W|
X
â„“=1
EÂ¯Ï€
ï£®
ï£°sup
wâ‰¤â„“âˆ’1

X
jâˆˆW

PÂ¯Ï€(j) âˆ’
âˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1
ï£¹
ï£»
â‰¤sup
â„“âˆˆW
EÂ¯Ï€
"
sup
wâ‰¤â„“âˆ’1
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1
#
.
Hence, it suffices to focus on supâ„“âˆˆW EÂ¯Ï€

supwâ‰¤â„“âˆ’1
aÂ¯Ï€,w âˆ’
wâˆ†
|W|âˆ’1

Â¯Ï€(â„“) = 1

.
Note that given the condition Â¯Ï€(â„“) = 1, Â¯Ï€ is still a random permutation on the resting |W| âˆ’1
numbers which permutes W âˆ’{â„“} to W âˆ’{1}. To that end, we will use the concentration inequality
for randomly permuted sums developed by [4].
Lemma C.5 (Proposition 2.2 in [4]). Let bj,l â‰¥0 be a collection of non-negative numbers and Â¯Ï€ be
a random uniform permutation in W, i.e., Â¯Ï€ âˆ¼U(W). Let Z = P
jâˆˆW bj,Â¯Ï€(j). Then for any x > 0,
P
ï£«
ï£¬
ï£­|Z âˆ’EÂ¯Ï€ Z| â‰¥2
v
u
u
u
t
ï£«
ï£­1
|W|
|W|
X
j,l=1
b2
j,l
ï£¶
ï£¸Â· x +
sup
1â‰¤j,lâ‰¤|W|
bj,l Â· x
ï£¶
ï£·
ï£¸â‰¤8e1/16 exp

âˆ’x
16

.
To apply Lemma C.5, we set bj,l = Pl Â· 1{jâ‰¤w,lÌ¸=1}. Hence, with Z = P
jâˆˆW bj,Â¯Ï€(j), we have
Z = aÂ¯Ï€,w and once w < â„“,
EÂ¯Ï€[Z|Â¯Ï€(â„“) = 1] = EÂ¯Ï€
ï£®
ï£°X
jâˆˆW
PÂ¯Ï€(j)
Â¯Ï€(â„“) = 1
ï£¹
ï£»=
wâˆ†
|W| âˆ’1.
Note that supj,l bj,l â‰¤P(2) and P|W|
j,l=1 b2
j,l â‰¤w Â· P|W|
j=2 P 2
(j). As a result of Lemma C.5, we know that
for any â„“âˆˆW, when conditioning on Â¯Ï€(â„“) = 1, there exists a universal constant c0 > 0 such that for
59

any Î´ > 0 and any k < â„“, with probability at least 1 âˆ’Î´,
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1
 â‰¤
v
u
u
u
t w
|W|
|W|
X
j=2
P 2
(j) Â· c0 log c0
Î´ + P(2) Â· c0 log c0
Î´ .
Taking a union bound, setting Î´ = 1/|W|2, and slightly modifying the value of c0, we then have
E sup
w<â„“
aÂ¯Ï€,w âˆ’
wâˆ†
|W| âˆ’1

Â¯Ï€(â„“) = 1

â‰¤
1
|W| +
v
u
u
u
t
|W|
X
j=2
P 2
(j) Â· c0 log(c0|W|) + P(2) Â· c0 log(c0|W|).
Taking the maximum over â„“âˆˆW, we then finish the proof.
C.3
Proof of Theorem 4.1
With Lemma 4.1 and Corollary C.1, we are ready to prove Theorem 4.1.
Proof of Theorem 4.1. By Lemma 4.1 and the definition of integral, it follows that for any r âˆˆ[0, 1],
lim
|W|â†’âˆPH0(Y dif
t
â‰¤r) =
1
|W|
|W|
X
i=1
[min {Î·(i) + r, 1} âˆ’max{Î·(i) âˆ’r, 0}]
=
Z 1
0
[min{x + r, 1} âˆ’max{x âˆ’r, 0}]dx
= 1 âˆ’(1 âˆ’r)2.
On the other hand, by setting J(x, y) = 1{|xâˆ’y|â‰¤r} in Corollary C.1, it follows that for any
r âˆˆ[0, 1 âˆ’âˆ†],
lim
|W|â†’âˆPH1(Y dif
t
â‰¤r|Pt) = 2âˆ†
Z 1
0
(1 âˆ’x)1{(1âˆ’âˆ†)xâ‰¤r}dx +
Z 1
0
dy
Z âˆ†y+1âˆ’âˆ†
âˆ†y
1{|xâˆ’y|â‰¤r}dx
= 2âˆ†
Z
r
1âˆ’âˆ†âˆ§1
0
(1 âˆ’x)dx + (1 âˆ’âˆ†) âˆ’2
Z 1
0
((1 âˆ’âˆ†)x âˆ’r)+dx
= 1 âˆ’

1 âˆ’
r
1 âˆ’âˆ†
2
.
Visualization of weak convergence on an extreme case
To better understand the weak
convergence established in Theorem 4.1, we study a special case where âˆ†â†’0 and obtain the
following result.
Corollary C.2. Let Pt,(1) = maxw Pt,w denote the largest probability in Pt. For any r âˆˆR,
lim
|W|â†’âˆ
log |W|Â·Pt,(1)â†’0
PH1(Y dif
t
â‰¤r|Pt) = 1{râ‰¥0}.
60

0.0
0.2
0.4
0.6
0.8
1.0
r
0.0
0.2
0.4
0.6
0.8
1.0
CDF
|W| = 4
P1
P2
0.2P1 + 0.8P2
0.8P1 + 0.2P2
0.0
0.2
0.4
0.6
0.8
1.0
r
0.0
0.2
0.4
0.6
0.8
1.0
CDF
|W| = 6
H0
H1, P âˆ¼Uniform
H1, P âˆ¼Pow(0.5, 1)
H1, P âˆ¼Pow(1, 1)
H1, P âˆ¼Pow(2, 1)
0.0
0.2
0.4
0.6
0.8
1.0
r
0.0
0.2
0.4
0.6
0.8
1.0
CDF
|W| = 100
H0
H1, P âˆ¼Uniform
H1, P âˆ¼Pow(0.5, 1)
H1, P âˆ¼Pow(1, 1)
H1, P âˆ¼Pow(2, 1)
0.0
0.2
0.4
0.6
0.8
1.0
r
0.0
0.2
0.4
0.6
0.8
1.0
CDF
|W| = âˆ
H0, asymptotic
H1, asymptotic
Figure 9: Exact CDFs of Y dif
t
under hypotheses H0 and H1, different NTP distributions Pt, and
different vocabulary size |W|. The first panel shows F dif
1,P (r) is not convex in P for most values of
r âˆˆ[0, 1]. Here P1 = (0.1, 0.2, 0.3, 0.4) and P2 = (0.8, 0.2, 0, 0). The remaining three panels consider
|W| = 6 (left column), |W| = 100 (middle column), and |W| = âˆ(right column). We say Pt is
Pow(a, b) if Pt,w âˆ(w + b)âˆ’a and Pt is uniform if Pt,w =
1
|W| for any w âˆˆW.
Let F dif
0 (r) = P(Y dif
t
â‰¤r|H0) and F dif
1,Pt(r) = P(Y dif
t
â‰¤r|Pt, H1) denote the finite-vocabulary
null and alternative CDFs respectively. For any r âˆˆ[0, 1], Lemma 4.1 shows F dif
0 (r) â†’1 âˆ’(1 âˆ’r)2
as |W| â†’âˆand Corollary C.2 shows F dif
1,Pt(r) â†’1{râ‰¥0} if log |W| Â· Pt,(1) â†’0. It means that Y dif
t
degenerates to a point mass distribution where all of the probability mass is concentrated on the
original point.
Figure 9 illustrates the CDF F dif
1,Pt under different hypotheses, token distributions, and vocabulary
sizes. We observe that when |W| = 100, F dif
0
already align with its asymptotic null CDF well.
Considering that in practice |W| typically ranges from 3 Ã— 104 to 5 Ã— 104, we can confidently replace
the finite-vocabulary null CDFF dif
0
with the asymptotic one, i.e. 1 âˆ’(1 âˆ’r)2
[0,1]. This stance
contrasts with what [49] suggests, implying that the null hypothesis distribution is not that complex
to compute from an asymptotic perspective. This closed-form distribution enables us to explicitly
compute the critical value Î³n,Î± using the empirical estimate bÎ³n,Î±, rather than resorting to simulation
[38].
However, whether the finite-vocabulary alternative CDF, i.e., F dif
1,Pt, converges to its limit CDF
depends on whether the condition log |W| Â· Pt,(1) = o(1) holds. For example, if Pt âˆ¼Pow(2, 1), then
log |W| Â· Pt,(1) = â„¦(1), and consequently, the finite-vocabulary alternative CDF does not converge to
the limit (see the purple dotted curves in Figure 9). In contrast, if Pt follows Pow(a, b) with a < 1,
then log |W| Â· Pt,(1) = o(1) is satisfied, so the finite-vocabulary CDF aligns well with its limit even if
|W| = 100.
C.4
Proof of Lemma 4.2
In the following, we provide the omitted proof for lemmas in Section 4.2.1, namely Lemma 4.4 and
Lemma 4.5.
Proof of Lemma 4.4. The exchangeability is a direct consequence of Lemma C.3. It suffices to fucos
on Ï•P ,h(1) = E1,P eâˆ’h(Y dif). We apply Lemma C.3 by setting J(x, y) = exp(âˆ’h(|x âˆ’y|)). It is easy
to check this particular choice of J is Lipschitz-continuous given that h is Lipschitz-continuous. Then
61

for any P âˆˆQâˆ†, we know that
lim
|W|â†’âˆÏ•P ,h(1)
exists and depends only on âˆ†and h. The uniform convergence in Lemma C.3 implies that we must
have
lim
|W|â†’âˆsup
âˆ†â€²â‰¥âˆ†
sup
P âˆˆQâˆ†â€²
Ï•P ,h(1) âˆ’
lim
|W|â†’âˆÏ•P ,h(1)
 = 0.
(46)
As a result, we must have
lim
|W|â†’âˆ
 sup
âˆ†â€²â‰¥âˆ†
sup
P âˆˆQâˆ†â€²
Ï•P ,h(1) âˆ’sup
âˆ†â€²â‰¥âˆ†
sup
P âˆˆQâˆ†â€²
lim
|W|â†’âˆÏ•P ,h(1)
 = 0.
Note that by the uniform convergence (46), we have
sup
P âˆˆQâˆ†â€²
lim
|W|â†’âˆÏ•P ,h(1) =
lim
|W|â†’âˆÏ•P ,h(1) =
lim
|W|â†’âˆsup
P âˆˆQâˆ†â€²
Ï•P ,h(1).
(47)
The last two equations imply that
lim
|W|â†’âˆsup
âˆ†â€²â‰¥âˆ†
sup
P âˆˆQâˆ†â€²
Ï•P ,h(1) = sup
âˆ†â€²â‰¥âˆ†
lim
|W|â†’âˆsup
P âˆˆQâˆ†â€²
Ï•P ,h(1).
Given that the limit exists, the lim sup also exists and equals the limit.
Proof of Lemma 4.5. As implied in the proof of Lemma 4.4, we can replace lim sup with lim. By
Theorem 4.1, we already have
lim
|W|â†’âˆE0 h(Y dif) =
Z
[0,1]
h(r)Fdif,0(dr).
It then suffices to find the expression of
lim
|W|â†’âˆÏ•P ,h(1).
Note that for any P âˆˆQâˆ†, it follows from integration by parts that
Ï•P ,h(1) =
Z 1
0
eâˆ’h(r)F dif
1,P (dr) = eâˆ’h(1) +
Z 1
0
F dif
1,P (r)eâˆ’h(r)h(dr),
(48)
where F dif
1,P is the alternative CDF introduced in Lemma 4.1. Theorem 4.1 implies the following
weak convergence: for each r âˆˆ[0, 1] and P âˆˆQâˆ†, as |W| â†’âˆ,
F dif
1,P (r) dâ†’Fdif,âˆ†(r)
where Fdif,âˆ†(r) = 1 âˆ’(1 âˆ’
r
1âˆ’âˆ†)2
[0,1] is the limit CDF under H1. The weak convergence holds
uniformly in the sense that
lim
|W|â†’âˆsup
râˆˆ[0,1]
|Fdif,âˆ†(r) âˆ’F dif
1,P (r)| = 0.
(49)
This is because weak convergence in probability implies uniform convergence in cumulative distribution
functions (see Lemma C.6).
62

Lemma C.6. Let Âµn and Âµ be probability measures on R with CDFs given by Fn and F respectively.
If Âµn weakly converges to Âµ and Fis continuous, then Fn converges uniformly to F on R.
Then, by (47), (48), and (49), it follows that
lim
|W|â†’âˆsup
P âˆˆQâˆ†
Ï•P ,h(1) =
lim
|W|â†’âˆÏ•P ,h(1) = eâˆ’h(1) +
Z 1
0
Fdif,âˆ†(r)eâˆ’h(r)h(dr) =
Z 1
0
eâˆ’h(r)Fdif,âˆ†(dr)
where the last equation uses the integration by parts.
C.5
Optimal Threshold Value
We set Î´ = eâˆ’1 for the indicator score function hind,Î´ because this particular value eâˆ’1 maximizes its
Râˆ†-efficiency when âˆ†â†’1 as the following lemma shows.
Lemma C.7.
eâˆ’1 = arg max
Î´âˆˆ[0,1] lim
âˆ†â†’1(1 âˆ’âˆ†)Râˆ†(hind,Î´(Y gum)).
To prove this lemma, we first derive the limit distribution of Y gum under the same setting in
Section 4.
Lemma C.8 (Asymptotics of diverging |W|). Recall the auxiliary belief class:
Qâˆ†=

P : max
wâˆˆW Pw = 1 âˆ’âˆ†, log |W| Â· P(2) â‰¤Îµ|W|

,
where P(2) is the second largest probability in P . It follows that
lim
|W|â†’âˆ
sup
âˆ†âˆˆ[0,1âˆ’
1
|W|]
sup
P âˆˆQâˆ†
|F1,P (r) âˆ’(1 âˆ’âˆ†)r1/(1âˆ’âˆ†) âˆ’âˆ†1{r=1}| = 0 for any r âˆˆ[0, 1].
Proof of Lemma C.8. First, consider the case r âˆˆ[0, 1). Note that the CDF of Y gum satisfies that
F1,P (r) =
X
wâˆˆW
Pwr1/Pw = (1 âˆ’âˆ†)r1/(1âˆ’âˆ†) +
X
wÌ¸=argmaxjPj
Pwr1/Pw.
Moreover, since P(2) log |W| â†’0, we have

X
wÌ¸=argmaxjPj
Pwr1/Pw
 â‰¤P(2)|W|r1/P(2) = P(2) exp
 1
P(2)
log r + log |W|

â†’0
for any fixed r âˆˆ[0, 1). For r = 1,
X
wÌ¸=argmaxjPj
Pwr1/Pw =
X
wÌ¸=argmaxjPj
Pw = âˆ†.
Then our claim holds.
With Lemma C.8, we can prove Lemma C.7.
63

Proof of Lemma C.7. For any fixed âˆ†, it follows from Lemma C.8 that
Râˆ†(hind,Î´(Y gum)) = Î´ log
Î´âˆ’
âˆ†
1âˆ’âˆ†
1 âˆ’âˆ†

+ (1 âˆ’Î´) log

1 âˆ’Î´
1 âˆ’(1 âˆ’âˆ†)Î´
1
1âˆ’âˆ†

.
By direct calculation, we have
lim
âˆ†â†’1(1 âˆ’âˆ†)Râˆ†(hind,Î´(Y gum)) = âˆ’Î´ log Î´.
Let f(Î´) = âˆ’Î´ log Î´. Then fâ€²(Î´) = âˆ’log Î´ âˆ’1 and fâ€²â€²(Î´) = âˆ’1/Î´ < 0 on Î´ âˆˆ[0, 1]. It means that
f(Î´) is maximized at Î´ = 1/e.
D
Details of Experiments
D.1
Simulation Setup
Choice of pseudorandom variable
We employ a context window of size m = 5, allowing the
randomness variable Î¶t = A(s(tâˆ’m):(tâˆ’1), Key) to depend on the previous m tokens. For the hash
function A, at each step t, we concatenate the m preceding tokens with Key, that is, (w(tâˆ’m):(tâˆ’1), Key),
to generate a random seed. This seed is then used with a pseudo-random number generator, specifically
the PCG-64 generator [45], which is the default option in Pythonâ€™s NumPy package [27].
Computation of critical values
Note that E0 hâ‹†
dif,âˆ†(Y dif) = âˆ’âˆ, indicating that the central
limit theorem cannot be applied to calculate the critical value for hâ‹†
dif,âˆ†(Y dif). To determine the
critical value for hâ‹†
dif,âˆ†, we resort to simulation. For each n, we generate n i.i.d. copies of hâ‹†
dif,âˆ†(Y dif
t
)
and calculate the sum Pn
t=1 hâ‹†
dif,âˆ†(Y dif
t
). This procedure is replicated 500 times, using the empirical
1 âˆ’Î± quantile as an initial estimate. To enhance the precision of this estimate, we repeat the process
10 times and average these ten initial estimates to establish the final critical value. For other score
functions, the central limit theorem allows us to estimate the critical value as:
Ë†Î³n,Î± = n Â· E0 h(Y ) + Î¦âˆ’1(1 âˆ’Î±) Â·
p
n Â· Var0(h(Y )).
This estimate is consistent in the sense that we have limnâ†’âˆÎ³n,Î±/Ë†Î³n,Î± â†’1 for any Î± âˆˆ(0, 1) and
any score function satisfying E0 |h(Y )|2 < âˆ.
Remark D.1. Hash collisions occur when the computed pseudorandom numbers are not truly i.i.d. It
makes the way using Ë†Î³n,Î± to control Type I error become invalid. This issue is well illustrated in
Figure 1 of [22], where it is shown that the empirical false positive rates for common watermarks often
exceed theoretical predictions. The primary reason for this discrepancy is that their pseudorandom
numbers rely only on local information, i.e., Î¶t = A(w(tâˆ’m):(tâˆ’1), Key) in our notation. Consequently,
if there exists any t Ì¸= tâ€² such that w(tâˆ’m):(tâˆ’1) = w(tâ€²âˆ’m):(tâ€²âˆ’1), then Î¶t = Î¶tâ€², leading to the same
pseudorandom numbers appearing multiple times. This violates the assumption of true i.i.d. nature
and thus our working hypothesis. To mitigate this, [22] proposed heuristic methods to address such
repetitions (see their Part C in Section III). By implementing these methods, they were able to
improve Type I error control, as demonstrated in the rightmost panel of Figure 1.
However, in our paper, we did not consider the issue of hash collisions, as this issue is orthogonal
to the main results of our paper, and addressing it is beyond the scope of our current work. Instead,
64

our approach assumes Î¶t = A(w1:(tâˆ’1), Key), which effectively corresponds to the case where m = âˆ.
Given that the hash function A is highly sensitive to its inputs, even slight differences in these
inputs result in outputs that are statistically independent. Therefore, as long as t Ì¸= tâ€², we have
w1:(tâˆ’1) Ì¸= w1:(tâ€²âˆ’1), ensuring that Î¶t âŠ¥Î¶tâ€². This implies that our pseudorandom numbers are truly
i.i.d., allowing us to confidently use Ë†Î³n,Î± to control the Type I error.
Additional results for other choices of b
Additional results are compiled in Figure 10. The
performance ranking is consistent with that observed in Figure 6. Notably, as thereâ€™s a higher chance
for âˆ†to assume larger values (e.g., âˆ†âˆ¼U(0.001, 0.7) compared to âˆ†âˆ¼U(0.001, 0.1)), fewer tokens
are needed to achieve a comparable Type II error rate.
D.2
Real-world Experimental Details
Experiment setup
The experimental setup we employed is largely based on the methodology
described in Appendix D.1 of [38]. In our approach, each generation is conditioned on a prompt
which is obtained by sampling documents from the news-oriented segment of the C4 dataset [52]. We
enforce a minimum prompt size of 50 tokens in all experiments and skip over any document that is
not long enough. Note that retokenization may not precisely match the original tokens. Therefore, to
guarantee that the verifier consistently receives at least n tokens, we augment its input with special
padding tokens, which vary according to the tokenizer of each model. Additionally, to mitigate the
need for padding, we initially generate many buffer tokens beyond n. We set the number of buffer
tokens to be 20 in every experiment. This additional buffer typically makes padding unnecessary.
The hashing function A is the Skip function from [36] with the window size m = 4. This choice
is motivated by the observations of Figure 2 in [36] that the hash function Skip with context width
c = 4 has a good balance between generation diversity and adversarial robustness. The way we use to
determine critical values for different score functions mirrors the approach taken in our simulations.
Extended results for the Sheared-LLaMA-2.7B model
In our real-world experiments, we
evaluate two models: OPT-1.3B [76] and Sheared-LLaMA-2.7B [71]. For both models, we adjust
the temperature parameter to 0.1. Due to limited space, the experimental outcomes related to
Sheared-LLaMA-2.7B [71] are documented in the appendix (refer to Figure 11). It will become
clear that most patterns observed are consistent with those found in the OPT-1.3B experiments,
which are elaborated on in the main text of the paper. It is worth noting that our introduced score
functions continue to exhibit comparable effectiveness with existing methods in the context of the
Sheared-LLaMA-2.7B model.
D.3
Details of Figure 1
We prompted ChatGPT-4 to generate a list of twenty open-ended and engaging questions to foster
thought-provoking conversations. These questions, detailed below, encompass a broad spectrum of
topics, including personal growth, societal challenges, and theoretical future scenarios. Subsequently,
we employed these questions as prompts, directing ChatGPT-3.5-turbo to respond by forecasting
the next token in the sequence where the temperature parameter is set to be 1. We gathered the
highest probability predictions maxw Pt,w across various steps t and different questions. About 5000
points of maxw Pt,w have been recorded, and a frequency histogram is plotted in Figure 1.
We list the considered twenty questions below.
65

0
200
400
600
Watermarked text length
10âˆ’1
100
Type II error
H1, âˆ†âˆ¼U(0.001, 0.1)
0
200
400
600
Watermarked text length
100
3 Ã— 10âˆ’1
4 Ã— 10âˆ’1
6 Ã— 10âˆ’1
Type II error
H1, âˆ†âˆ¼U(0.001, 0.1)
0
200
400
600
Watermarked text length
10âˆ’2
10âˆ’1
100
Type II error
H1, âˆ†âˆ¼U(0.001, 0.3)
0
200
400
600
Watermarked text length
10âˆ’1
100
Type II error
H1, âˆ†âˆ¼U(0.001, 0.3)
0
50
100
150
200
250
300
Watermarked text length
10âˆ’2
10âˆ’1
Type II error
H1, âˆ†âˆ¼U(0.001, 0.7)
hars
hlog
hind,eâˆ’1
hâ‹†
gum,0.01
hâ‹†
gum,0.005
0
200
400
600
Watermarked text length
10âˆ’1
100
Type II error
H1, âˆ†âˆ¼U(0.001, 0.7)
hneg
hâ‹†
dif,0.1
hâ‹†
dif,0.01
hâ‹†
dif,0.001
Figure 10: Empirical type II errors versus the text length on simulated datasets for the Gumbel-max
watermarks (left column) and inverse transform watermark (right column).
1. What do you think are the most significant changes humanity will face in the next 50 years?
2. If you could gain one quality or ability that you currently donâ€™t have, what would it be and
why?
3. What book or movie has profoundly impacted your view of the world, and in what way?
4. If you could live in any historical period, when would it be and why?
5. What do you believe is the most important issue facing the world today, and how would you
propose we address it?
66

0
50
100
150
200
Unwatermarked text length
10âˆ’1
3 Ã— 10âˆ’2
4 Ã— 10âˆ’2
6 Ã— 10âˆ’2
Type I error
0
50
100
150
200
Watermarked text length
6 Ã— 10âˆ’1
7 Ã— 10âˆ’1
8 Ã— 10âˆ’1
9 Ã— 10âˆ’1
Type II error
hars
hlog
hind,eâˆ’1
hâ‹†
gum,0.1
0
50
100
150
200
Unwatermarked text length
3 Ã— 10âˆ’2
4 Ã— 10âˆ’2
6 Ã— 10âˆ’2
Type I error
0
50
100
150
200
Watermarked text length
100
6 Ã— 10âˆ’1
7 Ã— 10âˆ’1
8 Ã— 10âˆ’1
9 Ã— 10âˆ’1
Type II error
hneg
hâ‹†
dif,0.1
hâ‹†
dif,0.01
hâ‹†
dif,0.001
Figure 11: Type I (first column) and type II errors (second column) versus the text length for
Sheared-LLaMA-2.7B. Each curve is averaged over 500 samples from the C4 dataset. Top row:
Gumbel-max watermark. Bottom row: Inverse transform watermark.
6. If you could instantly become an expert in any subject or skill, what would it be and why?
7. How do you think technology will affect human relationships in the future?
8. If you could solve one unsolved mystery, which one would you choose and why?
9. What does success mean to you, and do you feel youâ€™ve achieved it?
10. If you had the power to change one law or policy in your country, what would it be and why?
11. Whatâ€™s a belief you had as a child that youâ€™ve completely changed your opinion on?
12. If you could have a conversation with any person from history, who would it be and what
would you ask them?
13. How do you think the concept of work will evolve in the next 100 years?
14. If you could witness any event in history, what would it be and why?
15. Whatâ€™s something that youâ€™ve learned about yourself during a difficult time?
16. How do you define happiness, and what makes you happy?
17. Whatâ€™s one piece of advice you would give to your younger self?
67

0.0
0.1
0.2
0.3
0.4
0.5
âˆ†
0.75
0.80
0.85
0.90
0.95
1.00
Î³(âˆ†)
0.0
0.1
0.2
0.3
0.4
0.5
âˆ†
0.05
0.10
0.15
0.20
0.25
0.30
RPâˆ†(h)
hâ‹†
gum,âˆ†, arg max = 0.5
hars, arg max = 0.5
hlog, arg max = 0.5
hind,1/e, arg max = 0.5
0.0
0.1
0.2
0.3
0.4
0.5
âˆ†
0.05
0.10
0.15
0.20
Î³(âˆ†) Â· RPâˆ†(h)
hâ‹†
gum,âˆ†, arg max = 0.48651
hars, arg max = 0.46453
hlog, arg max = 0.48651
hind,1/e, arg max = 0.47652
Figure 12: Illustration of Î³(âˆ†) (left), RPâˆ†(h) (middle), and Î³(âˆ†) Â· RPâˆ†(h) (right) as functions of
âˆ†. Since âˆ†is typically small in practice, we focus on the range âˆ†âˆˆ[0.001, 0.5]. The experimental
setup is consistent with that of Figure 1.
18. If you could invent something that would make life easier for people, what would it be?
19. How do you think education will change in the future?
20. If humanity were to establish a colony on another planet, what do you think are the most
important considerations to ensure its success?
D.4
Discussion on the Selection of âˆ†
In practice, we choose âˆ†using prior knowledge. It is important to note that there is a trade-off
when selecting the value of âˆ†.
Assume that an exact Î³-fraction of P1, . . . , Pn belongs to Pâˆ†, and we lack information about
the remaining NTP distributions. Consequently, the rest are considered to be in the full class
P0 = Simp(W), which includes all categorical distributions over the vocabulary W. Proposition 2.1
indicates that lim sup
nâ†’âˆPH1(Th(Y1) = 0)1/n â‰¤eâˆ’Î³RPâˆ†(h). This inequality is tight in the worst-case
scenario, implying that without additional knowledge, this is the best convergence rate we can
expect.
Theoretically, we view Î³ as a function of âˆ†, interpreting it as the fraction of âˆ†-regular NTP
distributions among P1, . . . , Pn. Clearly, Î³(âˆ†) is a decreasing function of âˆ†. As âˆ†increases, we
impose a stricter condition on the largest probability in P , requiring maxwâˆˆW Pw â‰¤1 âˆ’âˆ†, resulting
in a smaller fraction satisfying this condition. However, as shown in Figure 4 (or the middle panel of
Figure 12), RPâˆ†(h) is an increasing function of âˆ†. Therefore, the final efficiency rate, Î³(âˆ†)RPâˆ†(h),
is a complex function of âˆ†, neither strictly increasing nor decreasing. This implies a trade-off in
choosing âˆ†, and an optimal choice exists.
We support the above analysis with further empirical study, following the same setup as in Figure
1 and presenting the results in Figure 12. Its left panel shows the empirical estimation of Î³(âˆ†),
which is clearly a decreasing function of âˆ†. The middle panel, essentially a zoomed-in version of
Figure 4, is presented here within the interval âˆ†âˆˆ[0.001, 0.5] for reader convenience. The arg max
denotes the value of âˆ†at which the value of the y-axis reaches its maximum. Since theoretically
RPâˆ†(h) is an increasing function of âˆ†, we find that arg max â‰¡0.5 for all score functions. However,
when we focus on the product Î³(âˆ†) Â· RPâˆ†(h), it typically increases at first and then decreases. This
68

can be validated by observing the value of arg max. In the right panel of Figure 12, the computed
values of arg max for all score functions shift from 0.5, indicating that the inclusion of Î³(âˆ†) indeed
impacts the final efficiency. We highlight that in our particular example, the effect of Î³(âˆ†) seems
limited, as the shift in arg max is moderate. However, there are cases where this deviation can be
significant. Hence, choosing a good value of âˆ†would be a case-by-case argument, but the idea of
choosing an optimal âˆ†would always be applicable.
69
