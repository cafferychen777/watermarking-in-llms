arXiv:2401.10360v1  [cs.CR]  18 Jan 2024
Excuse me, sir? Your language model is leaking (information)
Or Zamir 1
Abstract
We introduce a cryptographic method to hide
an arbitrary secret payload in the response of a
Large Language Model (LLM). A secret key is
required to extract the payload from the model’s
response, and without the key it is provably im-
possible to distinguish between the responses of
the original LLM and the LLM that hides a pay-
load. In particular, the quality of generated text
is not affected by the payload.
Our approach
extends a recent result of Christ, Gunn and Za-
mir (2023) who introduced an undetectable wa-
termarking scheme for LLMs.
1. Introduction
As the applications of machine learning models generat-
ing human-like text become widespread, concerns for mis-
use proportionally rise.
As detection of text generated
by Large Language Models (LLMs) seemingly approaches
infeasibility (Jawahar et al., 2020; Krishna et al., 2023;
Sadasivan et al., 2023; Chakraborty et al., 2023), the ability
to intentionally plant a watermark in LLM-generated text
becomes the most viable approach to differentiate LLM-
generated from human-generated text.
A
long
line
of
works
showed
that
a
watermark
can
be
planted
in
LLMs
by
altering
the
out-
put
texts
(Abdelnabi & Fritz,
2021;
Qiang et al.,
2023;
Yoo et al.,
2023a;
Munyer & Zhong,
2023;
Kirchenbauer et al., 2023b).
Recently, Christ, Gunn
and Zamir (Christ et al., 2023) showed that a watermark
can be planted in LLM-outputs without altering the
distribution of responses. Informally, CGZ show that any
LLM can be modiﬁed such that: 1) It is computationally
infeasible to distinguish between the original and the
modiﬁed LLMs unless you hold a secret key, even when
you are allowed to make many adaptive queries, 2) With
the secret key, outputs of the modiﬁed LLM can be
detected as watermarked. The importance of this notion of
1Department of Computer Science, Tel Aviv University. Cor-
respondence to: <orzamir@tauex.tau.ac.il>.
Prompt: Write an email asking my professor Prof. Han-
nity to not make the ﬁnal exam in Machine Learning 101
too difﬁcult.
Response: Dear Professor Hannity,
I hope this email ﬁnds you well. I am writing to respect-
fully request that you consider the level of difﬁculty for
the ﬁnal exam in Machine Learning 101. While I am
conﬁdent in my understanding of the course materials
and have put in a signiﬁcant amount of effort through-
out the semester, I do have concerns about the potential
difﬁculty of the ﬁnal exam.
As you may recall, several students in my previous sec-
tions of Machine Learning have found the ﬁnal exam
to be very challenging, leading to frustration and dis-
appointment. While I understand that the course is in-
tended to push students to their limits and beyond, I be-
lieve that some relaxation of the latter may be in order.
I would kindly ask that you consider reducing the difﬁ-
culty of the ﬁnal exam or offering some additional sup-
ports or resources to help students prepare. I believe that
this could enhance the learning experience
Hidden Payload: OZ (Encoded as 10 bits.)
Figure 1. We asked Llamma 2 to write an email urging a professor
for an easy exam, intended to be sent anonymously. Nevertheless,
the part of the response shown above secretly encodes the initials
of the user who used the LLM. This secret payload was encoded
without modifying the response distribution at all.
undetectibility is that it formally guarantees that the quality
of text does not degrade (or change at all) in the process of
planting the watermark.
In this work, we extend the watermarking scheme of CGZ
to plant an arbitrary payload into the LLM response, while
still maintaining the same property of undetectability. This
implies, for example, that an LLM may secretly hide
session-metadata (such as the user’s name, the time, or the
prompt used) in the response it gives. In particular, it can
extend the ability of detecting text generated by the LLM
to the ability of also knowing who used the LLM or when
they did so. It also means that even an LLM running off-
line may use the responses it gives to covertly leak inter-
nal information to those who will later be exposed to the
1

Excuse me, sir? Your language model is leaking (information)
seemingly-clean LLM responses. The main technical tool
we use to transform the CGZ scheme from watermarking to
embedding a message, is incorporating a dynamic variant
of Error Correcting Codes with feedback.
The process of encoding a hidden message into a
given channel (e.g., a picture or natural language) is
called Steganography and was extensively studied in
many domains (Hopper et al., 2008; Dedi´c et al., 2009;
de Witt et al., 2022). The unique property of our setting,
though, is that in the case of LLM responses the distri-
bution of the channel is unknown. The hidden message
should be recovered from the response of the LLM, with-
out knowing what the prompt used to generate the re-
sponse was.
In particular, the process of decoding the
message must be agnostic to the distribution of responses.
Recent manuscripts (Fernandez et al., 2023; Wang et al.,
2023; Yoo et al., 2023b) embed messages in LLM re-
sponses, but do so by altering the distribution of responses,
similar to the non-undetectable watermarking schemes. In
particular, those methods affect the response distribution
and may degrade the quality of generated text.
Under realistic assumptions which we verify empirically,
the amount of hidden bits our scheme can encode in an
LLM output is linear in the length of the response, which is
asymptotically optimal. Our scheme only negligibly affects
the complexity of the generation process and is thus easy
and efﬁcient to deploy. While this paper is theoretical in na-
ture and the properties of the suggested scheme are proven
rigorously, we also implemented the scheme and provide
empirical examples. An open problem not addressed in this
paper is that our encoding scheme is not very robust to ed-
its of the generated text, we discuss this is length in later
sections.
1.1. Organization of the paper
In Section 2 we give the formal deﬁnitions of the problem’s
setting, and state the main theorems of this paper rigorously.
We also give the necessary preliminaries. In Section 3 we
give a quick overview of the CGZ watermark. In Section 4
we give an high-level overview of our scheme. Then, in
Section 5 we introduce a simple dynamic error correcting
code with feedback, which we later use as a building block.
Finally, in Section 6 we give and analyze the full scheme.
In Section 7 we discuss our implementation of the scheme
and some empirical evaluation of it. In Section 8 we discuss
limitations of our scheme, and in particular robustness to
editing. We follow with open problems and conclusions.
2. Model and Preliminaries
Many
of
the
notions
in
this
section
are
adapted
from (Christ et al., 2023), the last subsection contains our
new deﬁnitions and theorems for Steganography in LLMs.
2.1. Preliminaries
Let λ be the security parameter, we denote by negl(λ) any
function that is in O

1
p(λ)

for every polynomial p(·). As
is standard in Cryptography research, we think of λ as the
“key size”, and of running times that are super-polynomial
in λ as “infeasible”. We denote by log and ln the loga-
rithm with base two and the natural logarithm, respectively.
For a sequence s = (. . . , si, . . .) we denote by s[i : j]
the sub-sequence (si, . . . , sj). The Hamming distance be-
tween two vectors is deﬁned as the number of indices on
which they differ, that is ∆(z, z′) := |{i | zi ̸= z′
i}|. We
denote by x || y the concatenation of the vectors x, y, and
by len(v) the dimension of the vector v.
Pseudorandom function (PRF).
Let F
=
{Fk
:
{0, 1}ℓ1(λ) →{0, 1}ℓ2(λ) | k ∈{0, 1}λ} be a family of
functions. F is a PRF if Fk is efﬁciently computable and
for all probabilistic polynomial-time distinguishers D,

Pr
k←{0,1}λ
h
DFk(·)(1λ) = 1
i
−Pr
f
h
Df(·)(1λ) = 1
i
≤negl(λ)
where f denotes a random function from {0, 1}ℓ1(λ) to
{0, 1}ℓ2(λ).
PRFs are a standard cryptographic prim-
itive equivalent to one-way functions and can be con-
structed from standard assumptions (Goldreich et al., 1986;
H˚astad et al., 1999). Intuitively, a PRF is simply a func-
tion that is indistinguishable from a totally random function
without knowledge of the secret key k that parameterizes it.
2.2. Language Models
We adapt the deﬁnition of (Christ et al., 2023) for language
models, which in turns follows that of (Kirchenbauer et al.,
2023a). We will often refer to language models simply as
models.
Deﬁnition 2.1. A language model Model over token set T
is a deterministic algorithm that takes as input a prompt
PROMPT and tokens previously output by the model x =
(x1, . . . , xi−1), and outputs a probability distribution pi =
Model(PROMPT, x) over T .
A language model Model is used to generate text as
a response to a prompt by iteratively sampling from
the returned distribution until a special terminating to-
ken done ∈T is drawn.
Deﬁnition 2.2. A language model’s response to PROMPT is
a random variable Model(PROMPT) ∈T ⋆that is deﬁned al-
gorithmically as follows. We begin with an empty list of to-
kens x = (). As long as the last token in x is not done, we
2

Excuse me, sir? Your language model is leaking (information)
draw a token xi from the distribution Model(PROMPT, x)
and append it to x. Finally, we set Model(PROMPT) = x.
We assume that our model never outputs text of length
super-polynomial in λ.
2.3. Entropy and Empirical Entropy
For a probability distribution D over elements of a ﬁnite
set X, the Shannon entropy of D is
H(D) =
E
x∼D[−log D(x)],
where D(x) is the probability of x in the distribution D.
The empirical entropy (also known as Shannon informa-
tion) of x in D is simply −log D(x). The expected em-
pirical entropy of x ∼D is H(D).
The following deﬁnition of empirical entropy of a model’s
response is taken from (Christ et al., 2023).
Deﬁnition
2.3.
For
a
language
model
Model,
a
prompt PROMPT, and a possible response x
∈
T ⋆,
we deﬁne the empirical entropy of Model responding
with x to PROMPT, denoted by He(Model, PROMPT, x), as
−log Pr
h
Model (PROMPT) = x
i
.
Note that in expectation, Deﬁnition 2.3 simply captures the
entropy in the response generation. That is, we have
E
x [He(Model, PROMPT, x)] = H
 Model (PROMPT)

,
where x ∼Model (PROMPT).
The following deﬁnition naturally generalizes empirical en-
tropy from whole outputs to substrings out of a model’s
output.
Deﬁnition
2.4.
For
a
language
model
Model,
a
prompt PROMPT, a possible response x
∈
T ⋆, and
indices i, j ∈[|x|] with i ≤j we deﬁne the empirical
entropy on substring [i, j] of Model responding with x
to PROMPT as
H[i,j]
e
(Model, PROMPT, x) :=
−log Pr
h
Model (PROMPT) [i : j] = x[i : j] |
Model (PROMPT) [1 : (i −1)] = x[1 : (i −1)]
i
.
2.3.1. EMPIRICAL ENTROPY IN NATURAL LANGUAGE
Studies
in
linguistics
(Genzel & Charniak,
2002;
Chen et al., 2017; Shi & Lei, 2022) conclude that in
natural language the entropy per unit of text (e.g., a word)
is usually constant throughout the text. In particular, the
empirical entropy of a LLM response is expected to be
linear in the length of the text, and roughly uniformly
distributed among the different tokens that assemble
the response.
This intuition was empirically veriﬁed
by (Christ et al., 2023). We reafﬁrm the above in Section 7
in which we also run empirical evaluations.
2.4. Steganography for LLMs
In this section we ﬁnally deﬁne rigorously steganography
for language models.
We ﬁrst explain the deﬁnition in-
tuitively. During the setup of the scheme, we generate a
secret key k of size λ. To generate a response, we use
a method Stegk that together with the key k, receives a
prompt PROMPT and a secret message PAYLOAD. A re-
trieval method Retrk should be able to retrieve the hid-
den PAYLOAD from an output generated using Stegk, while
also using the secret key k.
Deﬁnition 2.5 (Steganography Scheme). A steganography
scheme for a model Model over T is a tuple of algorithms
W = (Setup, Steg, Retr) where:
• Setup(1λ) →k outputs a secret key, with respect to a
security parameter λ.
• Stegk(PROMPT, PAYLOAD) is a randomized algo-
rithm that takes as input a prompt PROMPT and a pay-
load PAYLOAD, and generates a response in T ⋆.
• Retrk(x) →T ⋆is an algorithm that takes as input and
returns as an output sequences in T ⋆.
The most important property of the steganography scheme
we present in this paper is undetectability.
Intu-
itively, we require that without knowledge of the secret
key, Stegk(PROMPT, ⋆) and Model(PROMPT) are indistin-
guishable even to a user allowed to make many adaptive
queries. The payloads used in Stegk do not affect this prop-
erty.
Deﬁnition
2.6
(Undetectability).
A
steganography
scheme W
=
(Setup, Steg, Retr) is undetectable if
for every security parameter λ and all polynomial-time
distinguishers D,
 Pr[DModel,Model(1λ) →1] −
Pr
k←Setup(1λ)
[DModel,Stegk(1λ) →1]

≤negl(λ),
where the notation DO1,O2 means that D is allowed to
adaptively query both O1 and O2 with arbitrary prompts.
3

Excuse me, sir? Your language model is leaking (information)
Another important desired property is that Retrk should
succeed in retrieving the
PAYLOAD from the output
of Stegk(PROMPT, PAYLOAD). Such a successful retrieval
inherently requires making assumptions on the length of
the payload and the entropy of the model. It is for exam-
ple impossible to encode an arbitrary payload longer than
the output in the output, or to plant any payload at all in
a deterministic model (thus, without any entropy) while re-
maining undetectable. The best possible result is thus en-
coding He(Model, PROMPT, x) bits of payload in an out-
put x ←Stegk(PROMPT, PAYLOAD), this is because the
empirical entropy of an output exactly quantiﬁes the num-
ber of entropy bits used for its generation.
We
achieve
the
above
best-possible
bound
asymp-
totically
yet
we
require
two
additional
technical
conditions.
First,
we
can
plant
payloads
only
when He(Model, PROMPT, x)
=
Ω(λ).
Intuitively,
this is because outputs with a very low empirical entropy
(with respect to the security parameter) show up too
frequently and thus can’t be modiﬁed while maintaining
undetectability. The necessity of this condition, that also
appeared in the CGZ result, is proven in (Christ et al.,
2023).
Second, we require that the empirical entropy
is spread somewhat uniformly throughout the output x.
This appears to be necessary for technical reasons (e.g.,
avoiding a scenario in which the empirical entropy is high
solely due to a single very-low-probability token in the
response), yet could potentially be relaxed in future works.
As discussed in Section 2.3.1 and empirically reafﬁrmed in
Section 7, this condition is satisﬁed by natural language in
which the entropy is roughly constant per each unit of text.
A semi-formal version of the theorem follows, the formal
one appears in Section 6.
Theorem (Informal version of Theorem 6.3). Fix a
model Model. Let PROMPT, PAYLOAD be strings. Condi-
tioned on the empirical entropy of a response y generated
by Stegk(PROMPT, PAYLOAD) being high enough, the ex-
pected length of the preﬁxes of PAYLOAD and Retrk(y) that
identify is at least Θ(len(y)).
The deﬁnition of high enough, formally stated in Section 6,
roughly means that for any consecutive part of the response
consisting of a large enough number r of tokens, the empir-
ical entropy in that part is at least the square root ˜Ω(√r) of
the length. As discussed in Section 2.3.1 and veriﬁed em-
pirically in Section 7, in natural language we actually ex-
pect the entropy to grow linearly with the length of the text,
much higher than the required square root. Under this con-
dition, the theorem guarantees that a response of length L
will allow retrieving the ﬁrst Θ(L) bits of the PAYLOAD,
which is (up to constants) the best possible.
3. Overview of the CGZ Watermark
This
section
is
adapted
in
its
entirety
from
CGZ (Christ et al., 2023),
and contains a high-level
description of their watermarking scheme.
We ﬁrst simplify the deﬁnition of a language model (Def-
inition 2.1) by assuming that the token set is binary, T =
{0, 1}. We may assume this without loss of generality due
to a straightforward reduction that appears in Section 4.1 of
CGZ. We will implicitly use this reduction throughout our
work as well.
The intuitive idea behind the CGZ watermark is planting
a watermark not by changing the model’s distribution, but
by correlating the randomness used by the model with the
secret key. We begin by describing a simpliﬁed approach
that works only for generating a single response, of length
bounded by some parameter L. Let k = (k1, k2, . . . , kL)
be the secret key, chosen by drawing each ki uniformly
and independently from [0, 1]. To generate a response to a
prompt PROMPT, we run the model as intended yet use ki to
determine the random choice in the i-th response token gen-
eration. Let pi denote the probability, according to the real
model with the previously chosen tokens as preﬁx, of the i-
th token being 1. The watermarked model outputs xi = 1
if ki ≤pi and xi = 0 otherwise. Crucially, as ki was cho-
sen uniformly, the probability of xi = 1 is exactly pi and
hence the output distribution of the model is not affected
at all. On the other hand, we now expect some correlation
between xi and ki.
For each response bit xi, the detection algorithm may com-
pute the following score, that depends on the key and on
the response but not on the prompt, the model, or the distri-
butions pi,
s(xi, ki) =
(
ln 1
ki
if xi = 1
ln
1
1−ki
if xi = 0 .
Given a string x = (x1, . . . , xℓ), the detection algorithm
sums the score of all bits
c(x) =
ℓ
X
i=1
s(xi, ki).
The main observation is that the score is higher in responses
generated by the above procedure than it is for unrelated
strings. In non-watermarked text, the value of ki is inde-
pendent of the value of xi. Therefore, s(xi, ki) is simply
an exponential random variable with mean 1:
E
ki[s(xi, ki)] =
Z 1
0
ln(1/x) dx = 1,
and we have Ek[c(x) −|x|] = 0. For watermarked re-
4

Excuse me, sir? Your language model is leaking (information)
sponses, on the other hand,
E
ki[s(xi, ki)] =
Z pi
0
ln 1
x dx +
Z 1
pi
ln
1
1 −x dx
=
Z pi
0
ln 1
x dx +
Z 1−pi
0
ln 1
x dx
= 1 + −pi · ln pi −(1 −pi) · ln(1 −pi)
= 1 + ln(2) · H(pi),
and the total expected score is
E
k[c(x) −|x|] = ln 2 · H(Model(PROMPT)).
We thus observed that at least in expectation, the score of
watermarked texts is larger than that of arbitrary texts, and
that the difference between those quantities is roughly the
entropy in the response. To make this observation algorith-
mically useful, considering expectations is not sufﬁcient, as
we need to set some score threshold for detection and an-
alyze the probability of the score passing this threshold in
each of the two cases. This analysis will not be repeated in
this short overview and appears in CGZ.
To avoid having an upper bound L on the length of the re-
sponse, and to reduce the length of the key k, we use a
Pseudo-Random Function F (PRF, as deﬁned in Section 2).
The key will now simply be a random string of length λ,
and we would implicitly set ki := Fk(i). By the deﬁnition
of a PRF, those values are indistinguishable from indepen-
dently chosen random values.
The ﬁnal obstacle is remaining undetectable even when
many queries are allowed. In the above sketch the choice of
the key k fully determines all randomness, and thus for ex-
ample the same prompt will always get the same response.
To overcome this hurdle, we begin the generation of each
response with using real randomness (and not the key) to
sample tokens, while counting the empirical entropy of the
response preﬁx generated so far. When the response pre-
ﬁx passes a threshold of empirical entropy λ, we denote
the response’s preﬁx as r and start running the previous
scheme with r as an additional input to the PRF. That is,
after we set the preﬁx r we use the value Fk(r, i) to gen-
erate the i-th token. In the detection, we will enumerate
over all possible preﬁxes of the response as r. In CGZ, it is
shown that because the preﬁx r is set only after enough en-
tropy was used, it has negligible probability to ever repeat
itself in different queries. Thus the inputs to the PRF calls
are each unique and the scheme becomes undetectable even
with many queries being made.
The pseudo-codes for generation (Algorithm 1) and detec-
tion (Algorithm 2) of the watermark are attached. In CGZ,
those algorithms are then generalized to also support the
Algorithm 1 Watermarking algorithm Watk
Data: A prompt (PROMPT) and a secret key k
Result: Watermarked text x1, . . . , xL
i ←1
H ←0
while done /∈(x1, . . . , xi−1) do
pi ←Model(PROMPT, x1, . . . , xi−1)
if H < λ then
// Collect more internal entropy
Sample (xi, p) ←(1, pi) with probability pi, oth-
erwise (0, 1 −pi)
H ←H −log p
if H ≥λ then
r ←(x1, . . . , xi)
end
else
// Embed the watermark
xi ←
1[Fk(r, i) ≤pi]
end
i ←i + 1
end
Algorithm 2 Detector Detectk
Data: Text x1, . . . , xL and a secret key k
Result: True or False
for i ∈[L] do
r(i) ←(x1, . . . , xi)
Deﬁne v(i)
j
:= xj · Fk(r(i), j) + (1 −xj) · (1 −
Fk(r(i), j)) for j ∈[L]
if PL
j=i+1 ln(1/v(i)
j ) > (L −i) + λ
√
L −i then
return True
end
end
return False
detection of the watermark from a substring out of the re-
sponse and not only from the response in its entirety as is
sketched above.
4. High-Level Overview of Our Scheme
In this section we give an overview of our construction,
with the rigorous details appearing in Sections 5 and 6. As
in the CGZ overview in Section 3, we again assume without
loss of generality that the token space is binary.
As a ﬁrst attempt, we notice that one may generalize any
watermark into a steganography scheme by using several
keys. Let k1, . . . , km be m different secret keys, and setup
a watermarking scheme with each of them. To encode a
message i ∈[m] within a response, simply use the wa-
termarking instance corresponding to ki to generate said
response. In the retrieval step, we will use the detection al-
5

Excuse me, sir? Your language model is leaking (information)
gorithm with every key kj to ﬁnd which of them was used.
While undetectability is trivially preserved, as we only use
undetectable watermarks to generate responses, the scheme
becomes infeasible as soon as m isn’t very small. This is
because both the rate of “false-positives” and the detection
time grow by a multiplicative factor of m. In particular,
encoding ℓbits of information will cause a multiplicative
factor of 2ℓin the running time of the retrieval algorithm,
and will also require that the false-positive rate of the wa-
termarking scheme be much smaller than 2−ℓ.
A reasonable next step then, is breaking up the payload into
smaller parts (say, characters or bits), and encoding each
of those parts separately in a similar fashion to the previ-
ous suggestion. One subtle issue to overcome while imple-
menting this idea is that partitioning the response into those
smaller chunks is not straightforward. This is because we
know a successful watermarking requires high empirical
entropy, and it is not known in advance what parts of the
response would contain high empirical entropy. Moreover,
the retriever needs to be able to use the same partition as
the steganography algorithm. We solve this problem by im-
plicitly deﬁning the partition to chunks using the detection
score itself: Let t be some score threshold to be decided
later. Denote the ﬁrst bit of the payload by b ∈{0, 1}. We
start planting the payload in the same way as the CGZ wa-
termark is embedded, but with b as an additional input to
the PRF. That is, the randomness used in the i-th token
is Fk(i, b).
The retriever would compute the scores for
both options of b over more and more tokens until one of
the two scores passes a certain threshold determined by t.
For example, we can consider the minimal preﬁx of the re-
sponse x[: i] for which (c(x[: i]) −i)/
√
i > t with respect
to the score computed with either b = 0 or b = 1. Crucially,
both Stegk and Retrk can compute those scores, so both
can compute the index i in which one of the scores passed
the threshold for the ﬁrst time. Thus, both are able to agree
on the end of the chunk encoding the ﬁrst bit of the payload,
and thus can iteratively restart this process with the next bit
of the payload and so on. Following the CGZ analysis, a
threshold of roughly t ≈λ is required to make sure the
correct bit is always the ﬁrst one to pass the score thresh-
old, as otherwise the false-positive rate of passing the score
threshold is non-negligible. This means that whilst the de-
coding time and general error rate are now low enough, we
still can’t encode payloads that are too long. Even if every
token contains constant entropy, only Θ(L/λ) bits of the
payload can be encoded in a response of length L. Since λ
is rather large, we should desire to get this quantity down
to the optimal Θ(L).
Finally, we use Error Correcting Codes (ECC) to lower the
threshold t and in turn increase the number of hidden pay-
load bits. Intuitively, if instead of setting t ≈λ we set t to
be a large constant, then false-positives do happen, but only
with a small constant probability. Thus, if instead of encod-
ing the bits of the payload itself, we would encode the bits
of an ECC applied to the payload, we should be able to re-
cover the payload even if a constant fraction of the bits were
transmitted incorrectly. This would reduce t from λ to O(1)
as desired. A problem that remains is that applying a stan-
dard ECC “scrambles” the message and thus if only a preﬁx
of the code is successfully hidden then it would be impossi-
ble to decode anything out of the payload. For this reason,
in Section 5 we deﬁne and construct a Dynamic ECC, this
is a code in which for every i, the ﬁrst i bits of the code can
be used to retrieve the ﬁrst Θ(i) bits of the message. Thus,
a response of size L would hide the ﬁrst Θ(L) bits of the
payload. To signiﬁcantly simplify the construction of the
ECC, we use the fact our construction provides noiseless
feedback. As mentioned before, during the encoding pro-
cess, Stegk can also simulate Retrk and thus knows what
bit will be detected by the retriever at each step. Thus, the
ECC can depend on whether or not each previously sent bit
was transmitted correctly. In the construction of Section 5
we actually use a ternary code alphabet rather than binary,
which doesn’t affect the sketch of the construction much.
We ﬁnally note that to support multiple queries, we use the
same idea of CGZ sketched in Section 3, and begin by ob-
serving enough real entropy to set a unique preﬁx r, the
following random choices will be made using Fk(r, i, b).
For the detector to ﬁnd the correct preﬁx to use as r, we
need to use the ﬁrst λ bits of entropy after setting the pre-
ﬁx to place a normal watermark, which the detector would
use to verify the choice of r. This means that to be unde-
tectable with many queries, we start encoding the payload
only after the ﬁrst Θ(λ) tokens (or bits of entropy). As long
as L = Ω(λ) this does not matter asymptotically.
5. Dynamic Error Correcting Code
Error Correcting Codes (ECCs) are the means to compen-
sate for errors in the transmission of messages. An ECC
encoding is a function Enc : Σk →Γn from messages of
length k over alphabet Σ, to codewords of length n over
alphabet Γ. The rate of a code is R(Enc) :=
k
n, which
signiﬁes how efﬁcient the code is. The (relative) distance
of a code is δ(Enc) := 1
n minz̸=z′ Enc(z)∆Enc(z′), which
is twice the fraction of corrupt indices in a codeword that
still allows decoding it to the original message. A code
(or a family of codes) is considered good if both its rate
and distance are constant, which means that the length of
messages is only expanded by a constant factor, yet a con-
stant fraction of errors can be corrected. ECCs are exten-
sively studied and it is long known that good ECCs can
be constructed, even when Σ = Γ = F2. (Hamming,
1950; Gilbert, 1952; Varshamov, 1957; Justesen, 1972;
Sipser & Spielman, 1996)
6

Excuse me, sir? Your language model is leaking (information)
An ECC with feedback is an ECC in which we transmit the
symbols of the codeword Enc(x) one-by-one, and imme-
diately receive feedback with regards to whether an error
occurred in transmitting this symbol. The following sym-
bols we submit may adaptively depend on the feedback re-
ceived so far. We say that the feedback is noiseless if the
feedback received is always reliable. If the errors in trans-
mission occur randomly (i.e., each transmitted symbol has
the same probability of becoming corrupted), then it turns
out that noiseless feedback does not improve the parame-
ters of the best possible ECC. On the other hand, if the
small fraction of corrupted symbols is chosen adversarially,
then noiseless feedback does improve the best possible dis-
tance. Feedback also appears to allow simpler and more ef-
ﬁcient encoding and decoding schemes. (Berlekamp, 1964;
Cover, 1988)
We deﬁne a natural generalization of ECCs, in which the
length of the message (and hence also of the code) is not
known in advance. We call those Dynamic ECCs.
We
would require that for any k′ ≤k, the ﬁrst k′ symbols of
the message can be decoded from the ﬁrst O(k′) symbols
of the codeword, even if a small fraction of those codeword
symbols are corrupted. This deﬁnition is similar yet weaker
than the deﬁnition of Tree Codes (Schulman, 1993; 1996).
Deﬁnition 5.1. For alphabets Σ, Γ a family {Enck}k∈N of
functions Enck : Σk →Γ⋆is called a Dynamic ECC if for
every k ∈N, the function Enck : Σk →Γnk is an ECC,
where
Enck(x) := Enc1(x[: 1]) || Enc2(x[: 2]) || . . . || Enck(x),
nk := max
x∈Σk len(Enck(x)).
In simple words, a Dynamic ECC is a family of standard
ECCs where the codeword corresponding to the a preﬁx of
a message, is always a preﬁx of the codeword correspond-
ing to the entire message.
Deﬁnition 5.2. The rate of a Dynamic ECC is R(Enc) :=
infk∈N R(Enck)
=
infk∈N
k
nk .
The distance of it
is δ(Enc) := infk∈N δ(Enck).
In a similar manner, we also deﬁne a Dynamic ECC with
(noiseless) feedback to be a Dynamic ECC in which af-
ter each symbol transmitted we receive a feedback as to
which symbol was received. We next present a simple con-
struction of a Dynamic ECC with feedback where |Σ| =
2, |Γ|
=
3, and both the rate and distance are con-
stant.
This construction is rather straightforward and
is similar to constructions used in slightly different set-
tings (Efremenko et al., 2015).
Theorem 5.3. For any ε ∈(0, 1
2) there exists a Dynamic
ECC with noiseless feedback with |Σ| = 2, |Γ| = 3, in
which ε fraction of errors can be corrected and nk =
⌈
k
1−2ε⌉. Both encoding and correction take linear time.
We think of the message alphabet as binary Σ = {0, 1},
and to the codeword alphabet we add an additional sym-
bol Γ = {0, 1, ←}. We would think of the symbol ’←’
as a “backspace”. Intuitively, we will always compute the
message that is the decoding of what the receiver saw so
far, and if it is consistent with the input we simply send the
next bit of the input. If it is not consistent with input, we
will send a “backspace” to indicate that the last symbol is
incorrect. We will do so iteratively.
For a sequence y = (y1, . . . , yn) ∈Γ⋆, we recursively de-
ﬁne decode(y) to be decode(y[: (n −1)]) || (yn) if yn ∈
{0, 1}, and decode(y[: (n−1)])[: −1] if yn =′←′, where v[:
−1] means removing the last symbol from the vector v (un-
less its empty). As the base case, we have decode(()) = ().
For a message x = (x1, . . . , xk) ∈Σ⋆and a previously
transmitted (partial) codeword y = (y1, . . . , yn) ∈Γ⋆we
deﬁne the longest agreeing preﬁx of x and the decoding
of y as
last(x, y) := max
i {i | x[: i] = decode(y)[: i]}.
We then deﬁne the length of the wrong sufﬁx of the decod-
ing of y as suff(x, y) := len(decode(y)) −last(x, y).
Given a message x and partial codeword y, we deﬁne the
next symbol to be sent as next(x, y) =′←′ if suff(x, y) > 0,
and as next(x, y) = x[last(x, y) + 1] otherwise. Our pro-
tocol is thus simple, if x is the message and y is the code-
word received by the receiver so far (which we know us-
ing the noiseless feedback), then the next symbol we send
is next(x, y).
Lemma 5.4. Let x ∈Σ⋆be a message and y ∈Γn be a
partial codeword received by the receiver according to the
described protocol, and assume that at most εn of the sym-
bols in y were received differently than sent by the protocol.
Then, last(x, y) ≥(1 −2ε)n.
Proof. For any partial received codeword y′ we deﬁne the
potential function Φ(x, y′) := last(x, y′) −suff(x, y′).
We
ﬁrst
show
that
if
the
next
token
is
re-
ceived
correctly
then
the
potential
increases
by
one,
that
is,
Φ(x, y′
||
next(x, y′))
=
Φ(x, y′) + 1.
We show this by considering two
cases.
If suff(x, y′)
=
0 then decode(y′)
=
x[:
last(x, y′)]
and
next(x, y′)
=
x[last(x, y′) + 1],
thus decode(x, y′ || next(x, y′)) = x[: last(x, y′) + 1].
Otherwise, suff(x, y′) > 0 and next(x, y′) =′←′, and
hence suff(x, y′ || next(x, y′)) = suff(x, y′) −1.
Next, we show that if the next token is received incorrectly
then the potential decreases by one, that is Φ(x, y′ || s) =
max(0, Φ(x, y′) −1) whenever s ̸= next(x, y′). We again
consider two cases. If suff(x, y′) > 0 then we have s ∈
7

Excuse me, sir? Your language model is leaking (information)
{0, 1} and in turn suff(x, y′ || s) = suff(x, y′) + 1. Other-
wise suff(x, y′) = 0 and we either have suff(x, y′ || s) = 1
if s ̸=′←′ or have last(x, y′ || s) = max(0, last(x, y′) −1)
if s =′←′.
We conclude that if e out of the n symbols in y were re-
ceived incorrectly, then
Φ(x, y) ≥1 · (n −e) −1 · e = n −2e ≥n −2 · εn.
On the other hand,
as suff(x, y)
≥
0 we also
have last(x, y′) ≥Φ(x, y′).
Proof of Theorem 5.3. Denote by nk = ⌈
k
1−2ε⌉. Let x be
a message and y the ﬁrst nk tokens received by running the
protocol. Assume that at most εnk out of those tokens were
received incorrectly. By Lemma 5.4, we have
last(x, y) ≥(1 −2ε)nk ≥k.
Hence, decode(y)[: k] correctly retrieves x[: k].
6. Our Scheme
As in the overview of Section 3, we begin by analysing a
scheme in which only a single query is undetectable. Then,
in Section 6.1 we apply the same idea of CGZ to go from
undetectability for one query to complete undetectability.
An intuitive explanation of our scheme is covered in Sec-
tion 4.
Algorithm 3 naturally follows the sketch of Section 4, while
using the ECC (and notation) of Section 5. Let’s informally
describe the algorithm in words. The algorithm depends on
a score threshold t to be set later. We apply the Dynamic
ECC to the message PAYLOAD, and denote by next the next
symbol we are supposed to transmit according to the ECC.
We start generating response tokens following the CGZ
scheme, by comparing the actual distribution pi to the PRF
value Fk(i, next). As the PRF value depends on next, a cor-
relation between the randomness we use and the token we
are attempting to transmit is created. After each generated
response token, we also emulate the retriever: Ignoring our
knowledge of next, for every code symbol σ ∈{0, 1, ←}
we compute the score (as deﬁned in Section 3) of the re-
sponse tokens we saw so far. We wait until the score of a
some symbol σ, normalized with respect to the number of
tokens involved in the score computation, passes a thresh-
old t. That is, until
scoreσ −score len
√
score len
> t.
When this happens, we view σ as the symbol received by
the ECC receiver. While σ = next is supposed to be more
likely, the symbol could be incorrect. Whenever we add a
symbol to the code, we restart our computation of the score
Algorithm 3 One-query steganography algorithm Stegk
Data: A prompt (PROMPT), a payload (PAYLOAD), and a
secret key k
Result: Response x1, . . . , xL
i ←1
code ←()
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
next ←next(PAYLOAD, code)
while done /∈(x1, . . . , xi−1) do
pi ←Model(PROMPT, x1, . . . , xi−1)
xi ←
1[Fk(i, next) ≤pi]
score len ←score len + 1
for σ ∈{0, 1, ←} do
scoreσ ←scoreσ + s(xi, Fk(i, σ))
if (scoreσ −score len)/
√
score len > t then
code ←code || (σ)
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
next ←next(PAYLOAD, code)
break
end
end
i ←i + 1
end
and start transmitting the next code symbol. Algorithm 4
shows the retrieval process, which is identical to what is
emulated within the steganography algorithm. Note that
both algorithms have a linear running time.
We ﬁrst observe that the distribution of the response is in-
distinguishable from the distribution of the original model
(when a single query is made).
Lemma 6.1. For any Model and any PROMPT, PAYLOAD, t,
the distribution of Stegk(PROMPT, PAYLOAD) over a ran-
dom choice of key k is indistinguishable from the distribu-
tion of Model(PROMPT).
Proof. The proof is rather straightforward and follows
CGZ and its overview in Section 3. If we replace each
time Fk(i, next) is used (to determine xi) with a new uni-
formly chosen value in [0, 1], then the distribution of Model
is completely unaffected. Using a PRF instead of “fresh”
random values is indisitinguishable as long as we don’t use
the PRF on the same input more than once. As each input
to the PRF consists of the (unique) index i, we never call it
on the same input.
We should next show that Retrk(Stegk(PROMPT, PAYLOAD))
successfully retrieves PAYLOAD.
As discussed in Sec-
tion 2.4, doing so requires making assumptions on the
empirical entropy of the generated response. We prove that
8

Excuse me, sir? Your language model is leaking (information)
Algorithm 4 One-query retriever Retrk
Data: Response x1, . . . , xL, and a secret key k
Result: Retrieved payload PAYLOAD
code ←()
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
for i = 1, 2, . . ., L do
score len ←score len + 1
for σ ∈{0, 1, ←} do
scoreσ ←scoreσ + s(xi, Fk(i, σ))
if (scoreσ −score len)/
√
score len > t then
code ←code || (σ)
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
break
end
end
end
return decode(code)
a relatively weak assumption (which in particular covers
the case of natural languages) is sufﬁcient, yet it is very
likely that the proof can be adapted for other conditions as
well - as the algorithm itself is quite ﬂexible. We also note
that in the proof we don’t optimize for constants but for
simplicity (of proof), the empirical evaluation in Section 7
implies that the actual constants are far better than in the
following proof.
Deﬁnition 6.2. Let h = (h1, . . . , hL) be a sequence
of empirical entropies (i.e., non-negative numbers). We
say that h is r0-saturated if for every consecutive subse-
quence of h of length r ≥r0, the sum of entropies is at
least 10√r ln r. That is, for every r ≥r0 and 1 ≤i ≤
L −(r −1), we have Pi+r−1
j=i
hj ≥10√r ln r.
For example, if the empirical entropy in each consecutive
block of b tokens is at least some constant α > 0, then the
empirical entropies are ˜O

b2
α2

-saturated. This is because
a consecutive block of bk tokens contains at least αk en-
tropy, which is larger than 10
√
bk ln(bk) if k = ˜Ω
  b
α2

.
Hence, natural language which has this property (as dis-
cussed in Section 2.3.1) is O(1)-saturated. In fact, the en-
tropy of natural language grows linearly with the length of
the text, while our condition is merely for it to grow faster
than the square root of the length of the text. We verify
these claims empirically in Section 7.
Finally, we prove that if the empirical entropy of a response
is O(1)-saturated, and the response is of length L, then in
expectation at least the ﬁrst Θ(L) bits of the PAYLOAD are
retrieved correctly.
Theorem 6.3. Fix a model Model and an integer r0,
there exists a choice of threshold t for which the follow-
ing holds.
Let PROMPT, PAYLOAD be strings.
Condi-
tioned on the empirical entropy of a response y generated
by Stegk(PROMPT, PAYLOAD) being r0-saturated, the ex-
pected length of the preﬁxes of PAYLOAD and Retrk(y) that
identify is at least Θ(len(y)/r0).
Proof. We
need to
analyze two
quantities.
First,
when Stegk adds a symbol to the code, what is the prob-
ability it is incorrect (i.e., different than the intended next
symbol)?; Second, how many symbols do we manage to
add to the code? Or equivalently, how many response to-
kens do we usually see before adding a symbol to the code?
To answer both questions, we analyze the evolution of the
correct score (i.e., the one corresponding to the next sym-
bol) and incorrect scores from the time we start computing
them and until one of them passes the threshold.
While computing the score with respect to an incorrect
symbol, every token’s score is simply an exponential ran-
dom variable with mean 1. Denote by s1, s2, . . . the scores
of each individual token (i.e., independent Exp(1) random
variables), and by Si := Pi
j=1 sj their accumulative sums.
By Lemma 5 in CGZ we have that for any ℓ, τ > 0,
Pr[Sℓ≥ℓ+ τ
√
ℓ] ≤
4
5
τ
.
Let b ≥r0 be an integer to be chosen later. By a union
bound, the probability of the score passing the threshold t
at any time within the b ﬁrst steps is bounded by b
  4
5
t.
For the score with respect to the correct symbol, the
ﬁrst b tokens contain at least 10
√
b ln b empirical entropy,
as b ≥r0 and as we conditioned on our response being r0-
saturated. In CGZ it is shown that Sℓis still distributed as
the sum of ℓindependent Exp(1) variables, but it is now
additively shifted by the empirical entropy of those ℓvari-
ables. In particular, by Theorem 7 and Lemma 5 in CGZ, it
follows that for any τ > 0 we have
Pr[Sb < b + 10
√
b ln b −
√
τb] ≤e−τ/2.
Equivalently,
Pr
Sb −b
√
b
< 10 ln b −√τ

≤e−τ/2.
We choose t = 5 ln b and τ = (5 ln b)2 and deduce from the
above statements that: I) The probability that an incorrect
(normalized) score passed the threshold t within the ﬁrst b
steps is at most b
  4
5
t = b
  4
5
5 ln b = e(1−5 ln(5/4)) ln b <
e−(ln b)/10. II) The probability that the correct (normalized)
score passed the threshold t within the ﬁrst b steps, which
is at least the probability it was above the threshold at the
end of the b-th step, is at least 1−e−τ/2 = 1−e−25(ln b)2/2.
9

Excuse me, sir? Your language model is leaking (information)
By combining (I) and (II) we conclude that the probability
that the correct score passed the threshold within the ﬁrst b
steps, yet the two incorrect scores did not, is at least
1 −e−25(ln b)2/2 −2e−(ln b)/10,
denote this number by (1 −ε(b)). As limb→∞ε(b) = 0,
there exists a constant b0 such that for every b ≥b0 it holds
that ε(b) ≤
1
3. We set b = max(r0, b0). Note that b0
is a universal constant independent of r0 and other parame-
ters. We conclude that with probability at least 2
3 the correct
symbol is transmitted within the ﬁrst b tokens, and in par-
ticular the symbol is transmitted correctly with probability
at least 2
3.
As the probability of incorrectly transmitting a symbol is at
most 1
3 < 1
2, we can use Theorem 5.3 to conclude that if n
code symbols are transmitted overall, then the ﬁrst Θ(n)
bits of the PAYLOAD are retrieved correctly. It is thus only
left to analyze the number of transmitted code symbols.
We
again
consider
the
same
inequality
from
before,
that
holds
for
any
b′
≥
r0, τ
>
0,
Pr
h
Sb′ −b′
√
b′
< 10 ln b′ −√τ
i
≤
e−τ/2.
By choos-
ing τ
=
(5 ln b′)2 we observe that for any b′
>
b
we have 10 ln b′ −√τ = 5 ln b′ > 5 ln b = t.
And
thus, Pr
h
Sb′ −b′
√
b′
< t
i
≤e−25(ln b′)
2/2. Denote by ℓthe
random variable which is the ﬁrst step in which the score
(w.r.t. the correct symbol) passed the threshold t, by the
above inequality we have
E [ℓ] =
∞
X
i=1
Pr [ℓ≥i]
=
b
X
i=1
Pr [ℓ≥i] +
∞
X
i=b+1
Pr [ℓ≥i]
<
b
X
i=1
1 +
∞
X
i=b+1
e−25
2 (ln i)2
= b + O(1).
As the correct symbol is expected to pass the score thresh-
old after b + O(1) response tokens, in particular a symbol
is expected to be transmitted in the protocol at least once
every b + O(1) response tokens.
6.1. Complete Undetectability
To move from undetectability of a single response to the
general undetectability deﬁned in Deﬁnition 2.6, we simply
repeat the “trick” of CGZ as overviewed in Section 3.
Our revised algorithm partitions the generation of response
tokens into three parts:
1. We use real randomness to generate tokens and count
the amount of empirical entropy used in the process,
until enough (at least λ) empirical entropy was seen,
we call the preﬁx of tokens generated in this step r.
2. We begin generating tokens using the PRF, with both r
and the index as inputs. We don’t yet submit code
symbols and wait until the score passes λ. We do this
step to leave a signal to the detector with regards to
what preﬁx r was chosen by the generation algorithm.
3. We now run the one-query scheme, while adding r as
an additional input to the PRF calls.
We remark that the second part can be avoided, as the cor-
rect r can be detected implicitly during the third part, but
for simplicity of presentation we include it. With this struc-
ture, the proofs are rather straightforward. The pseudo-
code for Stegk appears in Algorithm 5, and for Retrk in
Algorithm 6.
Theorem 6.4. Algorithm 5 is undetectable as deﬁned in
Deﬁnition 2.6.
Proof. This follows from Theorem 11 in CGZ, as the in-
puts to the PRF in the generation process are all unique
within a single query, and all contain r which ever re-
peats within different queries only with negligible proba-
bility.
Theorem 6.3 also remains correct as-is, besides that we
“lose” O(λ) empirical entropy to Parts 1 and 2. As long
as L = Ω(λ) then, the same theorem statement still holds.
While the running time of Stegk is unaffected, the running
time of Retrk is now quadratic instead of linear. This can
be avoided by truncating the veriﬁcation of r, but we do not
do so for the sake of simplicity.
7. Empirical Evaluation
We implemented Algorithms 3 and 4 from Section 6, that
provide undetectability for a single query.1 We did so for
simplicity and as we only aim to evaluate the new contribu-
tions of this paper.
In Figure 2, we estimate the number of message bits we can
hide in a response of a certain length. For each length of
response, we ran our scheme for 100 times using the LLM
model GPT-2 (Radford et al., 2019) on a randomly chosen
prompt from the list of example prompts provided by Ope-
nAI on their GPT-2 webpage.2 We ran it with threshold
parameter t = 2, which we didn’t optimize. As expected,
it is evident that the number of hidden bits grows linearly
with the length of the response.
1Link to implementation: https://github.com/OrZamir/steg.
2https://openai.com/research/better-language-models
10

Excuse me, sir? Your language model is leaking (information)
Algorithm 5 Steganography algorithm Stegk
Data: A prompt (PROMPT), a payload (PAYLOAD), and a
secret key k
Result: Response x1, . . . , xL
i ←1
H ←0
r score ←0
r score len ←0
code ←()
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
next ←next(PAYLOAD, code)
while done /∈(x1, . . . , xi−1) do
pi ←Model(PROMPT, x1, . . . , xi−1)
if H < λ then
// Part 1
Sample (xi, p) ←(1, pi) with probability pi, oth-
erwise (0, 1 −pi)
H ←H −log p
if H ≥λ then
r ←(x1, . . . , xi)
end
else if (r score −r score len) ≤λ
√
r score len then
// Part 2
xi ←
1[Fk(r, i, None) ≤pi]
r score ←r score + s(xi, Fk(r, i, None))
r score len ←r score len + 1
else
// Part 3
xi ←
1[Fk(r, i, next) ≤pi]
score len ←score len + 1
for σ ∈{0, 1, ←} do
scoreσ ←scoreσ + s(xi, Fk(r, i, σ))
if (scoreσ −score len)/
√
score len > t then
code ←code || (σ)
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
next ←next(PAYLOAD, code)
break
end
end
end
i ←i + 1
end
Algorithm 6 Retriever algorithm Retrk
Data: Response x1, . . . , xL, and a secret key k
Result: Retrieved payload PAYLOAD
code ←()
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
for j = 1, 2, . . ., L do
r ←(x1, . . . , xj)
r score ←0
r score len ←0
for i = j + 1, j + 2, . . . , L do
if (r score −r score len) ≤λ
√
r score len then
// Verify r
r score ←r score + s(xi, Fk(r, i, None))
r score len ←r score len + 1
else
// Correct r found
score len ←score len + 1
for σ ∈{0, 1, ←} do
scoreσ ←scoreσ + s(r, xi, Fk(r, i, σ))
if (scoreσ −score len)/
√
score len > t
then
code ←code || (σ)
scoreσ ←0 for σ ∈{0, 1, ←}
score len ←0
break
end
end
end
end
if code ̸= () then
return decode(code)
end
end
return False
11

Excuse me, sir? Your language model is leaking (information)
20
40
60
80
100
0
5
10
15
20
4.19
8.01
11.65
14.75
17.87
Response length, in tokens.
Hidden payload length, in bits.
Figure 2. Plot of the number of successfully hidden payload bits,
by length of response. Experiments ran on GPT-2 with a random
choice of an example prompt taken from the OpenAI website. The
experiment was performed 100 times for each response length.
In Figure 3, we detail the decoding process of the example
shown in Figure 1. This example was generated using the
7B parameters version of the Llamma 2 model developed
by Meta (Touvron et al., 2023). For this example we im-
plemented a slight optimization in which we only took into
consideration the ﬁrst four bits out of the ﬁfteen bit repre-
sentation of each token when computing the scores. This
optimisation was useful as the Llama 2 model has a rela-
tively low generation entropy.
We note that our aim was to implement our scheme in the
simplest possible manner, and we did not study heuristics
or optimizations to improve the number of hidden bits. We
expect that experimental work would improve those num-
bers signiﬁcantly.
8. Limitations and Open Problems
The main issue we did not discuss so far is robustness
to editing.
That is, can the payload be recovered even
if the model’s response is somehow edited? We mainly
leave dealing with robustness to future work, yet next
list a couple of observations regarding robustness.
In
CGZ (Christ et al., 2023), the watermarking scheme is
adapted to “restart” once-in-a-while so that the watermark
will be detectable from any long enough consecutive sub-
string of the response (and not only from the entire re-
sponse). The same modiﬁcation can easily be applied to
our scheme as well, making the payload retrievable from
any long enough substring out of the model’s response. At
the other end of the spectrum, it is known that under certain
Prompt: Write an email asking my professor Prof. Han-
nity to not make the ﬁnal exam in Machine Learning 101
too difﬁcult.
Response:
Dear Professor Hannity,
I hope this email ﬁnds you well. I am writing to respect-
fully request that you consider the level of difﬁculty for
the ﬁnal exam in Machine Learning 101. While I am
conﬁdent in my understanding of the course materials
and have put in a significant 0 amount of effort through-
out the semester, I do 1 have 1 concerns 0 about the
potential 1 difﬁculty ←of the ﬁnal exam.
As you may recall, several students in my previous sec-
tions of ←
Machine 1
Learning have found the ﬁnal
exam to be very challenging 1
, leading to frustration
1 and ←disappointment 0 . While I ←understand
that the course is intended to push students to their limits
and beyond, I believe that some relaxation of the latter
1 may be in order.
I would kindly ask that you consider reducing the difﬁ-
culty of the ﬁnal exam or offering some additional sup-
ports or 1
resources to help students prepare. I 0
be-
lieve that this could 1 enhance 0
Error Correcting Code: 01101←←111←0←11010
Hidden Payload: “OZ”, encoded as 01111 11010.
Figure 3. A breakdown of the decoding algorithm for the example
in Figure 1.
conditions powerful users can edit any watermark out of a
model’s response (Zhang et al., 2023; Christ et al., 2023).
Intuitively, a complete rephrasing of the response, for ex-
ample, is supposed to remove any watermark. The previous
empirical works on watermarks and steganography, that do
not guarantee undetectability, showcase some robustness
to certain types of edits (e.g., changing only a small frac-
tion of tokens). It is thus very interesting to pin down the
strongest model of edits with respect to which undetectable
watermarking or steganography can be made robust.
We also stress that we didn’t make any non-trivial attempts
in heuristically improving the presented scheme. In partic-
ular, it is open and highly likely that the rate of encoding
the payload can be signiﬁcantly improved with comparison
to the evaluation of Section 7. Our empirical evaluation
should be viewed merely as a proof-of-concept, while the
theoretical parts and proof of undetectability are the main
contributions of this paper.
References
Abdelnabi, S. and Fritz, M.
Adversarial watermarking
transformer: Towards tracing text provenance with data
hiding. In 2021 IEEE Symposium on Security and Pri-
12

Excuse me, sir? Your language model is leaking (information)
vacy (SP), pp. 121–140. IEEE, 2021.
Berlekamp, E. R.
Block coding with noiseless feed-
back. PhD thesis, Massachusetts Institute of Technology,
1964.
Chakraborty, S., Bedi, A. S., Zhu, S., An, B., Manocha, D.,
and Huang, F. On the possibilities of AI-generated text
detection. arXiv preprint arXiv:2304.04736, 2023.
Chen, R., Liu, H., and Altmann, G. Entropy in different
text types. Digital Scholarship in the Humanities, 32(3):
528–542, 2017.
Christ, M., Gunn, S., and Zamir, O.
Undetectable
watermarks for language models.
arXiv preprint
arXiv:2306.09194, 2023.
Cover, T. M. The role of feedback in communication. In
Performance Limits in Communication Theory and Prac-
tice, pp. 225–235. Springer, 1988.
de Witt, C. S., Sokota, S., Kolter, J. Z., Foerster, J.,
and Strohmeier, M.
Perfectly secure steganography
using minimum entropy coupling.
arXiv preprint
arXiv:2210.14889, 2022.
Dedi´c, N., Itkis, G., Reyzin, L., and Russell, S. Upper and
lower bounds on black-box steganography. Journal of
Cryptology, 22:365–394, 2009.
Efremenko, K., Gelles, R., and Haeupler, B. Maximal noise
in interactive communication over erasure channels and
channels with feedback. In Proceedings of the 2015 Con-
ference on Innovations in Theoretical Computer Science,
pp. 11–20, 2015.
Fernandez, P., Chafﬁn, A., Tit, K., Chappelier, V.,
and Furon, T.
Three bricks to consolidate water-
marks for large language models.
arXiv preprint
arXiv:2308.00113, 2023.
Genzel, D. and Charniak, E. Entropy rate constancy in text.
In Proceedings of the 40th annual meeting of the Associ-
ation for Computational Linguistics, pp. 199–206, 2002.
Gilbert, E. N. A comparison of signalling alphabets. The
Bell system technical journal, 31(3):504–522, 1952.
Goldreich, O., Goldwasser, S., and Micali, S. How to con-
struct random functions. Journal of the ACM (JACM),
33(4):792–807, 1986.
Hamming, R. W. Error detecting and error correcting codes.
The Bell system technical journal, 29(2):147–160, 1950.
H˚astad, J., Impagliazzo, R., Levin, L. A., and Luby, M.
A pseudorandom generator from any one-way function.
SIAM Journal on Computing, 28(4):1364–1396, 1999.
Hopper, N., von Ahn, L., and Langford, J. Provably secure
steganography. IEEE Transactions on Computers, 58(5):
662–676, 2008.
Jawahar, G., Abdul-Mageed, M., and Lakshmanan, L. V.
Automatic detection of machine generated text: A criti-
cal survey. arXiv preprint arXiv:2011.01314, 2020.
Justesen, J. Class of constructive asymptotically good alge-
braic codes. IEEE Transactions on information theory,
18(5):652–656, 1972.
Kirchenbauer,
J.,
Geiping,
J.,
Wen,
Y.,
Katz,
J.,
Miers,
I.,
and Goldstein,
T.
A watermark for
large language models.
CoRR,
abs/2301.10226,
2023a.
doi:
10.48550/arXiv.2301.10226.
URL
https://doi.org/10.48550/arXiv.2301.10226.
Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I.,
and Goldstein, T. A watermark for large language mod-
els. arXiv preprint arXiv:2301.10226, 2023b.
Krishna, K., Song, Y., Karpinska, M., Wieting, J., and
Iyyer, M. Paraphrasing evades detectors of AI-generated
text, but retrieval is an effective defense. arXiv preprint
arXiv:2303.13408, 2023.
Munyer, T. and Zhong, X. Deeptextmark: Deep learning
based text watermarking for detection of large language
model generated text. arXiv preprint arXiv:2305.05773,
2023.
Qiang, J., Zhu, S., Li, Y., Zhu, Y., Yuan, Y., and Wu, X.
Natural language watermarking via paraphraser-based
lexical substitution. Artiﬁcial Intelligence, pp. 103859,
2023.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Sutskever, I., et al. Language models are unsupervised
multitask learners. OpenAI blog, 1(8):9, 2019.
Sadasivan, V. S., Kumar, A., Balasubramanian, S., Wang,
W., and Feizi, S. Can AI-generated text be reliably de-
tected? arXiv preprint arXiv:2303.11156, 2023.
Schulman, L. J. Deterministic coding for interactive com-
munication. In Proceedings of the twenty-ﬁfth annual
ACM symposium on Theory of computing, pp. 747–756,
1993.
Schulman, L. J.
Coding for interactive communication.
IEEE transactions on information theory, 42(6):1745–
1756, 1996.
Shi, Y. and Lei, L. Lexical richness and text length: An
entropy-based perspective. Journal of Quantitative Lin-
guistics, 29(1):62–79, 2022.
13

Excuse me, sir? Your language model is leaking (information)
Sipser, M. and Spielman, D. A. Expander codes. IEEE
transactions on Information Theory, 42(6):1710–1722,
1996.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., et al. Llama 2: Open foundation and ﬁne-
tuned chat models.
arXiv preprint arXiv:2307.09288,
2023.
Varshamov, R. R. Estimate of the number of signals in error
correcting codes. Docklady Akad. Nauk, SSSR, 117:739–
741, 1957.
Wang, L., Yang, W., Chen, D., Zhou, H., Lin, Y., Meng,
F., Zhou, J., and Sun, X.
Towards codable text wa-
termarking for large language models. arXiv preprint
arXiv:2307.15992, 2023.
Yoo, K., Ahn, W., Jang, J., and Kwak, N. Robust natural
language watermarking through invariant features. arXiv
preprint arXiv:2305.01904, 2023a.
Yoo, K., Ahn, W., and Kwak, N.
Advancing beyond
identiﬁcation: Multi-bit watermark for language models.
arXiv preprint arXiv:2308.00221, 2023b.
Zhang, H., Edelman, B. L., Francati, D., Venturi, D., Ate-
niese, G., and Barak, B. Watermarks in the sand: Im-
possibility of strong watermarking for generative models.
arXiv preprint arXiv:2311.04378, 2023.
14
