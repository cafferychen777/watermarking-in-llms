Preprint.
Optimizing watermarks for large language models
Bram Wouters 1
Abstract
With the rise of large language models (LLMs)
and concerns about potential misuse, watermarks
for generative LLMs have recently attracted much
attention. An important aspect of such water-
marks is the trade-off between their identifiability
and their impact on the quality of the generated
text. This paper introduces a systematic approach
to this trade-off in terms of a multi-objective opti-
mization problem. For a large class of robust, ef-
ficient watermarks, the associated Pareto optimal
solutions are identified and shown to outperform
the currently default watermark.
1. Introduction
In recent years, transformer-based LLMs have proven to be
remarkably powerful. Their societal impact is potentially
enormous. As a consequence of their rapid rise, concerns
about potential misuse have been raised. One can think of,
for example, plagiarism (Meyer et al., 2023), online pro-
paganda (Goldstein et al., 2023), examination in education
(Milano et al., 2023), misinformation (Vincent, 2022) and
copyright infringement (Rillig et al., 2023). One possible
strategy to partially address these concerns is to ensure that
LLM-generated text can be algorithmically distinguished
from human-generated text by means of a watermark.
Initialized by Aaronson (2022) and the seminal work of
Kirchenbauer et al. (2023a), the idea of watermarking LLM-
generated text has attracted much attention, in both the
scientific field (see Section 5 for an overview) and the in-
dustry (Bartz & Hu, 2023). Generally speaking, the process
of text generation by a LLM would be adjusted in a con-
trollable manner. Based on the generated text, a detector
with knowledge of the watermarking strategy is then able to
identify a text as generated by an LLM. This is usually done
with a hypothesis test, where the null hypothesis is that the
text has been generated by a human being.
This paper follows largely the watermarking strategy of
Kirchenbauer et al. (2023a). Causal LLMs typically gen-
1University of Amsterdam. Correspondence to: Bram Wouters
<b.m.wouters@uva.nl>.
erate text word-by-word. Before a word is generated, the
complete vocabulary of the LLM is split in two disjunct lists
labelled green and red. This split is pseudo-random, where
the seed is determined by the previous word(s). Green-list
words are then sampled with a higher probability than the
original LLM prescribes, and red-list words with a lower
probability.
A detector with knowledge of the pseudo-
random green-red split can count the number of green-list
words in a text. If this number is larger than one would ex-
pect from a text generated without knowledge of the green-
red split (e.g., a human-generated text), the null hypothesis
is rejected and the text is attributed to an LLM.
There are many perspectives (see Section 5 for an overview)
of what a good watermark for an LLM consists of, and
even their usefulness altogether is under debate (Sadasivan
et al., 2023; Jiang et al., 2023; Zhang et al., 2023). Roughly
speaking, the quality of a watermark is assessed along four
axes. A watermark must be
• identifiable, meaning that a detector is able to correctly
identify the generator (LLM vs. human) of a text.
• stealthy, meaning that a watermark does not noticeably
change the quality of the generated text.
• robust against (moderate) post-generation adjustments
of the text that could obfuscate the watermark.
• efficient at generation and detection, i.e., without the
need for computationally costly processes.
This paper focusses on the trade-off between identifiability
and stealthiness of watermarks. The probability that the
hypothesis test of the detector draws the correct conclu-
sion increases when the watermark more strongly promotes
green-list tokens. However, enforcing green-list tokens too
strongly can degrade the quality of the text in unacceptable
manners. We will refer to this trade-off between the quality
of test and text as the test-text trade-off.
Our contribution. For a large class of robust, efficient wa-
termarks based on the green-red split of the vocabulary, we
translate the test-text trade-off into a multi-objective opti-
mization problem and identify the associated Pareto optimal
solutions. We empirically validate the optimality of the
solutions and show that they outperform the watermark of
1
arXiv:2312.17295v1  [cs.CR]  28 Dec 2023

Preprint.
Optimizing watermarks for large language models
Kirchenbauer et al. (2023a) in terms of the test-text trade-
off. The contribution of this paper is therefore twofold. To
the best of our knowledge, this is the first systematic ap-
proach to optimizing the trade-off between identifiability
and stealthiness of watermarks for LLMs. Secondly, since
we optimize over a large class of robust, efficient water-
marks, we believe that the optimal watermarks introduced
in this paper have an excellent standing with respect to the
four criteria for good watermarks for LLMs.
2. Watermarks for LLMs
In this section a class of watermarks is defined, over which
the test-text trade-off will be optimized. The class can be
seen as a generalization of the watermark introduced by
Kirchenbauer et al. (2023a), based on a green-red split of
the vocabulary.
In the context of LLMs, a text is typically represented as
a sequence of tokens. Consider the sequence of random
variables V1, V2, . . . , VT , where Vt corresponds to the token
at position t. They have support set V of size N = |V|,
which is the vocabulary of the LLM. In addition, there
is the prompt V:1 = (V0, V−1, V−2, . . .) , whose length is
left unspecified because most modern causal LLMs do not
require a fixed prompt length. The joint probability mass
function (pmf) is
P[VT , VT −1, . . . V1, V:1] =
T
Y
t=1
P[Vt|V:t] × P[V:1] ,
(1)
where V:t is the subsequence of tokens prior to position t,
including the prompt. The causal LLM specifies P[Vt|V:t],
whereas P[V:1] represents the distribution of the text prompts
under consideration. Text generation occurs token-by-token
through sampling from the conditional pmf P[Vt|V:t].
Before defining the watermark, we introduce a function
gγ : V∗→Θ, where Θ is the space of all subsets of size
⌊γN⌋of the vocabulary V. The hyperparameter γ ∈(0, 1)
is fixed. Given a sequence v:t ∈V, the set Gt = gγ(v:t)
contains the so-called green-list tokens. The tokens in the
complement V \ Gt are the red-list tokens. This partitioning
of the vocabulary is pseudo-random, with a seed determined
by a hash of v:t and a key. The detector of the watermark
has the key and is therefore able to reconstruct the list of
green tokens for each position t in the sequence.
This paper considers a large class of watermarks, defined by
the conditional probability
˜P[Vt|V:t] = P[Vt|V:t] ×



1 + ∆(pt,Gt)
Γt
if Vt ∈Gt,
1 −∆(pt,Gt)
1−Γt
if Vt /∈Gt,
(2)
where pt is the function pt(v) = P[Vt = v|V:t] for v ∈V,
representing the conditional pmf of the LLM at position t,
and Γt = P[Vt ∈Gt|V:t] is the conditional probability that
token t is a green-list token. A watermark is specified by a
so-called shift function ∆: Ξ × Θ →[0, 1], where Ξ is the
space of all pmfs over the vocabulary V. By demanding that
∆(pt, Gt) ≥0, the shift function increases green-list proba-
bilities and decreases red-list probabilities. To be concrete,
˜P[Vt ∈Gt|V:t] = Γt + ∆(pt, Gt), i.e., the shift function is
the increase due to the watermark of the conditional proba-
bility that a token is on the green list. For consistency we
also need to demand that ∆(pt, Gt) ≤1 −Γt, where it is
important to realize that Γt, pt and Gt are all functions of
V:t. A watermarked LLM generates text by sampling from
˜P[Vt|V:t].
It must be emphasized that ∆(·, ·) in Equation (2) is not a
function of Vt. This creates the important conceptual ben-
efit that the watermark does not alter the relative proba-
bilities among green-list tokens, i.e., ˜P[Vt|V:t, Vt ∈Gt] =
P[Vt|V:t, Vt ∈Gt]. In other words, the watermark rescales
the probabilities of all green-list tokens by the same factor,
and vice versa for red-list tokens. In this sense, the type of
watermarks defined by Equation (2) are minimal. This must
be contrasted by several recent proposals for watermarks
that are also based on a green-red split of the vocabulary
(Fang et al., 2023; Fu et al., 2023; Li et al., 2023b; Chen
et al., 2023). They aim to mitigate the test-text trade-off
by defining a rescaling factor per individual token, usually
based on the semantic properties of that token. This more
flexibile deviation from the original LLM comes with po-
tential risks, as it is difficult to oversee all consequences it
can have on text quality.
2.1. The KGW watermark
Arguably the simplest example of the class of watermarks
defined by Equation (2) is the so-called hard watermark,
∆HARD(pt, Gt) = 1 −Γt, implying that ˜P[Vt ∈Gt|V:t] = 1.
Green-list tokens are generated with probability one. This
watermark is maximally strong, but at the same time impacts
the text quality in an unacceptable manner (Kirchenbauer
et al., 2023a).
This is mitigated by the introduction of what we call the
KGW watermark, after the first three author names of
Kirchenbauer et al. (2023a). Green-list logits are shifted by
a watermark parameters δ ≥0, while red-list logits are left
unaltered,
˜PKGW[Vt|V:t] =
exp[ℓ(Vt|V:t) + δ IGt(Vt)]
P
V ′
t ∈V exp[ℓ(V ′
t |V:t) + δ IGt(V ′
t )],
where ℓ(Vt|V:t) are the logits of the LLM and IGt(·) is an
indicator function. As already mentioned, the KGW wa-
termark is a member of the class of watermarks defined by
2

Preprint.
Optimizing watermarks for large language models
Equation (2), with
∆KGW(pt, Gt) = Γt(1 −Γt)(eδ −1)
1 −Γt + Γteδ
.
The parameter δ controls the test-text trade-off, where a
large δ favors the former.
The logic behind the KGW watermark is that green-list
tokens are only substantially favored if this does not hurt text
quality. Selecting a green-list token would hurt text quality
if all meaningfully probable tokens are on the red list. But
these tokens have much higher logits and a moderate shift δ
of green-list logits will not change that. We emphasize that
this particular choice of watermark is based on a heuristic
argument regarding the test-text trade-off, rather than an
optimization objective.
The KGW watermark is generally considered to be robust
(Shi et al., 2023; Kirchenbauer et al., 2023b; Piet et al.,
2023) and efficient (Wu et al., 2023). Other watermarks de-
fined by Equation (2) only differ from the KGW watermark
through the shift function, which does not impact robustness.
Instead, robustness of watermarks based on a green-red split
is typically determined by the choice of the green-list gener-
ator gγ (Liu et al., 2023b). And unless the shift function is
computationally expensive, which will not be the case in the
applications discussed in this paper, all watermarks defined
by Equation (2) have a comparable efficiency. We therefore
conclude that the watermarks introduced in this paper can
be considered robust and efficient.
3. Optimizing watermarks
Optimization of the test-text trade-off requires a precise
definition of both test and text quality. A simple criterion
for a good test is a high number of generated green-list
tokens, compared to the baseline of the non-watermarked
LLM. Let Ng be the number of green-list tokens in the
sequence V1, V2, . . . , VT . The expected number of green-
list tokens shifts, as a consequence of the watermark, by
∆Ng = ˜E[Ng] −E[Ng] , where E[·] is the expectation with
respect to the joint pmf of Equation (1) and the ˜E[·] is the
watermarked counterpart. It follows that
∆Ng =
T
X
t=1
˜E[∆(pt, Gt)] ,
(3)
as the shift function is the increase (due to the watermark)
of the probability that the token is a green-list token.
One common measure for text quality of a LLM is the
perplexity, which is the exponential of the negative (normal-
ized) log-likelihood. We consider the log-perplexity
log PPL = −1
T
T
X
t=1
log P[Vt|V:t],
and note that a high text quality corresponds to a low log-
perplexity. The shift in expected log-perplexity due to the
watermark, ∆log PPL = ˜E[log PPL] −E[log PPL] , is given
by
∆log PPL = 1
T
T
X
t=1
˜E[∆(pt, Gt)B(pt, Gt)] ,
(4)
where
B(pt, Gt) =
X
v∈V
Γt −IGt(v)
Γt(1 −Γt) pt(v) log pt(v).
This quantity B(pt, Gt) is the expected rate of change of the
log-perplexity, given V:t, due to a shift in the conditional
probability that the token is a green-list token. Roughly
speaking, B(pt, Gt) is large when there are no or few green-
list tokens with a (relatively) large probability. It should be
interpreted as the expected damage that promoting green-
list tokens has on the text quality. Equations (3) and (4)
are derived under the mild assumption that expectations are
unaffected by watermark-induced changes in the distribution
of the precursor V:t. For details, see Appendix A.
We are now in a position to find a watermark that optimizes
the text-test trade-off. Let Υ be the set of all shift func-
tions ∆(·, ·), as defined in Section 2, representing the class
of watermarks defined in Equation 2. The aim to maxi-
mize test quality and simultaneously minimize a decrease in
text quality translates into the multi-objective optimization
problem
max
∆∈Υ ∆Ng
and
min
∆∈Υ ∆log PPL,
(5)
which has Pareto optimal solutions parametrized by β ≥0,
∆OPT(pt, Gt) =
1 −Γt
if
B(pt, Gt) ≤β,
0
if
B(pt, Gt) > β.
(6)
We will call this the OPT watermark. For a token at position
t there are two options. If the expected damage to the text
quality is small, at most β, then the watermark is maximally
enforced by generating a green-list token with probability
one. Otherwise, no watermark is imposed and the token is
sampled from the original LLM. In other words, tokens that
are expected to damage the text quality the least are maxi-
mally watermarked before other tokens get any watermark
at all.
Note that maximally watermarking tokens for which
B(pt, Gt) < 0 is actually favorable for the text quality, be-
cause you make the sampling more greedy (towards green-
list tokens) and this potentially decreases the perplexity. In
fact, in the context of LLMs without watermarks greedy
sampling minimizes E[log PPL] . It should also be noted
that Equation (6) was obtained under the assumption that
all Vt, for t = 1, 2, . . . , T, are identically distributed. For
a derivation of the Pareto optimality of the watermarks in
Equation (6), see Appendix A.
3

Preprint.
Optimizing watermarks for large language models
3.1. Other optimized watermarks
The choice of optimization objectives in Equation (5) is not
unique. Different objectives could lead to different optimal
watermarks. When a detector performs a hypothesis test
based on a sequence of size T, it will typically count the
number of green-list words Ng and reject the null hypothesis
of a human-generated text if Ng ≥n∗. Here, n∗is set
beforehand and corresponds to a false-positive rate α∗=
P[Ng ≥n∗] . This is the probability of falsely attributing
a text to an LLM (type-I error). The quality of the test is
then commonly quantified as the power/sensitivity πn∗=
˜P[Ng ≥n∗] , i.e., the probability of correctly identifying an
LLM-generated text.
Suppose we want to maximize the power of the test for
the class of watermarks of Equation (2), i.e., consider the
multi-objective optimization problem
max
∆∈Υ πn∗
and
min
∆∈Υ ∆log PPL.
(7)
It has the same Pareto optimal solutions, given by Equation
(6), as the previous optimization problem, if we assume
(again) that the Vt are identically distributed and additionally
that
(i) the green-red split is unbiased, E[Γt] = γ,
(ii) the events Vt ∈Gt and Vt′ ∈Gt′ are independent for
all t ̸= t′.
The number of green-list tokens is then binomially dis-
tributed, Ng ∼BIN(T, γ + ˜E[∆(pt, Gt)]), and this means
that maximizing the power of the test is equivalent to maxi-
mizing ∆Ng (see Appendix A.1 for details).
Regarding text quality, a possible alternative objective is to
minimize the expectation of
1
T
T
X
t=1
(−log P[Vt|V:t])2 = log PPL2
+ 1
T
T
X
t=1
(−log P[Vt|V:t] −log PPL)2,
which can be intepreted as the bias with respect to zero
(squared) plus the variance. The idea behind this objective
is that it seeks to reduce the overall perplexity of a sequence,
but also large deviations from this overall perplexity at the
level of individual tokens. When simultaneously maximiz-
ing the power of the test, the Pareto optimal watermarks are
now
∆OPT′(pt, Gt) =
1 −Γt
if
B′(pt, Gt) ≤β′,
0
if
B′(pt, Gt) > β′,
(8)
parametrized by β′ ≥0, where
B′(pt, Gt) =
X
v∈V
IGt(v) −Γt
Γt(1 −Γt) pt(v)[log pt(v)]2.
4. Experiments
Our experimental setup follows largely Kirchenbauer et
al. (2023a). From the C4 dataset (Raffel et al., 2020) a
sample of 500 (news) articles is drawn randomly. For each
text the first (at most) 200 tokens serve as prompt, while
the rest is discarded. Based on a prompt, 64 sequences of
length T = 30 are generated by means of the OPT-350m
causal LLM (Zhang et al., 2022), or by a watermarked
version thereof. With this setup both V:1 and V1, . . . , VT are
sampled. Following Kirchenbauer et al. (2023a), we let the
list of green tokens for position t be determined by only the
token at position t−1, i.e., Gt = gγ(vt−1). This may not be
optimal in the trade-off between tampering-resistancy and
invisibility (Liu et al., 2023b), but this is outside the scope
of this paper. For more details about the experiments, see
Appendix B.
2.5
3.0
3.5
4.0
4.5
5.0
˜E[log PPL]
10
15
20
25
30
˜E[Ng]
0.1
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
5.0
6.0 7.0
10.0
-2.0
-1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.03.54.0
5.0
6.0 7.0
8.0
10.0
E[log PPL]
˜EHARD[log PPL]
optimal bound
LLM
HARD
KGW (δ)
OPT (β)
Figure 1. Test quality, measured as the expected number of green-
list tokens, versus text quality, measured as the expected log-
perplexity, is shown for different watermarks. For completeness,
the original language model without watermark is included (LLM).
Also shown is the Pareto optimal bound. Error bars (vertical and
horizontal) are omitted, as they are never larger than the marker
sizes.
In Figure 1 test quality, measured in terms of the expected
number of green-list tokens, and text quality, measured in
terms of the expected log-perplexity, are plotted for the hard,
KGW and OPT watermarks (the latter is defined in Equa-
tion (6)). The KGW and OPT watermarks are plotted for
different values of their respective hyperparameters δ and
β. As expected, OPT outperforms KGW in terms of the test-
text trade-off. We also show the curve of E[∆OPT(pt, Gt)T]
against E[∆OPT(pt, Gt)B(pt, Gt)] , parametrized by β. Un-
der the assumptions that V1, V2, . . . , VT are identically dis-
tributed and that the distribution of (pt, Gt) does not shift
due to the presence of a watermark, this represents the Pareto
optimal bound for the class of watermarks defined in Equa-
tion (2). The OPT watermark attains this bound, thereby
4

Preprint.
Optimizing watermarks for large language models
validating these assumptions. We stress that this is not en-
tirely trivial, as the optimal bound was computed with the
non-watermarked LLM.
Note that, as explained in Section 3, as long as β < 0
increasing test quality also increases text quality. These
token positions have so much probability mass on the green-
list tokens, that enforcing a token to be from the green list
increases the expected log-perplexity. Strictly speaking, the
solutions for β < 0 are not Pareto optimal, because β = 0
is better.
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
πn∗
0.5
1.0
1.5
-2.0
-1.0
0.0
0.5
1.0
n∗= 12 (α∗= 0.05066)
E[log PPL]
˜EHARD[log PPL]
optimal bound
LLM
HARD
KGW (δ)
OPT (β)
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
πn∗
0.5
1.0
1.5
2.0
2.5
-1.0
0.0
0.5
1.0
1.5
2.0
n∗= 15 (α∗= 0.00275)
2.5
3.0
3.5
4.0
4.5
5.0
˜E[log PPL]
0.0
0.2
0.4
0.6
0.8
1.0
πn∗
1.0
1.5
2.0
2.5
3.0
0.0
0.5
1.0
1.5
2.0
2.53.0
n∗= 18 (α∗= 0.00005)
Figure 2. Test quality, measured as the power, versus text quality,
measured as the expected log-perplexity, is shown for different
tests (n∗= 12, 15, 18) and watermarks. For completeness, the
original language model without watermark is included (LLM).
Also shown is the Pareto optimal bound. Error bars (vertical and
horizontal) are omitted, as they are never larger than the marker
sizes.
Figure 2 also shows test versus text quality, with the dif-
ference that now test quality is measured in terms of the
power of the test. The results are plotted for different tests,
corresponding to rejection regions n∗= 12, 15, 18 and asso-
ciated false-positive rates α∗. Note that the OPT watermark
(for β ≈2.0) achieves a high power, while the expected log-
perplexity is the same as for the original LLM. At similar
levels of test quality, the KGW watermark shows a signif-
icant decrease in text quality. Also note that strong OPT
watermarks (for β > 2.0) show a deviation from the Pareto
optimal bound. In Section 4.2 we argue that this discrepancy
can be attributed to a breakdown of independence, which
was assumed in Section 3.1.
0.01
0.02
10
20
30
˜E[Ng]
q = 0.01
0.5
1.0
10
20
30
q = 0.1
3
4
10
20
30
˜E[Ng]
q = 0.5
5
6
7
−log P[Vt|V:t]
10
20
30
q = 0.9
8
10
12
−log P[Vt|V:t]
10
20
30
˜E[Ng]
q = 0.99
LLM
HARD
KGW (δ)
OPT (β)
Figure 3. The qth percentile of −log P[Vt|V:t] is shown for differ-
ent watermarks and q = 0.01, 0.1, 0.5, 0.9 and 0.99.
Additional descriptive statistics of the effect of different
watermarks on text quality can be found in Figure 3, where
the qth percentile of −log P[Vt|V:t] is plotted for different
values of q. The relative difference in percentiles between
the KGW and OPT watermarks decreases with increasing q
and is virtually absent when q = 0.99. In other words, the
OPT watermark is not better than the KGW watermark for
tokens with a very large log-perplexity. This is not neces-
sarily problematic, as the original LLM already generates
these very large log-perplexities.
Finally, we ran the same experiments for the other optimal
watermark OPT′, defined in Equation (8). We found that the
OPT and OPT′ watermarks perform rather similarly. See
Appendix C for the results of these experiments.
4.1. Hyperparameter tuning
Watermarks defined by Equation (2) have a hyperparameter
γ, the fraction of the vocabulary V that is on the green list. A
large γ means that for each token relatively many green-list
options are available. This makes the expected deterioration
of text quality relatively small. However, when γ is large
5

Preprint.
Optimizing watermarks for large language models
a powerful test also requires relatively many green tokens
in a sequence of length T. For small γ the trade-off is vice
versa. This raises the question of an optimal value of γ.
2.75
3.00
3.25
˜E[log PPL]
0.0
0.2
0.4
0.6
0.8
1.0
πn∗
α∗= 0.01
E[log PPL]
γ = 0.1
γ = 0.25
γ = 0.5
γ = 0.75
2.75
3.00
3.25
˜E[log PPL]
α∗= 0.001
2.75
3.00
3.25
˜E[log PPL]
α∗= 0.0001
Figure 4. Pareto optimal bounds for different values of the hyperpa-
rameter γ, for tests with different false-positive rates α∗. It shows
that there is no universally “best” γ.
In Figure 4 optimal bounds are plotted for different values
of the hyperparameter γ. It shows that there is no universally
best γ, i.e., a hyperparameter value that gives the best test-
text trade-off for every possible false-positive rate α∗. One
possible definition of a “best” γ is the one that, given a
false-positive rate α∗, maximizes the power of the OPT
watermarks that keep the expected log-likelihood unaffected,
i.e., ˜E[log PPL] = E[log PPL] . Figure 5 shows that this
“best” γ usually lies between 0.1 and 0.2, where it should be
noted that the range of “near-best” hyperparameter values
increases with increasing α∗. Interestingly, Kirchenbauer et
al. (2023a) found a “best” γ around 0.1 for their non-optimal
KGW watermark. It should be emphasized that the optimal
value of γ is not only dependent on how the test-text trade-
off is defined, but is also a property of the LLM. A different
LLM could give a different optimal value for γ.
4.2. Biasedness and dependence within watermarks
As mentioned in Section 3.1, the derivation of the Pareto
optimal solutions to optimization problems involving the
power of the test, e.g. Equation (7), uses that Ng is binomi-
ally distributed and this requires two additional assumptions.
The first one is unbiasedness of the (conditional) probability
of a token being a green-list token in the absence of a water-
mark, E[Γt] = γ. For 10,000 prompts from the C4 dataset
30 tokens are generated without watermark. For each token
Γt is computed for a green-red split with γ = 0.25. The
sample average is Γt = 0.2574. Under the null hypothesis
of unbiasedness, and under the assumption of independence
of the observations in the sample, this corresponds to a
z-score of 16.1. This strongly suggests that E[Γt] ̸= γ.
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
γ
0.7
0.8
0.9
1.0
πn∗
α∗= 0.01
α∗= 0.001
α∗= 0.0001
Figure 5. The power of OPT watermarks without a change in ex-
pected log-perplexity ( ˜E[log PPL] = E[log PPL] ), as a function
of the hyperparameter γ, for tests with different false-positive rates
α∗. The “best” value for γ usually lies between 0.1 and 0.2.
This bias has its origin in the pseudo-random green-red split
of the vocabulary. For each pair of subsequent tokens in
a sentence, the key of the function gγ determines whether
the second token of the pair is green or red. This is the
same for each occurrence of the pair. Hence, the occurrence
frequencies of all possible pairs of tokens, together with
the key, determine the bias of E[Γt] with respect to γ. We
verified that a different generator gγ, based on a different
key, gives a different bias. We emphasize that it is unlikely
that this bias has an effect on the Pareto optimality of the
solutions presented in this paper, because also with a bias
the power of the test is a monotonic function of ∆Ng.
0
2
4
6
8
10
δ
0.00
0.02
0.04
0.06
KL divergence
−2
0
2
4
6
8
10
β
KGW
OPT
Figure 6. Shown is the Kullback-Leibler divergence between a
binomial distribution for Ng, based on the empirical fraction of
green-list tokens, and the empirical distribution of Ng, as a func-
tion of the strength (δ and β) of the respective watermarks.
The second assumption is that the event that a token is a
green-list token is (stochastically) independent for differ-
ent tokens in the same sequence. It turns out that this is
not the case, and that the dependence becomes stronger for
stronger watermarks. Appendix C shows empirical evidence
that the distribution of Ng for watermarked text is generally
not binomial. The heavier tails indicate a positive correla-
6

Preprint.
Optimizing watermarks for large language models
tion between the events that tokens from the same list are
green-list tokens. This is understandable, as different sen-
tences can have different amounts of freedom (i.e. entropy)
to insert green-list tokens. Figure 6 shows the Kullback-
Leibler divergence between a binomial distribution for Ng,
based on the empirical fraction of green-list tokens, and the
empirical distribution of Ng. The KL-divergence increases
with the strength of the watermarks, indicating a stronger
(stochastic) dependence. For very strong watermarks the
KL-divergence decreases again, as there is less space to
deviate when almost all tokens are green-list tokens.
We believe that this analysis can explain the deviation be-
tween strong OPT watermarks (β > 2.0) and the Pareto
optimal bound, as observed in Figure 2. To calculate the
Pareto optimal bound it was used that Ng is binomially dis-
tributed, which has here been invalidated. Furthermore, in
this regime exact Pareto optimality of the OPT watermark
for the optimization problem of Equation (7) is not guar-
anteed, as it used that the power of the test is a monotonic
function of the expected shift function. It should be noted
though that, if possible at all, there is not much room for
improvement of the test-text trade-off, as the power of the
test in this regime is already quite large.
5. Related work
Watermarking LLM-generated texts is an example of
steganography, the practice of representing information (e.g.,
a watermark) in other information (e.g., a text). The idea
of watermarking machine-learning generated text has been
around for some time (Venugopal et al., 2011; Ziegler et al.,
2019) and has different realizations.
A first option is to impose a distribution shift on the LLM
that is tractable for a detector, with Kirchenbauer et al.
(2023a) as the most prominent example. Since then, mostly
in view of mitigating between the four criteria for water-
marks outlined in Section 1, a myriad of alternatives has
been proposed (Zhao et al., 2023; Wang et al., 2023; Fang
et al., 2023; Lee et al., 2023; Liu et al., 2023a; Fu et al.,
2023; Liu et al., 2023b; Chen et al., 2023). A notable sub-
class of examples are weakly distortion-free watermarks,
meaning that the distribution shift of the watermark is unbi-
ased when averaged over the pseudo-random aspect of the
watermark (Hu et al., 2023; Wu et al., 2023).
A second option is to instill a watermark into the pseudo-
random sampling from the LLM (Christ et al., 2023; Ku-
ditipudi et al., 2023). This has the advantage that it can
be made perfectly stealthy, meaning that the conditional
distribution of the LLM is not distorted. However, exam-
ples come also with disadvantages of low robustness or
inefficiency (Wu et al., 2023).
A third option is a watermark based on an additional ML-
model (Abdelnabi & Fritz, 2021; Qiang et al., 2023; Yoo
et al., 2023; Yang et al., 2023; Munyer & Zhong, 2023). The
watermark is then imposed by making alterations to a text
that is already generated by the LLM. This requires extra
training and due to the extra flexibility ensuring the quality
of the watermark can be difficult. Finally, one could instill
a watermark into the weights of the LLM by adjusting the
training procedure (Li et al., 2023a; Gu et al., 2023).
Another branch of this developing field is the analysis of wa-
termarks for LLMs. This includes the design of benchmark
tasks and metrics to test the quality of watermarks (Piet
et al., 2023), some with a special focus on text quality (Tang
et al., 2023; Tu et al., 2023; Ajith et al., 2023) or robustness
(Krishna et al., 2023; Sadasivan et al., 2023; Shi et al., 2023;
Kirchenbauer et al., 2023b; Zhang et al., 2023).
To conclude, watermarking is not the only option to dis-
tinguish LLM-generated texts from human-generated texts.
An alternative is to train a binary classifier to detect LLM-
generated texts (see, e.g., Mitchell et al. (2023)). With the
rapid improvement of LLMs, this has become increasingly
difficult (Gambini et al., 2022). Another option is to let the
vendor of the LLM keep a copy of all generated output and
provide an API that compares a text with this database of
outputs (Krishna et al., 2023).
6. Conclusion
It was posited in Section 1 that the contribution of this paper
is twofold. It introduces new watermarks, which are the
Pareto optimal solutions of the multi-objective optimization
problem into which the trade-off between identifiability of
a watermark and its impact on the quality of a generated
text was translated. Since the watermarks over which we
optimize are generally considered robust and efficient, we
believe that the optimal watermarks have an excellent stand-
ing with respect to the four criteria for good watermarks
for LLMs: identifiability, stealthiness, robustness and effi-
ciency.
But this paper should also be read as the introduction of a
systematic method to optimizing the test-text trade-off for
watermarks of LLMs. The chosen translation of the trade-
off into an optimization problem is not unique, as it depends
on how you quantify test and text quality. And also the class
of watermarks over which we optimize, defined in Equation
(2), is not unique. It was chosen to be based on a green-
red split of the vocabulary, such that it includes the default
watermark of Kirchenbauer et al. (2023a). And the form of
the watermark was chosen to be so-called minimal, i.e., all
green-list probabilities are rescaled by the same factor and
the same holds for all red-list tokens.
It is conceivable that different choices regarding the above
lead, after optimization, to more preferable watermarks.
7

Preprint.
Optimizing watermarks for large language models
One option is to remove some of the implicit restrictions that
are imposed by Equation (2). The shift function ∆(pt, Gt)
that determines ˜P[Vt|V:t] is the same for each Vt ∈V, but
this does not have to be the case. Also note that the shift
function is determined by properties of token t alone. One
could try to make it dependent on subsequent tokens; a
choice for a red token at position t could enable a string of
green tokens in what follows. Another possibility is to keep
track of the number of green-list tokens already generated
and use this to adjust the shift function.
Acknowledgements
The author would like to thank Cees Diks and Floris Hol-
stege for useful discussions and feedback on the manuscript.
References
Aaronson, S. My AI Safety Lecture for UT Effective Altru-
ism, 2022. URL https://scottaaronson.blog/?p=6823.
Abdelnabi, S. and Fritz, M. Adversarial Watermarking
Transformer: Towards Tracing Text Provenance with
Data Hiding. 2021 IEEE Symposium on Security and
Privacy (SP), 00:121–140, 2021. doi: 10.1109/sp40001.
2021.00083.
Ajith, A., Singh, S., and Pruthi, D. Performance Trade-offs
of Watermarking Large Language Models. arXiv, 2023.
doi: 10.48550/arxiv.2311.09816. URL https://arxiv.org/
abs/2311.09816.
Bartz, D. and Hu, K. OpenAI, Google, others pledge to
watermark AI content for safety, White House says,
2023. URL https://www.reuters.com/technology/openai-
google-others-pledge-watermark-ai-content-safety-
white-house-2023-07-21/.
Chen, L., Bian, Y., Deng, Y., Li, S., Wu, B., Zhao, P., and
Wong, K.-f. X-Mark: Towards Lossless Watermarking
Through Lexical Redundancy. arXiv, 2023. doi: 10.
48550/arxiv.2311.09832. URL https://arxiv.org/abs/2311.
09832.
Christ, M., Gunn, S., and Zamir, O. Undetectable Water-
marks for Language Models. arXiv, 2023. doi: 10.48550/
arxiv.2306.09194. URL https://arxiv.org/abs/2306.09194.
Fang, J., Tan, Z., and Shi, X. COSYWA: Enhancing Seman-
tic Integrity in Watermarking Natural Language Genera-
tion. Lecture Notes in Computer Science, pp. 708–720,
2023. ISSN 0302-9743. doi: 10.1007/978-3-031-44693-
1\ 55.
Fu, Y., Xiong, D., and Dong, Y. Watermarking Conditional
Text Generation for AI Detection: Unveiling Challenges
and a Semantic-Aware Watermark Remedy. arXiv, 2023.
doi: 10.48550/arxiv.2307.13808. URL https://arxiv.org/
abs/2307.13808.
Gambini, M., Fagni, T., Falchi, F., and Tesconi, M. On
pushing DeepFake Tweet Detection capabilities to the
limits. 14th ACM Web Science Conference 2022, pp.
154–163, 2022. doi: 10.1145/3501247.3531560.
Goldstein, J. A., Sastry, G., Musser, M., DiResta, R.,
Gentzel, M., and Sedova, K.
Generative Language
Models and Automated Influence Operations: Emerg-
ing Threats and Potential Mitigations. arXiv, 2023. doi:
10.48550/arxiv.2301.04246. URL https://arxiv.org/abs/
2301.04246.
Gu, C., Li, X. L., Liang, P., and Hashimoto, T. On the
Learnability of Watermarks for Language Models. arXiv,
2023. doi: 10.48550/arxiv.2312.04469. URL https://arxiv.
org/abs/2312.04469.
Hu, Z., Chen, L., Wu, X., Wu, Y., Zhang, H., and Huang,
H. Unbiased Watermark for Large Language Models.
arXiv, 2023. doi: 10.48550/arxiv.2310.10669. URL
https://arxiv.org/abs/2310.10669.
Jiang, Z., Zhang, J., and Gong, N. Z. Evading Watermark
based Detection of AI-Generated Content. arXiv, 2023.
doi: 10.48550/arxiv.2305.03807. URL https://arxiv.org/
abs/2305.03807.
Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I.,
and Goldstein, T. A Watermark for Large Language
Models. arXiv, 2023a. doi: 10.48550/arxiv.2301.10226.
URL https://arxiv.org/abs/2301.10226.
Kirchenbauer, J., Geiping, J., Wen, Y., Shu, M., Saifullah,
K., Kong, K., Fernando, K., Saha, A., Goldblum, M., and
Goldstein, T. On the Reliability of Watermarks for Large
Language Models. arXiv, 2023b. doi: 10.48550/arxiv.
2306.04634. URL https://arxiv.org/abs/2306.04634.
Krishna, K., Song, Y., Karpinska, M., Wieting, J., and Iyyer,
M. Paraphrasing evades detectors of AI-generated text,
but retrieval is an effective defense. arXiv, 2023. doi:
10.48550/arxiv.2303.13408. URL https://arxiv.org/abs/
2303.13408.
Kuditipudi, R., Thickstun, J., Hashimoto, T., and Liang, P.
Robust Distortion-free Watermarks for Language Models.
arXiv, 2023. doi: 10.48550/arxiv.2307.15593. URL
https://arxiv.org/abs/2307.15593.
Lee, T., Hong, S., Ahn, J., Hong, I., Lee, H., Yun, S., Shin,
J., and Kim, G. Who Wrote this Code? Watermarking for
Code Generation. arXiv, 2023. doi: 10.48550/arxiv.2305.
15060. URL https://arxiv.org/abs/2305.15060.
8

Preprint.
Optimizing watermarks for large language models
Li, L., Jiang, B., Wang, P., Ren, K., Yan, H., and Qiu, X.
Watermarking LLMs with Weight Quantization. arXiv,
2023a. doi: 10.48550/arxiv.2310.11237. URL https:
//arxiv.org/abs/2310.11237.
Li, Y., Wang, Y., Shi, Z., and Hsieh, C.-J.
Improving
the Generation Quality of Watermarked Large Language
Models via Word Importance Scoring. arXiv, 2023b. doi:
10.48550/arxiv.2311.09668. URL https://arxiv.org/abs/
2311.09668.
Liu, A., Pan, L., Hu, X., Li, S., Wen, L., King, I., and Yu,
P. S. A Private Watermark for Large Language Models.
arXiv, 2023a. doi: 10.48550/arxiv.2307.16230. URL
https://arxiv.org/abs/2307.16230.
Liu, A., Pan, L., Hu, X., Meng, S., and Wen, L. A Semantic
Invariant Robust Watermark for Large Language Models.
arXiv, 2023b. doi: 10.48550/arxiv.2310.06356. URL
https://arxiv.org/abs/2310.06356.
Meyer, J. G., Urbanowicz, R. J., Martin, P. C. N., O’Connor,
K., Li, R., Peng, P.-C., Bright, T. J., Tatonetti, N., Won,
K. J., Gonzalez-Hernandez, G., and Moore, J. H. Chat-
GPT and large language models in academia: opportu-
nities and challenges. BioData Mining, 16(1):20, 2023.
ISSN 1756-0381. doi: 10.1186/s13040-023-00339-9.
Milano, S., McGrane, J. A., and Leonelli, S. Large language
models challenge the future of higher education. Nature
Machine Intelligence, 5(4):333–334, 2023. doi: 10.1038/
s42256-023-00644-2.
Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., and
Finn, C. DetectGPT: Zero-Shot Machine-Generated Text
Detection using Probability Curvature. arXiv, 2023. doi:
10.48550/arxiv.2301.11305. URL https://arxiv.org/abs/
2301.11305.
Munyer, T. and Zhong, X. DeepTextMark: Deep Learning
based Text Watermarking for Detection of Large Lan-
guage Model Generated Text. arXiv, 2023. doi: 10.48550/
arxiv.2305.05773. URL https://arxiv.org/abs/2305.05773.
Piet, J., Sitawarin, C., Fang, V., Mu, N., and Wagner, D.
Mark My Words: Analyzing and Evaluating Language
Model Watermarks. arXiv, 2023. doi: 10.48550/arxiv.
2312.00273. URL https://arxiv.org/abs/2312.00273.
Qiang, J., Zhu, S., Li, Y., Zhu, Y., Yuan, Y., and Wu, X. Nat-
ural language watermarking via paraphraser-based lexical
substitution. Artificial Intelligence, 317:103859, 2023.
ISSN 0004-3702. doi: 10.1016/j.artint.2023.103859.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 21,
2020. ISSN 1532-4435. doi: 10.5555/3455716.3455856.
Rillig, M. C., ˚Agerstrand, M., Bi, M., Gould, K. A., and
Sauerland, U. Risks and Benefits of Large Language
Models for the Environment. Environmental Science &
Technology, 57(9):3464–3466, 2023. ISSN 0013-936X.
doi: 10.1021/acs.est.3c01106.
Sadasivan, V. S., Kumar, A., Balasubramanian, S., Wang,
W., and Feizi, S. Can AI-Generated Text be Reliably
Detected? arXiv, 2023. doi: 10.48550/arxiv.2303.11156.
URL https://arxiv.org/abs/2303.11156.
Shi, Z., Wang, Y., Yin, F., Chen, X., Chang, K.-W., and
Hsieh, C.-J. Red Teaming Language Model Detectors
with Language Models. arXiv, 2023. doi: 10.48550/arxiv.
2305.19713. URL https://arxiv.org/abs/2305.19713.
Tang, L., Uberti, G., and Shlomi, T. Baselines for Iden-
tifying Watermarked Large Language Models. arXiv,
2023.
doi: 10.48550/arxiv.2305.18456.
URL https:
//arxiv.org/abs/2305.18456.
Tu, S., Sun, Y., Bai, Y., Yu, J., Hou, L., and Li, J. Water-
Bench: Towards Holistic Evaluation of Watermarks for
Large Language Models. arXiv, 2023. doi: 10.48550/
arxiv.2311.07138. URL https://arxiv.org/abs/2311.07138.
Venugopal, A., Uszkoreit, J., Talbot, D., Och, F., and Gan-
itkevitch, J. Watermarking the Outputs of Structured Pre-
diction with an application in Statistical Machine Trans-
lation. Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pp. 1363—
1372, 2011. URL https://aclanthology.org/D11-1126.
Vincent, J.
AI-generated answers temporarily banned
on coding Q&A site Stack Overflow, 2022.
URL
https://www.theverge.com/2022/12/5/23493932/
chatgpt-ai-generated-answers-temporarily-banned-
stack-overflow-llms-dangers.
Wang, L., Yang, W., Chen, D., Zhou, H., Lin, Y., Meng,
F., Zhou, J., and Sun, X. Towards Codable Text Water-
marking for Large Language Models. arXiv, 2023. doi:
10.48550/arxiv.2307.15992. URL https://arxiv.org/abs/
2307.15992.
Wu, Y., Hu, Z., Zhang, H., and Huang, H. DiPmark: A
Stealthy, Efficient and Resilient Watermark for Large
Language Models. arXiv, 2023. doi: 10.48550/arxiv.
2310.07710. URL https://arxiv.org/abs/2310.07710.
Yang, X., Chen, K., Zhang, W., Liu, C., Qi, Y., Zhang,
J., Fang, H., and Yu, N. Watermarking Text Generated
by Black-Box Language Models. arXiv, 2023. doi: 10.
48550/arxiv.2305.08883. URL https://arxiv.org/abs/2305.
08883.
9

Preprint.
Optimizing watermarks for large language models
Yoo, K., Ahn, W., Jang, J., and Kwak, N. Robust Multi-
bit Natural Language Watermarking through Invariant
Features. arXiv, 2023. doi: 10.48550/arxiv.2305.01904.
URL https://arxiv.org/abs/2305.01904.
Zhang, H., Edelman, B. L., Francati, D., Venturi, D., Ate-
niese, G., and Barak, B. Watermarks in the Sand: Impos-
sibility of Strong Watermarking for Generative Models.
arXiv, 2023.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-
haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,
Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L.
OPT: Open Pre-trained Transformer Language Models.
arXiv, 2022. doi: 10.48550/arxiv.2205.01068. URL
https://arxiv.org/abs/2205.01068.
Zhao, X., Ananth, P., Li, L., and Wang, Y.-X. Provable
Robust Watermarking for AI-Generated Text. arXiv, 2023.
doi: 10.48550/arxiv.2306.17439. URL https://arxiv.org/
abs/2306.17439.
Ziegler, Z., Deng, Y., and Rush, A.
Neural Linguistic
Steganography.
Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pp. 1210–1215,
2019. doi: 10.18653/v1/d19-1115.
10

Preprint.
Optimizing watermarks for large language models
A. Optimization objectives and their solutions
A.1. Measures for test quality
One measure for test quality is Ng, the number of green-list tokens in a watermarked sequence of length T. A strong test,
i.e. a test with a large identifiability, corresponds to a large number of green-list tokens. If we define binary random variables
Yt such that
Yt =
1
if Vt ∈Gt,
0
if Vt /∈Gt,
(9)
then Ng = Y1 + Y2 + . . . + YT . The conditional random variables Yt|V:t are Bernoulli distributed,
Yt|V:t ∼BIN(1, Γt)
(10)
in the absence of a watermark and
Yt|V:t ∼BIN(1, Γt + ∆(pt, Gt))
(11)
in the presence of a watermark. The shift, due to a watermark defined by Equation (2), in the expected number of green-list
tokens is given by Equation (3) and can be derived as follows:
˜E[Ng] =
T
X
t=1
˜E
˜E[Yt|V:t]

(12a)
=
T
X
t=1
˜E[Γt + ∆(pt, Gt)]
(12b)
=
T
X
t=1
n
˜E[Γt] + ˜E[∆(pt, Gt)]
o
(12c)
=
T
X
t=1
n
E[Γt] + ˜E[∆(pt, Gt)]
o
(12d)
= E[Ng] +
T
X
t=1
˜E[∆(pt, Gt)]
(12e)
where in (12d) we used the assumption that expectations are unaffected by watermark-induced changes in the distribution of
the precursor V:t. It should be emphasized that this is an approximative assumption. The idea is that watermark-induced
changes in the distribution of the precursor V:t are (approximately) averaged out.
Another measure for test quality is the power of the test, πn∗= ˜P[Ng ≥n∗] . If we assume that ˜E[Γt] = E[Γt] = γ, that the
Yt are identically distributed and that they are (stochastically) independent, then
Ng ∼BIN(T, γ + ˜E[∆(pt, Gt)]),
(13)
because ˜E[Yt] = ˜E
˜E[Yt|V:t]

= ˜E[Γt + ∆(pt, Gt)] . This implies
πn∗=
T
X
n=n∗
T
n
  γ + ˜E[∆(pt, Gt)]
n  1 −γ −˜E[∆(pt, Gt)]
T −n .
(14)
In other words, the watermark determines the power of the test only through ˜E[∆(pt, Gt)] and does this in a monotonically
increasing way. Hence, maximizing the power of the test over a class of watermarks is equivalent to maximizing Ng.
A.2. Measures for text quality
One measure for text quality is the log-perplexity, defined in Section 3. A large log-perplexity is interpreted as a low text
quality. The shift, due to a watermark defined by Equation (2), in the expected log-perplexity is given by Equation (4) and
11

Preprint.
Optimizing watermarks for large language models
can be derived as follows:
˜E[log PPL] = −1
T
T
X
t=1
˜E[log P[Vt|V:t]]
(15a)
= −1
T
T
X
t=1
˜E
"X
v∈V
˜P[Vt = v|V:t] log P[Vt = v|V:t]
#
(15b)
= −1
T
T
X
t=1
˜E[E[log P[Vt|V:t] | V:t]]
(15c)
−1
T
T
X
t=1
˜E
"X
v∈V
∆(pt, Gt)
IGt(v)
Γt
−1 −IGt(v)
1 −Γt

P[Vt = v|V:t] log P[Vt = v|V:t]
#
(15d)
= −1
T
T
X
t=1
˜E[E[log P[Vt|V:t] | V:t]] + 1
T
T
X
t=1
˜E[∆(pt, Gt)B(pt, Gt)]
(15e)
= −1
T
T
X
t=1
E[log P[Vt|V:t]] + 1
T
T
X
t=1
˜E[∆(pt, Gt)B(pt, Gt)]
(15f)
= E[log PPL] + 1
T
T
X
t=1
˜E[∆(pt, Gt)B(pt, Gt)] .
(15g)
In (15e) we used the definition of B(pt, Gt), see Section 3. In (15f) we used the assumption that expectations are unaffected
by watermark-induced changes in the distribution of the precursor V:t. It should be emphasized that this is an approximative
assumption. The idea is that watermark-induced changes in the distribution of the precursor V:t are (approximately) averaged
out.
A.3. Derivation of Pareto optimal solutions
The multi-objective optimization problem of Equation (5) can be written as
max
∆∈Υ
˜E[∆(pt, Gt)]
and
min
∆∈Υ
˜E[∆(pt, Gt)B(pt, Gt)] ,
(16)
where Υ is the set of shift functions ∆: Ξ×Θ →[0, 1], where Ξ is the space of all pmfs over the vocabulary V, and Θ is the
space of all subsets of size ⌊γN⌋of the vocabulary V, with the additional (consistency) requirement that ∆(pt, Gt) ≤1−Γt.
The expectations ˜E[·] are taken over all possible sequences V1, . . . , VT generated by a watermarked LLM, and all possible
prompts V:1. However, the optimization problem only depends on the tokens through two quantities: Γt and B(pt, Gt). It
can therefore be rephrased as an optimization problem for a function h(x, y) of a bivariate random variable (X, Y ) that has
support [0, 1] × R :
max
h(·,·) E(X,Y )[h(X, Y )]
and
min
h(·,·) E(X,Y )[h(X, Y )Y ] ,
(17)
where the expectations are with respect to the joint distribution of (X, Y ) and with the constraints 0 ≤h(x, y) ≤1 −x. We
claim that
h∗(x, y) =
1 −x
if y ≤β,
0
if y > β,
(18)
which corresponds to Equation (6), is Pareto optimal if β ≥0. We prove this by showing that a function h′(x, y) that obeys
the same constraints and that improves the first objective in Equation (17), necessarily does worse for the second objective
in Equation (17).
Hence, assume that E(X,Y )[h′(X, Y )] > E(X,Y )[h∗(X, Y )] . Suppose h′(x, y) ̸= h∗(x, y) if y ∈A1 ∪A2, where A1 ⊂
(−∞, β] and A2 ∈(β, ∞). This means that h′(x, y) < 1 −x if y ∈A1 ⊂(−∞, β] and h′(x, y) > 0 if y ∈A2 ⊂(β, ∞).
The initial assumption then translates into
Z
A1
Z 1
0
[(1 −x) −h′(x, y)] f(x, y) dx dy <
Z
A2
Z 1
0
h′(x, y)f(x, y) dx dy,
(19)
12

Preprint.
Optimizing watermarks for large language models
where f(x, y) is the joint pdf of (X, Y ). Note that A1 can be empty. Let’s now focus on the second objective, which is to
minimize (assuming β > 0)
E[h′(x, y)y] =
Z ∞
−∞
Z 1
0
h′(x, y)yf(x, y) dx dy
(20a)
=
Z ∞
−∞
Z 1
0
h∗(x, y)yf(x, y) dx dy
(20b)
+
Z
A2
Z 1
0
h′(x, y)yf(x, y) dx dy −
Z
A1
Z 1
0
[(1 −x) −h′(x, y)] yf(x, y) dx dy
(20c)
>
Z ∞
−∞
Z 1
0
h∗(x, y)yf(x, y) dx dy
(20d)
+ β
Z
A2
Z 1
0
h′(x, y)f(x, y) dx dy −
Z
A1
Z 1
0
[(1 −x) −h′(x, y)] f(x, y) dx dy

(20e)
>
Z ∞
−∞
Z 1
0
h∗(x, y)yf(x, y) dx dy
(20f)
= E[h∗(x, y)y]
(20g)
where in (20f) we used that E(X,Y )[h′(X, Y )]
>
E(X,Y )[h∗(X, Y )] and (20d) is a strict inequality because
E(X,Y )[h′(X, Y )] > E(X,Y )[h∗(X, Y )] is a strict inequality. Note that the second inequality is an equality in the spe-
cial case β = 0. This derivation shows that the second objective is worse off. One can make a similar argument for
improving on the second objective in Equation (17) and showing that the first objective will then necessarily become worse.
As already explained in Appendix (A.1), the multi-objective optimization problem defined by Equation (7) has the same
Pareto optimal solutions.
The watermarks OPT’, defined in Equation (8), can be shown to be Pareto optimal in a similar fashion. The only difference
is that −log P[Vt = v|V:t] must be replaced by [−log P[Vt = v|V:t]]2.
B. Experimental details
Prompts are created by randomly selecting (news) texts from the C4 dataset (Raffel et al., 2020). Only texts of at least 250
tokens are taken into account. If a text has at most 400 tokens, the final 200 tokens are removed and the remainder forms the
prompt. If a text has more than 400 tokens, the first 200 tokens are used as prompt and the rest is discarded. All prompts
have a length between 50 and 200 tokens.
Sampling from the LLM takes place with a temperature of 1.0.
B.1. Estimation of the log-perplexity
Suppose v1, v2, . . . , vT is an actual realization of the random variables V1, V2, . . . , VT . The log-perplexity of this sequence
is
−1
T
T
X
t=1
log P[Vt = vt|V:t = v:t].
(21)
In order to estimate ˜E[log PPL] , the above formula could be used. However, in order to reduce estimation noise, we used
−1
T
T
X
t=1
X
v∈V
˜P[Vt = v|V:t = v:t] log P[Vt = v|V:t = v:t].
(22)
C. Additional results
This section shows some additional results, refered to in the main text.
13

Preprint.
Optimizing watermarks for large language models
2.5
3.0
3.5
4.0
4.5
5.0
˜E[log PPL]
10
15
20
25
30
˜E[Ng]
0.1
0.5
1.0
1.5
2.0
2.5
3.0
4.0
6.0
10.0
-2.0
-1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
4.0
6.0
8.0
10.0
E[log PPL]
˜EHARD[log PPL]
optimal bound
LLM
HARD
KGW (δ)
OPT (β)
OPT′ (β′)
Figure 7. same as Figure 1, but now including the OPT′ watermark. Note that OPT and OPT′ hardly differ in terms of test-text trade-off,
when text quality is defined in terms of expected log-perplexity. Error bars (vertical and horizontal) are never larger than the marker sizes.
2.5
3.0
3.5
4.0
4.5
5.0
5.5
˜E

(−log P [Vt|V:t] −log PPL)2
10
15
20
25
30
˜E[Ng]
LLM
HARD
KGW (δ)
OPT (β)
OPT′ (β′)
Figure 8. Test quality, measured as the expected number of green-list tokens, versus between-token variance of the log-perplexity, is
shown for different watermarks. For completeness, the original language model without watermark is included (LLM). Error bars (vertical
and horizontal) are never larger than the marker sizes. Note that, as expected, OPT′ outperforms OPT, albeit marginally. Also note that
the KGW watermark outperforms the optimized watermarks. This is not in contradiction with our method, as we did not optimize for this
trade-off.
15
20
25
30
˜E[log PPL2] + ˜E

(−log P [Vt|V:t] −log PPL)2
10
15
20
25
30
˜E[Ng]
LLM
HARD
KGW (δ)
OPT (β)
OPT′ (β′)
Figure 9. Test quality, measured as the expected number of green-list tokens, versus expected log-perplexity squared plus between-token
variance of the log-perplexity, is shown for different watermarks. For completeness, the original language model without watermark is
included (LLM). Error bars (vertical and horizontal) are never larger than the marker sizes. Note that, as expected, OPT′ outperforms
both OPT and KGW, as this trade-off is the optimization objective for which OPT′ is optimized.
14

Preprint.
Optimizing watermarks for large language models
0
5
10
15
20
25
Ng
0.00
0.05
0.10
0.15
No watermark
LLM-generated
binomial (exact)
simulation
5
10
15
20
25
30
Ng
OPT with β = 4.0
5
10
15
20
25
30
Ng
oracle OPT with β = 4.0
Figure 10. A comparison between the empirical distribution of Ng, the number of green-list tokens in a sequence of T = 30 tokens, and
the exact binomial distribution, for text generated without watermark (left panel) and with the OPT watermark with β = 4.0 (middle
panel). The oracle OPT (right panel) is the OPT watermark, but with a different, randomly selected key for green-red split generation at
every generation step. Because oracle OPT and OPT differ from the exact binomial distribution similarly, we conclude the discrepancy is
not because of the pseudo-random green-red split. Also included is a simulation sampled from the exact binomial distribution, of the same
sample size as the LLM generated data.
15
