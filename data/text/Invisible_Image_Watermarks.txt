Invisible Image Watermarks Are Provably Removable Using Generative AI
Xuandong Zhao†∗, Kexun Zhang†∗, Zihao Su†, Saastha Vasan†, Ilya Grishchenko†,
Christopher Kruegel†, Giovanni Vigna†, Yu-Xiang Wang†, Lei Li‡
†University of California, Santa Barbara, ‡Carnegie Mellon University
∗Equal contribution
Abstract—Invisible watermarks safeguard images’ copyright
by embedding hidden messages only detectable by owners.
They also prevent people from misusing images, especially
those generated by AI models. We propose a family of
regeneration attacks to remove these invisible watermarks. The
proposed attack method first adds random noise to an image
to destroy the watermark and then reconstructs the image.
This approach is flexible and can be instantiated with many
existing image-denoising algorithms and pre-trained generative
models such as diffusion models. Through formal proofs
and empirical results, we show that all invisible watermarks
are vulnerable to the proposed attack. For a particularly
resilient watermark, RivaGAN, regeneration attacks remove
93-99% of the invisible watermarks while the baseline attacks
remove no more than 3%. However, if we do not require
the watermarked image to look the same as the original one,
watermarks that keep the image semantically similar can be an
alternative defense against our attack. Our finding underscores
the need for a shift in research/industry emphasis from invisible
watermarks to semantically similar ones. Code is available at
https://github.com/XuandongZhao/WatermarkAttacker.
1. Introduction
Invisible watermarks on digital images have long been
used in protecting the content of the image from misuse
[1, 2, 3]. They do so by embedding hidden messages in
images that can later be identified by a detection algorithm
and used as evidence for various purposes. Their broad appli-
cations include transaction tracking [4], proof of ownership
[5], copy control [6], authentication [7] and many others [8].
The era of generative AI has seen a new and more
challenging application of invisible watermarks - telling
images that are generated by an AI model from human-
created ones. This application is particularly important and
difficult because visual differences between AI-generated
images and real images are becoming hard to identify as
AI models get stronger, making watermarks one of the
few methods that still work for AI-generation identification.
Generative models such as DALL-E [9], Imagen [10] and
Stable Diffusion [11] are able to produce both photo-realistic
and artistic images that are often visually indistinguishable
from the works of human photographers and artists. Such
visual indistinguishability may lead to false beliefs in the
Figure 1: AI-generated fake images from Twitter depicting
the arrest of Donald Trump.
content of the image and create misunderstandings (see
examples in Figure 1).
To address these challenges, major AI companies such
as Google [12], Microsoft [13], Meta [14], and OpenAI [15]
have pledged to add watermarks to the content generated by
their AI products, as government leaders [16] and legislators
[17] start working on promoting and regulating responsible
and ethical use of AI. Invisible watermarks are preferred
because it preserves the image quality and it is less likely
for a layperson to tamper with it. On the other hand, abusers
are not laypersons. They are well aware of the watermark
being added and will make a deliberate attempt to remove
these watermarks. Therefore, it is crucial that the injected
invisible watermark is robust to evasion attacks.
It turns out that this is an old problem and there is a
plethora of existing work on both the design of invisible
watermarks and the attacks on them. Classical invisible water-
marking schemes include manipulating the least significant
bit of each pixel [3] or injecting small signals in the frequency
domain [18, 19].
Existing attacks on these watermarks can be categorized
into two types. The first type of attack is destructive, where
the watermark is considered part of the image and removed
by corrupting the image. Typical destructive attacks include
modifying the brightness or contrast of an image, JPEG
compression, image rotation and adding Gaussian noise.
These approaches are effective at removing watermarks but
they result in significant quality loss. The second type of
attack is constructive, where the watermark is treated as some
noise added to the original image and removed by purifying
the image. Constructive attacks include traditional image-
denoising techniques like Gaussian blur [20], BM3D [21],
and learning-based approaches like DnCNNs [22]. However,
arXiv:2306.01953v2  [cs.CR]  6 Aug 2023

Detected
Add
Watermark
No watermark
Attacker removes
the watermark
Watermark
Detector
Original Image
Watermarked Image
Attacked Image
Map
Noise
Embedding
Undetected
Noisy Embedding
Regenerate
Figure 2: Removing invisible watermarks: The proposed first maps the watermarked image to its embedding, which is
another representation of the image. Then the embedding is noised to destruct the watermark. After that, a regeneration
algorithm reconstructs the image from the noisy embedding.
constructive attacks cannot remove resilient watermarks
easily because they are not noise-like. To counter these
attacks, learning-based watermarking methods [23, 24, 25]
were proposed which explicitly train against these known
attacks to be robust to them. But how about other attacks?
What is the end of this cat-and-mouse game?
In this paper, we ask a more fundamental question:
Is an invisible watermark necessarily non-robust?
To be more precise, is there a fundamental tradeoff between
the invisibility of a watermark and its resilience to any attack
that preserves the image quality to a certain-level.
To address this question, we propose a regeneration
attack that leverages the strengths of both destructive and
constructive approaches. The pipeline of the attack is given
in Figure 2. Our attack first corrupts the image by adding
Gaussian noise to its latent representation. Then, we recon-
struct the image from the noisy embedding using a generative
model. The proposed regeneration attack is flexible in that
it can be instantiated with various regeneration algorithms,
including traditional denoisers and deep generative models
such as diffusion [26]. Ironically, the recent advances in
generative models that created the desperate need for invisible
watermarks are also making watermark removal easier when
integrated into the proposed attack.
Surprisingly, we prove that the proposed attack guarantees
the removal of any invisible watermark such that no detection
algorithm could work. That is to say, for any watermarks
that perturb the image within a limited range of ℓ2-distance
(which is the Euclidian distance in the pixel-space), whether
they have been proposed or have not yet been invented, our
attack is provably effective in removing them. We also show
that the image quality of the generated image is as high as
if the watermark is not added in the first place.
To validate our theory empirically, we conduct exten-
sive experiments on five widely used invisible watermarks
[23, 25, 27, 28, 29] including the ones currently used by
the popular open-source image generative model Stable
Diffusion. We compare our attack’s performance with four
baselines in terms of image quality and watermark removal.
The experiment results indicate that the proposed family
of attacks works significantly better than the baselines. For
a particularly resilient watermark, RivaGAN, regeneration
attacks remove 93-99% of the invisible watermarks while
the baseline attacks remove no more than 3%. Specifically,
when the denoising algorithm is set to be Stable Diffusion,
our attack performs the best. With the empirical results and
the theoretical guarantee, we claim that all invisible image
watermarks are vulnerable to our attack and thus should not
be considered in the settings where our attack applies.
Given the vulnerability of invisible watermarks, we try
to explore other possibilities for image watermarking. One
particular possibility that comes to our attention is the
semantic watermarks. That is to say, we no longer want
the watermarked image to look the same as the original
one. As long as the watermarked image looks similar and
contains similar content, it is considered suitable for use.
One instance of such semantic watermark is Tree-Ring [30],
which in our experiments, does show robustness against our
attack. Obviously, using semantic watermarks is not a perfect
solution because the watermark becomes somewhat “visible”.
However, with the path of invisible watermarks provably
blocked, it does shed some light on protecting the proper
use of AI-generated images.
1.1. Summary of Contributions
• We propose a family of regeneration attacks for image
watermark removal that can be instantiated with many
existing denoising algorithms and generative models.
• We prove that the proposed attack is guaranteed to
remove any pixel-based invisible watermarks against
any detection algorithms and regenerate images that are
close to the original unwatermarked image.
• We evaluate the proposed attack on various invisible
watermarks to demonstrate their vulnerability and its
effectiveness compared with strong baselines, especially
when instantiated with diffusion models.
• We explore other possibilities to embed watermarks in a
visible yet semantically similar way. Although relaxing
2

the invisibility constraint is not the perfect solution,
empirical results indicate that it works better under our
attack and is worth looking into as an alternative.
1.2. Organization
The rest of the paper is organized as follows. Section 2
describes the problem setup for invisible watermark removal
and introduces the baseline watermarking schemes, detection
algorithms and attacking methods. Section 3 presents our
regeneration attack family and how it can be instantiated
with different denoising algorithms and pre-trained models.
Section 4 contains the theoretical guarantees on certified
watermark removal and the quality of regenerated images
from the proposed attack. We report and analyze empirical
results from watermark removal experiments under multiple
settings in Section 5. Section 6 explores other possible ways
to embed watermarks in an image. Some results on Tree-
Ring show that relaxing the invisibility constraint and using
a visible yet semantically similar watermark proves to be
more robust against our attack. Section 7 reviews the existing
literature on image watermarks, image steganography and
attacks against them. Section 8 concludes the paper. Section
9 includes proofs not present in the main body of the paper.
2. Background
This work focuses on the removal of invisible watermarks,
as opposed to visible watermarks, for several reasons. Visible
watermarks are straightforward for potential adversaries
to visually identify and locate within an image. Addition-
ally, removal techniques for visible watermarks are already
extensively studied in prior work such as [33, 34, 35]. In
contrast, invisible watermarks do not have explicit visual cues
that reveal their presence or location. Developing removal
techniques for imperceptible embedded watermarks presents
unique challenges.
2.1. Problem Setup
In this section, we first define invisible watermarks and
the properties of an algorithm for their detection. Then,
we discuss the threat model for the removal of invisible
watermarks.
Definition 2.1 (Invisible watermark). Let x ∈X be the origi-
nal image and xw = Watermark(x, aux) be the watermarked
image for a watermarking scheme that is a function of x
and any auxiliary information aux, e.g., a secret key. We say
that the watermark is ∆-invisible on a clean image x w.r.t. a
“distance” function dist : X × X →R+ if dist(x, xw) ≤∆.
Definition 2.2 (Watermark detection). A watermark detection
algorithm Detect : X ×aux →{0, 1} determines whether an
image ˜x ∈X is watermarked with the auxiliary information
(secret key) being aux. Detect may make two types of
mistakes, false positives (classifying an unwatermarked
image as watermarked) and false negatives (classifying a
watermarked image as unwatermarked). ˜x could be drawn
from either the null distribution P0 or watermarked distri-
bution P1. We define Type I error (or false positive rate)
ϵ1 := Prx∼P0[Detect(x) = 1] and Type II error (or false
negative rate) ϵ2 := Prx∼P1[Detect(x) = 0].
A watermarking scheme is typically designed such that P1
is different from P0 so the corresponding (carefully designed)
detection algorithm can distinguish them almost perfectly,
that is, to ensure ϵ1 and ϵ2 are nearly 0.
An attack on a watermarking scheme aims at post-
processing a possibly watermarked image which changes
both P0 and P1 with the hope of increasing Type I and Type
II error at the same time, hence evading the detection.
We consider the following threat model for removing
invisible watermarks from images:
Adversary’s capabilities. We assume that an adversary
only has access to the watermarked images. The watermark-
ing scheme Watermark, the auxiliary information aux, and
the detection algorithm Detect are unknown to the adversary.
The adversary can make modifications to these already
watermarked images it has access to using arbitrary side
information and computational resources, but it cannot rely
on any specific property of the watermarking process and it
cannot query Detect.
Adversary’s objective. The primary objective of an
adversary is to render the watermark detection algorithm
ineffective. Specifically, the adversary aims to produce an
image ˜x from the watermarked image xw which causes
the Detect algorithm to always have a high Type I error
(false positive rate) or a high Type II error (false negative
rate). Simultaneously, the output image ˜x should maintain
comparable quality to the original, non-watermarked image.
The adversary’s objective will be formally defined later.
2.2. Invisible Watermarking Methods
In this section, we review several well-established in-
visible watermarking schemes that are evaluated in our
experiments (Section 5). These approaches cover a range of
methods, including traditional signal processing techniques
and more recent deep learning methods. They include the
default watermarking schemes employed by the widely used
Stable Diffusion models [11]. We show some invisible
watermarking examples in Figure 3
DwtDctSvd watermarking. The DwtDctSvd watermarking
method [27] combines Discrete Wavelet Transform
(DWT), Discrete Cosine Transform (DCT), and Singular
Value Decomposition (SVD) to embed watermarks in
color images. First, the RGB color space of the cover
image is converted to YUV. DWT is then applied to
the Y channel, and DCT divides it into blocks. SVD
is performed on each block. Finally, the watermark is
embedded into the blocks. DwtDctSvd is the default
watermark used by Stable Diffusion.
RivaGAN watermarking. RivaGAN [23] presents a robust
image watermarking method using GANs. It employs
two adversarial networks to assess watermarked image
3

Original Image
DwtDctSvd WM
RivaGAN WM
SSL WM
StegaStamp WM
Figure 3: Examples of different invisible watermarking methods. The top row shows real photos from the MS-COCO [31]
dataset. The second row shows AI-generated content from DiffusionDB [32]. Invisible watermarks can protect both real and
synthetic imagery. Of the four watermarks shown, StegaStamp introduces the most distortion and lowest quality.
quality and remove watermarks. An encoder embeds the
watermark, while a decoder extracts it. By combining
these, RivaGAN offers superior performance and robust-
ness. RivaGAN is another watermark used by Stable
Diffusion.
StegaStamp watermarking. StegaStamp [28] is a robust
CNN-based watermarking method. It uses differentiable
image perturbations during training to improve noise
resistance. Additionally, it incorporates a spatial trans-
former network to resist minor perspective and geo-
metric changes like cropping. This adversarial training
and spatial transformer enable StegaStamp to withstand
various attacks.
SSL watermarking. SSL watermarking [25] utilizes pre-
trained neural networks’ latent spaces to encode water-
marks. Networks pretrained with self-supervised learn-
ing (SSL) extract effective features for watermarking.
The method embeds watermarks through backpropaga-
tion and data augmentation, then detects and decodes
them from the watermarked image or its features.
Stable Signature watermarking. Stable
Signature
[29]
fine-tunes the decoder of Latent Diffusion Models
(LDM), such that the generated images have hidden
invisible watermarks. The method first trains two CNN
models that encode and extract watermarks in the same
way as HiDDeN [36], and only keeps the watermark
extractor. Then, it fine-tunes the LDM decoder to
conceal a fixed watermark signature by minimizing
the distance between the signature extracted by the
watermark extractor and the target signature, while
trying to keep the generated images close to the original
one. Unlike other methods, Stable Signature merges
watermarking into the generation process itself rather
than using post-processing.
2.3. Invisible Watermark Detection
Watermarking methods embed k-bits of secret informa-
tion into watermarked images. The watermark detection
algorithm Detect, has an extractor that can then extract
the hidden signal (secret bits) from watermarked images. It
uses a statistical test to set a threshold for the extracted bits
to decide if the extracted message closely matches the secret
information.
The statistical test compares the extracted message m′
to the original secret message m ∈{0, 1}k. Following
previous work [37, 38] the test counts the number of matching
bits M(m, m′). If M(m, m′) ≥τ for some threshold
τ ∈{0, . . . , k}, then the image is flagged as watermarked.
This allows some resilience to imperfect watermark extrac-
tion.
Formally, we test the hypothesis H1: the image x was
created by Watermark, against the null hypothesis H0: the
image was not produced by Watermark. Under H0 (i.e., for
standard images), we assume the extracted bits m′
1, . . . , m′
k
are independent and identically distributed Bernoulli random
variables with probability 0.5. Thus, M(m, m′) follows a
binomial distribution B(k, 0.5). The Type I error (false posi-
tive rate, ϵ1) equals the probability that M(m, m′) exceeds τ,
derived from the binomial cumulative distribution function.
This has a closed form using the regularized incomplete beta
function Ix(a; b):
ϵ1(τ) = P (M (m, m′) > τ | H0) = 1
2k
k
X
i=τ+1
k
i

= I1/2(τ + 1, k −τ).
(1)
4

Decision threshold. We consider a watermark to be
removed, if we can reject the null hypothesis H0 with a
p-value less than 0.01. The null hypothesis states that τ
matching bits were extracted from the standard images by
random chance. In practice, for a k = 32-bit watermark, we
require at least 24 bits to be extracted correctly in order
to confirm the presence of a watermark. This provides a
reasonable balance between detecting real watermarks and
avoiding false positives.
2.4. Existing Attacking Methods
In this section, we review common attacking methods
that aim to degrade or remove invisible watermarks in images.
These methods are widely used to measure the robustness
of watermarking algorithms against removal or tampering
[23, 25, 28, 29, 30]. The attacking methods can be categorized
into destructive attacks, where the watermark is considered
part of the image and actively removed by corrupting the
image, and constructive attacks, where image processing
techniques like denoising are used to obscure the watermark.
Destructive attacks intentionally corrupt the image to
degrade or remove the embedded watermark. Common
destructive attack techniques include:
Brightness/Contrast adjustment. This attack adjusts the
brightness and contrast parameters of the image globally.
Adjusting the brightness/contrast makes the watermark
harder to detect.
JPEG compression. JPEG is a common lossy image com-
pression technique. It has a quality factor parameter that
controls the amount of compression. A lower quality
factor leads to more loss of fine details and a higher
chance of degrading the watermark.
Image rotation. Rotating the watermarked image by a spe-
cific angle can cause the watermark detector to fail to
find the watermark. Rotation causes the synchronization
between the watermark embedder and detector to be
lost.
Gaussian noise. This attack adds random Gaussian noise to
each pixel of the watermarked image. The variance of
the Gaussian noise distribution controls the strength of
the noise. A higher variance leads to more degradation
of the watermark signal.
Constructive attacks aim to remove the watermark by
improving image quality and restoring the original unwater-
marked image. Examples include:
Gaussian blur. Blurring the image by convolving it with a
Gaussian kernel smoothens the watermark signal and
makes it less detectable. The kernel size and standard
deviation parameter control the level of blurring.
BM3D denoising. BM3D is a highly effective image denois-
ing technique based on Block-matching and 3D filtering
[21]. It exploits self-similarities within the image to
remove noise and disturbances. BM3D can attenuate
watermark signals during the denoising process.
3. The Proposed Regeneration Attack
We first present in Section 3.1 an overview of the
proposed regeneration attack that combines both destructive
and constructive attacks. Then we demonstrate in Sections
3.2, 3.3, 3.4 how different combinations of destruction and
construction algorithms can instantiate the attack. Note that
information in Section 3.1 is enough for readers to get the
gist of the algorithm and to understand the theoretical and
empirical results. The details about specific instantiations
in Sections 3.2, 3.3, 3.4 are provided for implementation
references and not necessary for understanding the attack.
Examples of watermark attacks are shown in Figure 3.
3.1. Overview
Our attack method first destructs a watermarked image
by adding noise to its representation, and then reconstructs
it from the noised representation.
Specifically, we consider the following type of algorithms
specified by an embedding function ϕ : Rn →Rd, a
regeneration function A : Rd →Rn and a source of i.i.d.
standard Gaussian random variables. The attack algorithm
takes the watermarked image xw ∈Rn as input and returns
ˆx = A

destructive
z
}|
{
ϕ(xw) + N(0, σ2Id)

|
{z
}
constructive
.
(2)
The first step of the algorithm is destructive. It maps the
watermarked image xw to an embedding ϕ(xw) (which is
a possibly different representation of the image), and adds
some random Gaussian noise. The explicit noise shows the
destructive nature of the first step. The second step of the
algorithm is constructive. The corrupted image representation
ϕ(xw)+N(0, σ2Id) is passed through a regeneration function
A to reconstruct the original clean image.
There are various different choices for ϕ and A that
can instantiate our attack. ϕ can be as simple as identity
map, or as complicated as deep generative models including
variational autoencoders [39]. A can be traditional denoising
algorithms from image processing and recent AI models
such as diffusion [26]. The choice of ϕ and A may change
the empirical results, but it does not affect the theoretical
guarantee. In the following sections, we introduce three
combinations of ϕ and A to instantiate the attack. Among
the three, the diffusion instantiation is the most complicated
and we describe it with pseudocode in Algorithm 1.
3.2. Attack Instance 1: Identity Embedding with
Denoising Reconstruction
Set ϕ to be identity map, then A can be any image
denoising algorithm, e.g., BM3D [21], TV-denoising [40],
bilateral filtering [41] DnCNNs [22], or a learned natural
image manifold [21, 22, 42, 43]. A particular example of
interest is a “denoising autoencoder” [44], which takes ϕ to
be identity, adds noise to the image deliberately, and then
5

Algorithm 1 Regeneration Attack Instance:
Removing invisible watermarks with a diffusion model
input The watermarked image xw, a time step t∗determin-
ing the level of noise added.
output A reconstructed clean image ˆx.
1: z0 ←ϕ(xw) // map the watermarked image xw to the
latent space
2: ϵ ∼N(0, Id) // sample a random normal Gaussian noise
3: zt∗←
p
α(t∗)z0 +
p
1 −α(t∗)ϵ // add noise to the
latent, noise level determined by time step t∗
4: ˆz0 ←solve(zt∗, t∗, s, f, g) // denoise the noised latent
to reconstruct a clean latent
5: ˆx ←θ(z0) // map the reconstructed latent back to a
watermark-free image
6: return ˆx
denoises by attempting to reconstruct the image. Observe
that for “denoising autoencoder” we do not need to add
additional noise.
3.3.
Attack Instance 2: VAE Embedding and
Reconstruction
The regeneration attack in Equation 2 can be instantiated
with a variational autoencoder (VAE). A VAE [39] consists of
an encoder qϕ(z|x) that maps a sample x to the latent space
z and a decoder pθ(x|z) that maps a latent z back to the data
space x. Both the encoder and decoder are parameterized
with neural networks. VAEs are trained with a reconstruction
loss that measures the distance from the reconstructed sample
to the original sample and a prior matching loss that restricts
the latent to follow a pre-defined prior distribution.
Instead of mapping x directly to z, the encoder maps it to
the mean µ(x) and variance σ(x) of a Gaussian distribution
and samples from it. Therefore, VAE already adds noise
during the encoding stage (though its variance depends on
the sample x, which is not exactly the same as defined in
Equation 2), so there is no need to add extra noise. Note
that this is similar to the situation of denoising autoencoders
described in Section 3.2, as the denoising autoencoder is a
trivial case of VAE where µ(x) is identity.
3.4. Attack Instance 3: Diffusion Embedding and
Reconstruction
The regeneration attack can also be instantiated with
diffusion models. Diffusion models [26] define a generative
process that learns to sample from an unknown true distri-
bution p(z0). This process is learned by trying to estimate
original samples from samples perturbed with random noise.
In other words, diffusion models are trained to denoise,
which makes them candidates for the regeneration function
A in the proposed attack. Image-generating diffusion models
either directly denoise images in the pixel space [26], or map
images to latent representations and denoise in the latent
space [11]. The former takes ϕ to be identity, while the
latter, known as latent diffusions, takes ϕ to be the mapping
function from the pixel space to the latent space. In our
paper, we use latent diffusions because they are known to
generate better-quality images.
For diffusion models, the process of adding noise to a
clean sample is known as the forward process. Likewise,
the process of denoising a noisy sample is known as the
backward process. The forward process is defined by the
following stochastic differential equation (SDE):
dz = f(z, t)dt + g(t)dw,
(3)
where t ∈[0, 1], z ∈Rd, w(t) ∈Rd is a standard Wiener
process, and f, g are real-valued functions. The backward
process can then be described with its reverse SDE:
dˆz =

f(ˆz, t) −g(t)2∇ˆz log pt(ˆz)

dt + g(t)d ¯w,
(4)
where ˆw is a reverse Wiener process. Diffusion models
parameterize ∇ˆz log pt(ˆz) with a neural network s(z, t) and
train it by minimizing the following loss
Z 1
0
Ep(z0)pt(zt|x0)

λ(t)∥log ∇ztpt(zt|x0) −s(z, t)∥2
2

,
(5)
where λ(t) assigns different weights to different time steps.
By substituting s(z, t) into Equation 4, the backward SDE be-
comes known and solvable using numerical solvers [45, 46],
ˆz0 = solve(zt, t, s, f, g).
(6)
Among many ways to define f and g in Equation 3,
variance preserving SDE (VP-SDE) is commonly used
[11, 26]. It defines the forward process with the following
SDE,
dz = −1
2β(t)zdt +
p
β(t)dw,
(7)
where t ∈[0, 1], z ∈Rd, w(t) ∈Rd is a standard Wiener
process, and β(t) is a pre-defined real valued function de-
picting the noise schedule. Under this setting, the conditional
distribution of the noised sample is the following Gaussian
[47],
p(zt|z0) = N(
p
α(t)z0, 1 −α(t)),
(8)
where α(t) = exp(−
R t
0 β(s)ds). The variance of the original
distribution p(z0) is preserved at any time step.
To remove invisible watermarks from images, we use VP
diffusion models trained to denoise in the latent space [11].
Latent diffusion models do not directly operate on image
pixels. Instead, they first map an image x ∈X to a latent
representation z0 = ϕ(x) and then operate in the latent
space Z. After that, an image can be decoded from the
latent representation, i.e., ˆx = θ(ˆz0).
As defined in Algorithm 1, our algorithm removes the
watermark from the watermarked image xw (defined in
Definition 2.1) using diffusion models. xw is first mapped to
the latent representation z0, which is then noised to the time
step t∗. A latent diffusion model is then used to reconstruct
the latent ˆz0, which is mapped back to an image ˆx.
Similar to denoising autoencoders, in either diffusion
or VAEs, the noise-injection is integral to the algorithms
themselves, and no additional noise-injection is needed.
6

4. Theoretical Analysis
We show in this section that the broad family of re-
generation attacks as defined in Equation 2 enjoy provable
guarantees on their ability to remove invisible watermarks
while retaining the high quality of the original image.
4.1. Certified Watermark Removal
How do we quantify the ability of an attack algorithm
to remove watermarks? We argue that if after the attack,
no algorithm is able to distinguish whether the result is
coming from a watermarked image or the corresponding
original image without the watermark, then we consider the
watermark certifiably removed. More formally:
Definition 4.1 (f-Certified-Watermark-Free). We say that a
watermark removal attack is f-Certified-Watermark-Free (or
f-CWF) against a watermark scheme for a non-increasing
function f : [0, 1] →[0, 1], if for any detection algorithm
Detect : X × aux →{0, 1}, the Type II error (false negative
rate) ϵ2 of Detect obeys that ϵ2 ≥f(ϵ1) for all Type I error
0 ≤ϵ1 ≤1.
Let us also define a parameter to quantify the effect of
the embedding function ϕ.
Definition 4.2 (Local Watermark-Specific Lipschitz property).
We say that an embedding function ϕ : X →Rd satisfies
Lx,w-Local Watermark-Specific Lipschitz property if for a
watermark scheme w that generates xw with x,
∥ϕ(xw) −ϕ(x)∥≤Lx,w∥xw −x∥.
The parameter Lx,w measures how much the embedding
compresses the watermark added on a particular clean image
x. If ϕ is identity, then Lx,w ≡1. If ϕ is a projection matrix
to a linear subspace then 0 ≤Lx,w ≤1 depending on the
magnitude of the component of xw −x in this subspace.
For a neural image embedding ϕ, the exact value of Lx,w
is unknown but given each xw and x it can be computed
efficiently.
Theorem 4.3. For a ∆-invisible watermarking scheme
with respect to ℓ2-distance. Assume the embedding function
ϕ of the diffusion model is Lx,w-Locally Lipschitz. The
randomized algorithm A(ϕ(·) + N(0, σ2Id)) produces a
reconstructed image ˆx which satisfies f-CWF with
f(ϵ1) = Φ

Φ−1(1 −ϵ1) −Lx,w∆
σ

,
where Φ is the Cumulative Density Function function of the
standard normal distribution.
Figure 4 illustrates what the tradeoff function looks
like. The result says that after the regeneration attack, it
is impossible for any detection algorithm to correctly detect
the watermark with high confidence. In addition, it shows
that such detection is as hard as telling the origin of a single
sample Y from either of the two Gaussian distributions
N(0, 1) and N(L2
x,w∆2/σ2, 1).
0.0
0.2
0.4
0.6
0.8
1.0
Type I error
0.0
0.2
0.4
0.6
0.8
1.0
Type II error
Impossibility Region
= 1.16
Indistinguishable
Diffusion attack
Figure 4: Theoretical and empirical trade-off functions of
watermark detectors after our attack. Trade-off functions
indicate how much less Type II error (false negative rate)
the detector gets in return by having more Type I error (false
positive rate). Theoretically, after the attack, no detection
algorithm can fall in the Impossibility Region and have both
Type I error and Type II error at a low level. Empirically,
the watermark detector performs even worse than the theory,
indicating the success of our attack and the validity of the
theoretical bound. The noise level σ is set to 1.16∆.
The proof, deferred to the end of the paper, leverages an
interesting connection to a modern treatment of differential
privacy [48] known as the Gaussian differential privacy [49].
The work of [49] itself is a refinement and generalization
of the pioneering work of [50] and [51] which established a
tradeoff-function view.
Let us instantiate the algorithm with a latent diffusion
model by choosing σ =
p
(1 −α(t∗))/α(t∗) (see Algorithm
1) and discuss the parameter choices.
Remark 4.4 (Two trivial cases). Observe that when α(t∗) = 0,
the result of the reconstruction does not depend on the input
xw, thus there is no information about the watermark in ˆx(0),
i.e., the trade-off function is f(ϵ1) = 1 −ϵ2 — perfectly
watermark-free, however, the information about x (through
xw) is also lost. When α(t∗) = 1, the attack trivially returns
ˆx(0) = xw, which does not change the performance of the
original watermark detection algorithm at all (and it could
be perfect, i.e., ϵ1 = ϵ2 = 0).
Remark 4.5 (Choice of t∗). In practice, the best choice t∗
is in between the two trivial cases, i.e., one should choose
it such that Lx,w∆
p
α(t∗)/(1 −α(t∗)) is a small constant.
The smaller the constant, the more thoroughly the watermark
is removed. The larger the constant, the higher the fidelity
of the regenerated image w.r.t. xw (thus x0 too).
Remark 4.6 (VAE). Strictly speaking, Theorem 4.3 does
7

not directly apply to VAE because the noise added on
the latent embedding depends on the input data. So if it
chooses σ(x0) = 1 and σ(xw) = 0, then it is easy to
distinguish between the two distributions. We can still provide
provable guarantees for VAE if either the input image is
artificially perturbed (so VAE becomes a denoising algorithm)
or the latent space is artificially perturbed after getting the
embedding vector. When σ(x) itself could be stable, more
advanced techniques from differential privacy such as Smooth
Sensitivity or Propose-Test-Release can be used to provide
certified removal guarantees for the VAE attack. In practice,
we find that the VAE attack is very effective in removing
watermarks as is without adding additional noise.
Remark 4.7 (The role of embedding function ϕ). Readers
may wonder why having an embedding function ϕ is helpful
for removing watermarks. We give three illustrative examples.
Pixel quantization. This ϕ is effective against classical
Least Significant Bit (LSB) watermarks. By remov-
ing the lower-significance bits ϕ(x) = ϕ(xw) thus
Lx,w = 0.
Low-pass filtering. By choosing ϕ to be a low-pass filter,
one can effectively remove or attenuate watermarks
injected in the high-frequency spectrum of the Fourier
domain, hence resulting in a Lx,w ≪1 for these
watermarks.
Deep-learning-based image embedding. Modern
deep-
learning-based image models effectively encode a
“natural image manifold,” which allows a natural
image x to pass through while making the added
artificial watermark ϵ smaller. To be more concrete,
consider ϕ to be a linear projection to a d-dimensional
“natural image subspace”. For a natural image x
and watermarked image xw
=
x + ε, we have
ϕ(xw) = ϕ(x) + ϕ(ε) = x + ϕ(ε). If ε ∼N(0, σ2
wIn)
then E[∥ϕ(ε)∥2] = dσ2
w ≪nσ2
w = E[∥ε∥2]. This
projection compresses the magnitude of the watermark
substantially while preserving the signal, thereby
boosting
the
effect
of
the
noise
added
in
the
embedding
space
in
obfuscating
the
differences
between watermarked and unwatermarked images.
Finally, we note that while Theorem 4.3 and 4.8 are
specific to ℓ2-distance, the general idea applies to other
distance functions (e.g., ℓ1 distance). ℓ2-distance is natural
for the Gaussian noise natively introduced by diffusion and
VAE-based regeneration attacks.
4.2. Utility Guarantees
In this section, we prove that the regenerated image ˆx
is close to the original (unwatermarked) image x0. This is
challenging because the denoising algorithm only gets access
to the noisy version of the watermarked image.
Interestingly, we can obtain a general extension lemma
showing that for any black-box generative model that can
successfully denoise a noisy yet unwatermarked image
with high probability, the same result also applies to the
watermarked counterpart, except that the failure probability
is slightly larger.
Theorem 4.8. Let x0 be an image with n pixels and ϕ :
Rn →Rd be an embedding function. Let A be an image
generation / denoising algorithm such that with probability
at least 1−δ, ∥A(ϕ(x0)+N(0, σ2Id))−x0∥≤ξx0,σ,δ. Then
for any ∆-invisible watermarking scheme that produces xw
from a clean image x0, then ˆx = A(ϕ(xw) + N(0, σ2Id))
satisfies that
∥ˆx −x0∥≤ξx0,σ,δ
with a probability at least 1 −˜δ where
˜δ = min
v∈R
(
δ · ev + Φ
 ˜∆
2σ −vσ
˜∆
!
−evΦ
 
−
˜∆
2σ −vσ
˜∆
!)
in which Φ denotes the standard normal CDF and ˜∆:=
Lx0,w∆.
The theorem says that if a generative model is able to
denoise a noisy version of the original image, then the cor-
responding watermark-removal attack using this generative
model provably produces an image with similar quality.
Corollary 4.9. The expression for ˜δ above can be (conser-
vatively) simplified to
˜δ ≤e
L2
x,w∆2
2σ2
· δ1/2.
For example if σ ≍Lx,w∆, then this is saying that if ξx0,σ,δ
depends logarithmically on 1/δ, the same exponential tail
holds for denoising the watermarked image.
The above result is powerful in that it makes no as-
sumption about what perturbation the watermarking schemes
could inject and which image generation algorithm we use.
We give a few examples below.
For denoising algorithms with theoretical guarantees, e.g.,
TV-denoising [52, Theorem 2], our results imply provable
guarantees on the utility for the watermark removal attack
of the form, “w.h.p., 1
n∥ˆx −x0∥2 = ˜O

σTV2d(x0)
n

”, i.e.,
vanishing mean square error (MSE) as n gets bigger.
For modern deep learning-based image denoising and
generation algorithms where worst-case guarantees are usu-
ally intractable, Theorem 4.8 is still applicable for each
image separately. That is to say, as long as their empirical
denoising quality is good on an unwatermarked image, the
quality should also be good on its watermarked counterpart.
5. Evaluation
5.1. Setup
Datasets. We evaluate our attack on two types of images
- real photos taken by humans and images generated by AI
systems. For real photos, we randomly sample 500 images
from the MS-COCO dataset [31], a large-scale image dataset
containing over 328K captioned images. For AI-generated
content, we randomly sample 500 images from DiffusionDB
8

MS-COCO Dataset
DiffusionDB Dataset
Attacker
PSNR↑SSIM↑FID↓Bit Acc↓Detect Acc↓PSNR↑SSIM↑FID↓Bit Acc↓Detect Acc↓
DctDwtSvd watermarking:
Brightness 0.5
27.55
0.795
15.48
0.474
0.132
27.71
0.803
19.62
0.462
0.124
Contrast 0.5
26.44
0.780
12.70
0.473
0.130
26.58
0.787
17.00
0.463
0.118
JPEG 50
28.22
0.796
26.16
0.691
0.398
28.40
0.806
31.91
0.720
0.488
Rotate 90
25.41
0.756 128.45
0.356
0.000
25.51
0.763 115.26
0.366
0.000
Gaussian noise
25.02
0.736
37.94
0.996
0.996
25.13
0.744
45.78
0.989
0.988
Gaussian blur
24.99
0.742
34.09
0.999
1.000
25.10
0.750
36.60
0.999
1.000
BM3D denoise
27.66
0.783
62.59
0.577
0.090
27.94
0.795
52.21
0.623
0.190
VAE-Bmshj2018 26.95
0.767
53.64
0.528
0.006
27.25
0.780
45.74
0.538
0.006
VAE-Cheng2020 26.00
0.744
48.91
0.523
0.016
26.33
0.760
42.45
0.538
0.026
Diffusion model
25.92
0.746
46.34
0.547
0.016
26.32
0.762
48.44
0.563
0.094
RivaGAN watermarking:
Brightness 0.5
27.59
0.793
21.60
0.990
0.998
27.75
0.803
24.26
0.934
0.906
Contrast 0.5
26.48
0.779
17.95
0.993
0.998
26.61
0.788
20.79
0.934
0.904
JPEG 50
28.26
0.795
26.98
0.953
0.982
28.44
0.806
33.45
0.880
0.816
Rotate 90
25.44
0.755 129.77
0.470
0.000
25.54
0.764 116.90
0.478
0.000
Gaussian noise
25.05
0.735
38.27
0.998
1.000
25.16
0.744
45.91
0.958
0.960
Gaussian blur
25.02
0.740
38.88
0.999
1.000
25.13
0.750
39.90
0.974
0.984
BM3D denoise
27.69
0.781
63.04
0.948
0.978
27.98
0.795
53.10
0.874
0.800
VAE-Bmshj2018 26.98
0.766
53.91
0.637
0.062
27.27
0.780
45.87
0.609
0.040
VAE-Cheng2020 26.02
0.742
48.37
0.639
0.058
26.34
0.759
41.95
0.605
0.036
Diffusion model
25.93
0.743
47.60
0.590
0.018
26.33
0.762
48.61
0.567
0.010
SSL watermarking:
Brightness 0.5
27.61
0.792
23.92
0.999
1.000
27.82
0.801
29.24
0.991
0.996
Contrast 0.5
26.49
0.778
21.73
1.000
1.000
26.68
0.786
27.19
0.990
0.994
JPEG 50
28.27
0.793
33.06
0.808
0.800
28.52
0.804
37.65
0.759
0.616
Rotate 90
25.45
0.754 135.86
0.983
1.000
25.61
0.763 121.08
0.964
0.986
Gaussian noise
25.07
0.734
41.60
0.790
0.722
25.23
0.744
48.21
0.736
0.530
Gaussian blur
25.03
0.739
42.23
1.000
1.000
25.20
0.750
46.63
0.996
0.996
BM3D denoise
27.70
0.780
64.89
0.663
0.226
28.05
0.793
53.75
0.639
0.192
VAE-Bmshj2018 26.96
0.764
56.44
0.633
0.142
27.31
0.778
47.16
0.606
0.094
VAE-Cheng2020 25.98
0.740
50.66
0.637
0.154
26.34
0.757
43.40
0.608
0.124
Diffusion model
25.88
0.741
53.84
0.643
0.152
26.33
0.759
55.88
0.587
0.052
StegaStamp watermarking:
Brightness 0.5
24.66
0.737
36.71
1.000
1.000
24.56
0.737
43.12
1.000
1.000
Contrast 0.5
23.76
0.729
36.20
1.000
1.000
23.66
0.728
42.38
0.999
1.000
JPEG 50
25.20
0.735
52.93
1.000
1.000
25.10
0.736
59.80
1.000
1.000
Rotate 90
22.92
0.708 142.35
0.507
0.002
22.81
0.708 124.72
0.511
0.006
Gaussian noise
22.65
0.691
56.11
1.000
1.000
22.55
0.692
63.65
1.000
1.000
Gaussian blur
22.60
0.696
51.12
1.000
1.000
22.50
0.697
57.04
1.000
1.000
BM3D denoise
24.96
0.722
82.71
1.000
1.000
24.91
0.725
73.37
1.000
1.000
VAE-Bmshj2018 24.51
0.703
68.47
0.999
1.000
24.49
0.707
64.63
1.000
1.000
VAE-Cheng2020 23.81
0.672
62.73
1.000
1.000
23.82
0.679
60.65
1.000
1.000
Diffusion model
23.67
0.665
66.84
0.863
0.992
23.76
0.673
66.63
0.859
0.990
Stable Signature watermarking:
Brightness 0.5
28.53
0.864
11.75
0.967
0.990
28.14
0.860
16.38
0.951
0.996
Contrast 0.5
27.20
0.842
10.73
0.965
0.990
26.85
0.838
14.86
0.948
0.996
JPEG 50
29.37
0.873
15.01
0.866
0.966
28.96
0.869
19.66
0.839
0.938
Rotate 90
25.99
0.815 126.68
0.446
0.000
25.65
0.811 120.72
0.452
0.000
Gaussian noise
25.46
0.788
30.60
0.920
0.972
25.15
0.785
43.26
0.906
0.952
Gaussian blur
25.48
0.798
17.72
0.896
0.986
25.16
0.795
23.19
0.867
0.952
BM3D denoise
29.24
0.871
31.65
0.946
0.952
28.83
0.867
33.90
0.935
0.938
VAE-Bmshj2018 28.95
0.867
31.86
0.636
0.248
28.54
0.863
36.22
0.628
0.202
VAE-Cheng2020 28.67
0.864
29.43
0.682
0.442
28.28
0.861
33.89
0.655
0.334
Diffusion model
29.33
0.879
20.64
0.486
0.000
28.96
0.876
27.22
0.497
0.002
TABLE 1: Performance of attacks on different watermarking methods. The table shows attack results on MS-COCO and
DiffusionDB datasets. Image quality of attacked watermarked images is compared to clean images using PSNR, SSIM,
and FID. Watermark removal effectiveness is measured by average bit accuracy and detection accuracy under p < 0.01.
Regeneration attacks (with gray background), especially the diffusion model, consistently achieve high watermark removal
rates across schemes.
9

Original Image DwtDctSvd WM
Brightness
JPEG
Gaussian blur
VAE attack
Diffusion attack
Figure 5: Examples of watermarking attacks against DwtDctSvd, including destructive attacks like brightness change and
JPEG compression, constructive attacks like Gaussian blur, and regeneration attacks using VAEs and diffusion models.
Brightness change, JPEG compression, VAE attack, and diffusion attack successfully remove the watermark. The diffusion
attack maintains high image quality while removing the watermark. The VAE attack over-smooths the image, resulting in
blurriness.
[32], which contains 14 million images produced by Stable
Diffusion using prompts and hyperparameters from real users.
DiffusionDB includes both photorealistic and stylistic images
such as paintings. Using these diverse datasets allows us
to comprehensively test our attack on invisible watermarks
across human and AI-created images.
Watermark settings. To evaluate our attack, we test five
publicly available watermarking methods: DwtDctSvd [27],
RivaGAN [23], StegaStamp [28], SSL watermark [25], and
Stable Signature [29]. These methods represent a range of
approaches, from traditional signal processing to recent deep
learning techniques, as introduced in Section 2. To consider
watermarks with different lengths, we use k = 32 bits for
DwtDctSvd, RivaGAN, and SSL watermark; k = 96 bits for
StegaStamp; and k = 48 bits for Stable Signature. To detect
the watermarks, we set the decision threshold to reject the
null hypothesis with p < 0.01, requiring detection of 24/32,
61/96, and 34/48 bits corrected for the respective methods,
as described in Section 2.3. For watermark extraction, we
use the publicly available code for each method with default
inference and fine-tuning parameters specified in their papers.
Attack baselines. To thoroughly evaluate the robustness
of our proposed watermarking method, we test it against a
10

COCO Dataset
Generated Dataset
Watermark
PSNR↑SSIM↑FID↓Bit Acc↑Detect Acc↑PSNR↑SSIM↑FID↓Bit Acc↑Detect Acc↑
DwtDctSvd
39.38
0.983
5.28
1.000
1.000
37.73
0.972
9.62
1.000
1.000
RivaGAN
40.55
0.978 10.83
1.000
1.000
40.64
0.979 13.56
1.000
1.000
SSL
41.79
0.984 18.86
1.000
1.000
41.88
0.983 23.87
0.998
1.000
StegaStamp
28.50
0.911 35.91
1.000
1.000
28.28
0.900 41.63
1.000
1.000
Stable Signature 30.87
0.898
6.28
0.990
1.000
30.75
0.900
8.29
0.980
1.000
TABLE 2: Performance of different watermarking methods. All methods successfully detect the embedded watermark.
comprehensive set of baseline attacks that represent common
image perturbations. Aligning with the existing benchmark
attacks summarized in Section 2.4, we select both geomet-
ric/quality distortions and noise manipulations that could
potentially interfere with embedded watermarks. Specifically,
the baseline attack set consists of: brightness change of 0.5,
contrast change of 0.5, JPEG compression with quality 50,
rotation by 90 degrees, addition of Gaussian noise with
standard deviation 0.05, Gaussian blur with kernel size 5 and
standard deviation 1, and the BM3D denoising algorithm
with standard deviation 0.1. These manipulations represent
common image processing operations that could potentially
interfere with watermarks.
Proposed attacks. For attacks using variational autoen-
coders, we evaluate two pre-trained image compression
models from the CompressAI library [53]: Bmshj2018 [54]
and Cheng2020 [55]. The compression factors are set to 3
for both models. For diffusion model attacks, we use the
stable-diffusion-2-1 model from Stable Diffusion
[11], a state-of-the-art generative model capable of high-
fidelity image generation. The number of noise steps is set
to 60 and we use pseudo numerical methods for diffusion
models (PNDMs) [45] to generate samples.
Evaluation metrics. We evaluate the quality of at-
tacked and watermarked images compared to the original
cover image using two common metrics: Peak Signal-to-
Noise Ratio (PSNR) defined as PSNR (x, x′) = −10 ·
log10 (MSE (x, x′)), for images x, x′ ∈[0, 1]c×h×w, and
Structural Similarity Index (SSIM) [56] which measures
perceptual similarity. To evaluate the diversity and quality
of generated images, we use Fr´echet Inception Distance
(FID) [57] between generated and real image distributions.
For images watermarked with Stable Signature, we generate
watermarked and unwatermarked images using MS-COCO
captions data and prompts from DiffusionDB. We evaluate
FID using images generated without watermarks. To assess
watermark robustness, we measure the average bit accuracy
(percentage of correctly decoded bits) and average detect
accuracy (percentage of images where decoded bits exceed
the detection threshold). The detection threshold is set to
correctly decode 24/32, 61/96, and 34/48 bits for different
watermarking methods, as described in watermark settings.
The experiments are conducted on Nvidia A6000 GPUs.
5.2. Results and Analysis
This section presents detailed results and analysis of the
regeneration attack experiments on different watermarking
methods. We evaluated five image watermarking methods -
DwtDctSvd, RivaGAN, SSL, StegaStamp, and Stable Signa-
ture - under various attacks and analyzed their robustness.
Some attacking examples are shown in Figure 5.
Watermarking performance without attacks. Table 2
summarizes the watermarked image quality and detection
rates for images watermarked by the five methods without
any attacks. All methods successfully embed and recover
messages in the images. DwtDctSvd, RivaGAN, SSL, and
StegaStamp are post-processing techniques that add water-
marks to existing images, while Stable Signature incorporates
watermarking into the image generation process. Among the
post-processing methods, SSL achieves the best PSNR and
SSIM, indicating higher perceptual quality compared to the
original images. DwtDctSvd and Stable Signature obtain the
lowest FID scores, suggesting the watermarked images have
fidelity comparable to clean images. In contrast, StegaStamp
shows significantly degraded quality with the lowest PSNR
and highest FID. As illustrated in Figure 3, StegaStamp
introduced noticeable blurring artifacts.
Watermark removal effectiveness. Table 1 summarizes
the results of applying different regeneration attacks to
remove watermarks. The VAE and diffusion model attacks
(VAE-Bmshj2018, VAE-Cheng2020, Diffusion) consistently
achieved over 90% removal rates across four out of five
watermarking methods, demonstrating high effectiveness.
Specifically, they remove 91-99% of DwtDctSvd watermarks,
93-99% of RivaGAN watermarks, 85-97% of SSL water-
marks, and 65-100% of Stable Signature watermarks. Image
rotation also shows high removal rates of for DwtDctSvd,
RivaGAN, StegaStamp, and Stable Signature. However,
it was ineffective against SSL, removing only 0-1% of
watermarks. Since image rotation is easy to detect, a pre-
processing step could allow watermark detectors to correct it.
Among all watermarking methods, StegaStamp exhibits the
most robustness with only 1% removal by the diffusion model
and negligible removal by other attacks. The poor visual
quality of StegaStamp suggests a trade-off between higher
watermarked image fidelity using techniques like SSL versus
more robust watermark detection achieved by StegaStamp.
Overall, the consistently high removal rates across various
watermarking schemes demonstrate the effectiveness of
11

0.10
0.15
0.20
0.25
0.30
0.35
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Bit Acc
Detect Acc
0.10
0.15
0.20
0.25
0.30
0.35
25.4
25.6
25.8
26.0
26.2
26.4
26.6
26.8
PSNR
0.10
0.15
0.20
0.25
0.30
0.35
Noise level 
0.74
0.75
0.76
0.77
SSIM
0.10
0.15
0.20
0.25
0.30
0.35
Noise level 
25
30
35
40
45
50
55
60
FID
Figure 6: Regeneration attack with different diffusion steps.
The noise level σ corresponds to the noising steps of {5,
10, 20, 60, 100} for the diffusion model. Larger noise
reduces watermark detection but also harms image quality,
as measured by increasing PSNR and FID and decreasing
SSIM. There is a trade-off between attack efficacy and image
quality.
regeneration attacks for watermark removal, with diffusion
models showing slightly better performance than VAEs.
Image quality reservation. The regeneration attacks
generally preserved image quality well according to PSNR
and SSIM. VAE models achieved higher PSNR and SSIM
while diffusion models obtained better FID, indicating higher
GAN-based perceptual quality. Qualitative inspection of
example images in Figure 5 reveals the VAE outputs exhibit
some blurring compared to the sharper diffusion outputs.
Since PSNR and SSIM are known to be insensitive to blurring
artifacts [58, 59], we conclude that the diffusion models better
preserved image quality under attacks. All attacks resulted in
negligible perceptual differences from the original images.
Watermark removal with different noise levels. To
test the influence of noise level on the effectiveness of
regeneration attacks, we evaluated the diffusion embedding
and reconstruction attack with different denoising steps. As
shown in Figure 6, we test this attack against the DwtDctSvd
watermark while varying the number of denoising steps
from {5, 10, 20, 60, 100}. This resulted in corresponding
noise levels of σ: {0.07, 0.10, 0.14, 0.25, 0.34}. The
results demonstrate that as noise level increases, watermark
detection becomes more difficult. However, higher noise
levels also negatively impact image quality, as evidenced
by increasing PSNR and FID scores, and decreasing SSIM.
This illustrates the trade-off between attack effectiveness
and preservation of image quality. More aggressive noise
insertion degrades watermarks more completely but also
introduces more distortion.
(a) No Watermark
(b) Pixel WM
(c) Semantic WM
Figure 7: The image with a pixel-based watermark such
as DwtDctSvd looks almost the same as the original. The
image with a semantic watermark such as Tree-Ring contains
the same content but is visibly different from the original.
Original image generated with the prompt “an astronaut
riding a horse in Zion National Park” from [30].
6. Defense with Semantic Watermarks
In this section, we discuss possible defenses that are
resilient to the proposed attack, and although Theorem 4.3 has
guaranteed that no detection algorithm will be able to detect
the watermark after our attack, the guarantee is based on the
invisibility with respect to ℓ2 distance. Therefore, by relaxing
that invisibility constraint and thus making the watermark
more visible, we may be able to prevent the watermark
from being removed. One less-harmful way to loosen the
invisibility constraint is with semantic watermarks. As shown
in Figure 7, pixel-based watermarks such as DwtDctSvd keep
the image almost intact, while semantic watermarks change
the image significantly but retain its content.
6.1. Tree-Ring Watermarks
Tree-Ring Watermarking [30] is a new technique that
robustly fingerprints diffusion model outputs in a way that
is semantically hidden in the image space. An image with a
Tree-Ring watermark does not look the same as the image,
but it is semantically similar (in Figure 7, both the original
and the semantically watermarked image contain an astronaut
riding a horse in Zion National Park). Unlike existing
methods that perform post-hoc modifications to images after
sampling, Tree-Ring Watermarking subtly influences the
entire sampling process, resulting in a model fingerprint. The
watermark embeds a pattern into the initial noise vector used
for sampling. These patterns are structured in Fourier space so
that they are invariant to convolutions, crops, dilations, flips,
and rotations. After image generation, the watermark signal
is detected by inverting the diffusion process to retrieve the
noise vector, which is then checked for the embedded signal.
Wen et al. [30] demonstrated that Tree-Ring Watermarking
can be easily applied to arbitrary diffusion models, including
text-conditioned Stable Diffusion, as a plug-in with negligible
loss in FID.
6.2. Defense Experiments
To evaluate Tree-Ring as an alternative watermark, we
use the same datasets from the previous experiments in
12

Attacker
COCO Detect Acc↓Generated Detect Acc↓
Brightness 0.5
1.000
1.000
Contrast 0.5
1.000
1.000
JPEG 50
1.000
0.994
Rotate 90
1.000
1.000
Gaussian noise
1.000
0.996
Gaussian blur
1.000
1.000
BM3D denoise
1.000
1.000
VAE-Bmshj2018
0.998
0.994
VAE-Cheng2020
1.000
0.994
Diffusion model
1.000
0.998
TABLE 3: Tree-Ring watermarks are robust against all the
tested attacks, including the ones we proposed.
0
100
200
300
2 distance
Image
DwtDctSvd
RivaGAN
SSL
StegaStamp
Stable Signature
Tree-Ring
0
10
20
30
40
2 distance
Latent
Figure 8: The ℓ2 distances between original images and
watermarked ones. Tree-Ring watermarked images are much
more different in both pixel space and latent space, making
Tree-Ring a visible watermark.
Section 5 - MS-COCO and DiffusionDB. However, since
Tree-Ring adds watermarks during the generation process of
diffusion, it cannot directly operate on AI-generated images.
Instead, it needs textual inputs that describe the content of
the images. We use captions from MS-COCO and the user
prompts of DiffusionDB as the input prompts. The same set
of attacks (including our proposed attack) is applied to the
Tree-Ring watermarked images.
As shown in Table 3, Tree-Ring watermarks show excep-
tional robustness against all the attacks tested. However, such
robustness does not come for free. We depict the ℓ2-distances
between original images and watermarked ones in Figure 8.
Images with Tree-Ring watermarks are significantly more
different from the original images in both the pixel space and
the latent space. These results indicate that Tree-Ring, as an
instance of semantic watermarks, shows the potential to be
an alternative solution to the image watermarking problem.
However, as our theory predicts, the robustness comes at the
price of more visible differences.
7. Related Work
Image watermarking and steganography. Steganogra-
phy and invisible watermarking are key techniques in infor-
mation hiding, serving diverse purposes such as copyright
protection, privacy-preserved communication, and content
provenance. Early works in this area employ hand-crafted
methods, such as Least Significant Bit (LSB) embedding
[3], which subtly hides data in the lowest order bits of
each pixel in an image. Over time, numerous techniques
have been developed to imperceptibly embed secrets in the
spatial [18] and frequency [19, 60] domains of an image.
Additionally, the emergence of deep learning has contributed
significantly to this field. Deep learning methods offer
improved robustness against noise while maintaining the
quality of the generated image. SteganoGAN [24] uses
generative adversarial networks (GAN) for steganography
and perceptual image optimization. RivaGAN [23], further
improves GAN-based watermarking by leveraging attention
mechanisms. SSL watermarking [25], trained with self-
supervision, enhances watermark features through data aug-
mentation. Stable Signature [29] fine-tunes the decoder of
Latent Diffusion Models to add the watermark. Tree-Ring
[30] proposes a semantic watermark, which watermarks
generative diffusion models using minimal shifts of their
output distribution.
Deep generative models. The high-dimensional nature
of images poses unique challenges to generative modeling. In
response to these challenges, several types of deep generative
models have been developed, including Variational Auto-
Encoders (VAEs) [61, 62], Generative Adversarial Networks
(GANs) [63], flow-based generative models [64], and dif-
fusion models [11, 26]. These models leverage deep latent
representations to generate high-quality synthetic images
and approximate the true data distribution. One particularly
interesting use of generative models is for data purification,
i.e., removing the adversarial noise from a data sample.
The purification is similar to watermark removal except that
purification is a defense strategy while watermark removal
is an attack. The diffusion-based approach in [65] is similar
to an instance of our regeneration attack, but the usage
is different in our paper and our theoretical guarantee of
watermark removal is stronger. In this paper, we aim to
demonstrate the capability of these deep generative models
in removing invisible watermarks from images by utilizing
the latent representations obtained through the encoding and
decoding processes.
8. Conclusion
We proposed a regeneration attack on invisible water-
marks that combines destructive and constructive attacks. Our
theoretical analysis proved that the proposed regeneration
attack is able to remove any invisible watermark from images
and make the watermark undetectable by any detection
algorithm. We showed with extensive experiments that the
proposed attack performed well empirically. The proofs and
13

experiments revealed the vulnerability of invisible water-
marks. Given this vulnerability, we explored an alternative
defense that uses visible but semantically similar watermarks.
Our experiments with one instance of semantic watermark
showed promising results. Our findings on the vulnerability
of invisible watermarks underscore the need for shifting the
research/industry emphasis from invisible watermarks to their
alternatives.
References
[1]
A. Z. Tirkel, G. Rankin, R. Van Schyndel, W. Ho, N. Mee, and
C. F. Osborne, “Electronic watermark,” Digital Image Computing,
Technology and Applications (DICTA’93), pp. 666–673, 1993.
[2]
R. G. Van Schyndel, A. Z. Tirkel, and C. F. Osborne, “A digital
watermark,” in Proceedings of 1st international conference on image
processing, vol. 2.
IEEE, 1994, pp. 86–90.
[3]
R. B. Wolfgang and E. J. Delp, “A watermark for digital images,”
in Proceedings of 3rd IEEE International Conference on Image
Processing, vol. 3.
IEEE, 1996, pp. 219–222.
[4]
D. Boneh and J. Shaw, “Collusion-secure fingerprinting for digital
data,” IEEE Transactions on Information Theory, vol. 44, no. 5, pp.
1897–1905, 1998.
[5]
S. A. Craver, N. D. Memon, B.-L. Yeo, and M. M. Yeung, “Can
invisible watermarks resolve rightful ownerships?” in Storage and
Retrieval for Image and Video Databases V, vol. 3022.
SPIE, 1997,
pp. 310–321.
[6]
J. A. Bloom, I. J. Cox, T. Kalker, J.-P. Linnartz, M. L. Miller, and
C. B. S. Traw, “Copy protection for dvd video,” Proceedings of the
IEEE, vol. 87, no. 7, pp. 1267–1276, 1999.
[7]
M. Goljan, J. J. Fridrich, and R. Du, “Distortion-free data embedding
for images,” in Information Hiding: 4th International Workshop, IH
2001 Pittsburgh, PA, USA, April 25–27, 2001 Proceedings 4. Springer,
2001, pp. 27–41.
[8]
I. J. Cox and M. L. Miller, “The first 50 years of electronic
watermarking,” EURASIP Journal on Advances in Signal Processing,
vol. 2002, pp. 1–7, 2002.
[9]
A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hier-
archical text-conditional image generation with clip latents,” arXiv
preprint arXiv:2204.06125, 2022.
[10] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,
K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans et al.,
“Photorealistic text-to-image diffusion models with deep language
understanding,” Advances in Neural Information Processing Systems,
vol. 35, pp. 36 479–36 494, 2022.
[11] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
“High-resolution image synthesis with latent diffusion models,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 10 684–10 695.
[12] Google,
“Google
keynote
(google
i/o
23),”
Google
blog, 2023. [Online]. Available: https://io.google/2023/program/
396cd2d5-9fe1-4725-a3dc-c01bb2e2f38a/
[13] K.
Wiggers,
“Microsoft
pledges
to
watermark
ai-
generated
images
and
videos,”
TechCrunch
blog,
2023.
[Online].
Available:
https://techcrunch.com/2023/05/23/
microsoft-pledges-to-watermark-ai-generated-images-and-videos/
[14] M. AI, “Make-a-video,” Meta website, 2023. [Online]. Available:
https://makeavideo.studio/
[15] D.
Bartz
and
K.
Hu,
“Openai,
google,
others
pledge
to
watermark
ai
content
for
safety,
white
house
says,”
Jul
2023.
[Online].
Available:
https://www.reuters.com/technology/
openai-google-others-pledge-watermark-ai-content-safety-white-house-2023-07-21/
[16] M. Kelly, “White house rolls out plan to promote ethical ai,” May 2023.
[Online]. Available: https://www.theverge.com/2023/5/4/23710533/
google-microsoft-openai-white-house-ethical-ai-artificial-intelligence
[17] ——,
“Chuck
schumer
calls
on
congress
to
pick
up
the
pace
on
ai
regulation,”
Jun
2023.
[On-
line].
Available:
https://www.theverge.com/2023/6/21/23768257/
ai-schumer-safe-innovation-framework-senate-openai-altman
[18] K. Ghazanfari, S. Ghaemmaghami, and S. R. Khosravi, “Lsb++: An
improvement to lsb+ steganography,” in TENCON 2011-2011 IEEE
Region 10 Conference.
IEEE, 2011, pp. 364–368.
[19] V. Holub and J. Fridrich, “Designing steganographic distortion using
directional filters,” in 2012 IEEE International workshop on informa-
tion forensics and security (WIFS).
IEEE, 2012, pp. 234–239.
[20] O. Hosam, “Attacking image watermarking and steganography-a sur-
vey,” International Journal of Information Technology and Computer
Science, vol. 11, no. 3, pp. 23–37, 2019.
[21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image denois-
ing by sparse 3-d transform-domain collaborative filtering,” IEEE
Transactions on image processing, vol. 16, no. 8, pp. 2080–2095,
2007.
[22] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a
gaussian denoiser: Residual learning of deep cnn for image denoising,”
IEEE transactions on image processing, vol. 26, no. 7, pp. 3142–3155,
2017.
[23] K. A. Zhang, L. Xu, A. Cuesta-Infante, and K. Veeramachaneni,
“Robust invisible video watermarking with attention,” ArXiv, vol.
abs/1909.01285, 2019.
[24] K. A. Zhang, A. Cuesta-Infante, L. Xu, and K. Veeramachaneni,
“Steganogan: High capacity image steganography with gans,” arXiv
preprint arXiv:1901.03892, 2019.
[25] P. Fernandez, A. Sablayrolles, T. Furon, H. J’egou, and M. Douze,
“Watermarking images in self-supervised latent spaces,” ICASSP 2022 -
2022 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3054–3058, 2021.
[26] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
models,” Advances in Neural Information Processing Systems, vol. 33,
pp. 6840–6851, 2020.
[27] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker, Digital
watermarking and steganography.
Morgan kaufmann, 2007.
[28] M. Tancik, B. Mildenhall, and R. Ng, “Stegastamp: Invisible hyperlinks
in physical photographs,” in Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, 2020, pp. 2117–2126.
[29] P. Fernandez, G. Couairon, H. J´egou, M. Douze, and T. Furon, “The
stable signature: Rooting watermarks in latent diffusion models,” arXiv
preprint arXiv:2303.15435, 2023.
[30] Y. Wen, J. Kirchenbauer, J. Geiping, and T. Goldstein, “Tree-ring
watermarks: Fingerprints for diffusion images that are invisible and
robust,” arXiv preprint arXiv:2305.20030, 2023.
[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.
Springer, 2014, pp. 740–755.
[32] Z. J. Wang, E. Montoya, D. Munechika, H. Yang, B. Hoover, and
D. H. Chau, “DiffusionDB: A large-scale prompt gallery dataset
for text-to-image generative models,” arXiv:2210.14896 [cs], 2022.
[Online]. Available: https://arxiv.org/abs/2210.14896
[33] Y. Liu, Z. Zhu, and X. Bai, “Wdnet: Watermark-decomposition network
for visible watermark removal,” in Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision, 2021, pp.
3685–3693.
[34] A. Hertz, S. Fogel, R. Hanocka, R. Giryes, and D. Cohen-Or, “Blind
visual motif removal from a single image,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 6858–6867.
14

[35] X. Cun and C.-M. Pun, “Split then refine: stacked attention-guided
resunets for blind single image visible watermark removal,” in
Proceedings of the AAAI conference on artificial intelligence, vol. 35,
no. 2, 2021, pp. 1184–1192.
[36] J. Zhu, R. Kaplan, J. Johnson, and L. Fei-Fei, “Hidden: Hiding data
with deep networks,” 2018.
[37] N. Yu, V. Skripniuk, S. Abdelnabi, and M. Fritz, “Artificial fingerprint-
ing for generative models: Rooting deepfake attribution in training
data,” in Proceedings of the IEEE/CVF International conference on
computer vision, 2021, pp. 14 448–14 457.
[38] N. Lukas and F. Kerschbaum, “Ptw: Pivotal tuning watermarking for
pre-trained image generators,” arXiv preprint arXiv:2304.07361, 2023.
[39] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
arXiv preprint arXiv:1312.6114, 2013.
[40] L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based
noise removal algorithms,” Physica D: nonlinear phenomena, vol. 60,
no. 1-4, pp. 259–268, 1992.
[41] C. Tomasi and R. Manduchi, “Bilateral filtering for gray and color
images,” in Sixth international conference on computer vision (IEEE
Cat. No. 98CH36271).
IEEE, 1998, pp. 839–846.
[42] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image
denoising,” in 2005 IEEE computer society conference on computer
vision and pattern recognition (CVPR’05), vol. 2.
Ieee, 2005, pp.
60–65.
[43] K. Zhang, Y. Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte,
“Plug-and-play image restoration with deep denoiser prior,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 44,
no. 10, pp. 6360–6376, 2021.
[44] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol,
and L. Bottou, “Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising criterion.”
Journal of machine learning research, vol. 11, no. 12, 2010.
[45] L. Liu, Y. Ren, Z. Lin, and Z. Zhao, “Pseudo numerical methods
for diffusion models on manifolds,” in International Conference on
Learning Representations, 2021.
[46] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, “Dpm-solver: A
fast ode solver for diffusion probabilistic model sampling in around 10
steps,” Advances in Neural Information Processing Systems, vol. 35,
pp. 5775–5787, 2022.
[47] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon,
and B. Poole, “Score-based generative modeling through stochastic
differential equations,” in International Conference on Learning
Representations, 2020.
[48] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise
to sensitivity in private data analysis,” in Theory of Cryptography:
Third Theory of Cryptography Conference, TCC 2006, New York, NY,
USA, March 4-7, 2006. Proceedings 3.
Springer, 2006, pp. 265–284.
[49] J. Dong, A. Roth, and W. J. Su, “Gaussian differential privacy,” Journal
of the Royal Statistical Society Series B: Statistical Methodology,
vol. 84, no. 1, pp. 3–37, 2022.
[50] L. Wasserman and S. Zhou, “A statistical framework for differential
privacy,” Journal of the American Statistical Association, vol. 105, no.
489, pp. 375–389, 2010.
[51] P. Kairouz, S. Oh, and P. Viswanath, “The composition theorem for
differential privacy,” in International conference on machine learning.
PMLR, 2015, pp. 1376–1385.
[52] J.-C. H¨utter and P. Rigollet, “Optimal rates for total variation de-
noising,” in Conference on Learning Theory.
PMLR, 2016, pp.
1115–1146.
[53] J. B´egaint, F. Racap´e, S. Feltman, and A. Pushparaja, “Compressai: a
pytorch library and evaluation platform for end-to-end compression
research,” arXiv preprint arXiv:2011.03029, 2020.
[54] J. Ball´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston,
“Variational image compression with a scale hyperprior,” arXiv preprint
arXiv:1802.01436, 2018.
[55] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Learned image com-
pression with discretized gaussian mixture likelihoods and attention
modules,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 7939–7948.
[56] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.
[57] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
“Gans trained by a two time-scale update rule converge to a local
nash equilibrium,” Advances in neural information processing systems,
vol. 30, 2017.
[58] P. Ndajah, H. Kikuchi, M. Yukawa, H. Watanabe, and S. Muramatsu,
“Ssim image quality metric for denoised images,” in Proc. 3rd WSEAS
Int. Conf. on Visualization, Imaging and Simulation, 2010, pp. 53–58.
[59] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality
assessment: from error visibility to structural similarity,” IEEE Trans-
actions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.
[60] T. Pevn`y, T. Filler, and P. Bas, “Using high-dimensional image models
to perform highly undetectable steganography,” in Information Hiding:
12th International Conference, IH 2010, Calgary, AB, Canada, June
28-30, 2010, Revised Selected Papers 12.
Springer, 2010, pp. 161–
177.
[61] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting
and composing robust features with denoising autoencoders,” in
Proceedings of the 25th international conference on Machine learning,
2008, pp. 1096–1103.
[62] A. Van Den Oord, O. Vinyals et al., “Neural discrete representation
learning,” Advances in neural information processing systems, vol. 30,
2017.
[63] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
networks,” Communications of the ACM, vol. 63, no. 11, pp. 139–144,
2020.
[64] D. Rezende and S. Mohamed, “Variational inference with normalizing
flows,” in International conference on machine learning.
PMLR,
2015, pp. 1530–1538.
[65] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandku-
mar, “Diffusion models for adversarial purification,” in International
Conference on Machine Learning.
PMLR, 2022, pp. 16 805–16 827.
[66] B. Balle and Y.-X. Wang, “Improving the gaussian mechanism for
differential privacy: Analytical calibration and optimal denoising,” in
International Conference on Machine Learning.
PMLR, 2018, pp.
394–403.
[67] I. Mironov, “R´enyi differential privacy,” in IEEE computer security
foundations symposium (CSF).
IEEE, 2017, pp. 263–275.
15

9. Proofs of Technical Results
Proof of Theorem 4.3. By the conditions on the invisible
watermark and the local Lipscthiz assumption on ϕ, we get
that
∥ϕ(x0) −ϕ(xw)∥2 ≤Lx0,w∆.
This can be viewed as the ℓ2-local-sensitivity of ϕ at x0 in
the language of differential privacy literature.
By [49][Theorem 2.7] (Gaussian mechanism), we get
that the tradeoff function is as stated.
Finally, by [49][Proposition 2.8] (postprocessing), the
tradeoff function for the generated image ˆx also satisfies the
same tradeoff function as stated.
Proof of Theorem 4.8. The key idea is to use the definition
of indistinguishability (differential privacy, but for a fixed
pair of neighbors, rather than for all neighbors). So we say
two input x, x′ are (v, w)-indistinguishable using the output
of a mechanism M if for any event S,
Pr[M(x) ∈S] ≤ev Pr[M(x′) ∈S] + w.
and the same also true when x, x′ are swapped. In our case,
we have already shown that (from the proof of Theorem 4.3)
for any post-processing algorithm A, x0 and xw are indis-
tinguishable using ˆx in the trade-off function sense. [66]
obtained a “dual” characterization which says that the same
Gaussian mechanism satisfies (v, w)-indistinguishability with
w = Φ
 ˜∆
2σ −vσ
˜∆
!
−evΦ
 
−
˜∆
2σ −vσ
˜∆
!
for all v ∈R. By instantiating the event S to be that ∥ˆx −
x0∥≤ξx0,σ,δ, then we get
Pr
xw[∥ˆx −x0∥≤ξx0,σ,δ] ≤ev Pr
x0 [∥ˆx −x0∥≤ξx0,σ,δ] + w
= ev · δ + w.
This completes the proof.
Proof of Corollary 4.9. The w, v “privacy profile” implies a
Renyi-divergence bound (one can also get that directly from
the Renyi-DP of gaussian mechanism) which implies (by
Proposition 10 of [67]) that
˜δ ≤min
u≥1

e
u ˜
∆2
2σ2 · δ
(u−1)/u
.
The stated result is obtained by setting u = 2.
16
