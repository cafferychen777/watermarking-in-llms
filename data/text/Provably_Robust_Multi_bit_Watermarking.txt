Provably Robust Multi-bit Watermarking for AI-generated Text
Wenjie Qu1 Wengrui Zheng1 Tianyang Tao1 Dong Yin1 Yanze Jiang1
Zhihua Tian3 Wei Zou2 Jinyuan Jia2 Jiaheng Zhang1
1National University of Singapore 2Pennsylvania State University 3Zhejiang University
Abstract
Large Language Models (LLMs) have demonstrated remark-
able capabilities of generating texts resembling human lan-
guage. However, they can be misused by criminals to create
deceptive content, such as fake news and phishing emails,
which raises ethical concerns. Watermarking is a key tech-
nique to address these concerns, which embeds a message
(e.g., a bit string) into a text generated by an LLM. By em-
bedding the user ID (represented as a bit string) into gener-
ated texts, we can trace generated texts to the user, known
as content source tracing. The major limitation of existing
watermarking techniques is that they achieve sub-optimal per-
formance for content source tracing in real-world scenarios.
The reason is that they cannot accurately or efficiently extract
a long message from a generated text. We aim to address the
limitations.
In this work, we introduce a new watermarking method
for LLM-generated text grounded in pseudo-random segment
assignment. We also propose multiple techniques to further
enhance the robustness of our watermarking algorithm. We
conduct extensive experiments to evaluate our method. Our
experimental results show that our method substantially out-
performs existing baselines in both accuracy and robustness
on benchmark datasets. For instance, when embedding a mes-
sage of length 20 into a 200-token generated text, our method
achieves a match rate of 97.6%, while the state-of-the-art
work Yoo et al. only achieves 49.2%. Additionally, we prove
that our watermark can tolerate edits within an edit distance
of 17 on average for each paragraph under the same setting.
1
Introduction
Generative models such as GPT-4 [48], Stable Diffusion [58],
Wavenet [46], VideoPoet [31] could generate remarkably
human-like content such as text, image, audio, video, etc..
Consequently, they are utilized in various real-world applica-
tions such as ChatBot [47], Web searching [43], and program-
ming assistance [19]. Despite their fascinating capabilities,
1
1
1
1
0
0
1
1
1
0
0
0
1
0
1
1
Â  Â  Â User ID:Â 0xF38B.
Packing
ECC Encode
Health authorities
in New Zealand
said that about
200 passengers
â€¦
Balanced Segment
Assignment
Embed
Segments:
Prompt:
Passengers had
been infected by
a norovirus. The
ship passengers
â€¦Â 
Suspicious Text:
Extract
Â UserÂ 0xF38BÂ 
generated the text
Message:
LLM
15
3
8
11
15
3
8
11
9
6
Figure 1: Outline of our watermarking application scenario
and workflow. Users query the LLM with a prompt. During
text generation, using our watermarking method, the service
provider embeds a unique user ID into the generated text.
Later, when some suspicious LLM-generated text used for
malicious purposes is found, the service provider can identify
and extract the watermark to trace the original user who gen-
erated the text.
LLMs could also be misused to generate fake news [55], phish-
ing emails [28], fake product reviews [2], resulting in severe
ethical concerns for their deployment in the real world. Wa-
termarking [13] is a key technique to address those concerns.
It enables the detection and tracing of machine-generated
content to specific users, ensuring authenticity and deterring
misuse. Generally speaking, a watermarking method consists
of two functions: embedding and extraction. The embedding
function embeds a piece of pre-selected message (usually
a bit string) into generated content (e.g., image, text, audio,
video), while the extraction function identifies the watermark
and extracts the message embedded in the content.
In this paper, we consider typical usage scenarios for wa-
termarking LLM-generated texts. Suppose a model provider
deploys an LLM (e.g., GPT-4 [48]) as a cloud service. When
a user queries the LLM with a prompt, the watermarking al-
gorithm enables the service provider to embed a unique user
ID into the generated text. Given a text that is suspected to be
LLM-generated (e.g., fake news on social media), the water-
1
arXiv:2401.16820v3  [cs.CR]  7 Sep 2024

mark extraction function can be used to identify whether the
text is watermarked. Furthermore, if the watermark is identi-
fied, the user ID can be extracted from the watermark to trace
the original user who uses the LLM to generate the given text.
Limitations of existing text watermarking methods. Most
of the existing studies in text watermarking [10, 16, 29, 33,
34,51,70] can only be used to detect whether a text is gen-
erated by an LLM or not. However, they are not suitable
for use under the previously mentioned scenario of content
source tracing. This stems from the fact that they only embed
a single indicator that the text is watermarked. In response,
several pioneering works [17,63] have explored the design
of multi-bit watermarking schemes for LLMs. Nevertheless,
their deployments in real-world scenarios are hindered by
the high computational cost of their extraction function. For
example, [17] requires about 8 hours to extract a 32-bit mes-
sage from a given text. In other words, these approaches lack
scalability when dealing with long messages, posing signif-
icant challenges for practical deployments. Although [68]
enhances the efficiency of extraction, their method falls short
in the accuracy of extracting the embedded message from
watermarked texts. For example, when the message length is
16 bits, their method can only extract the embedded message
from the watermarked text with a probability of 73.6%. In
essence, a scalable text watermarking method that can accu-
rately and efficiently embed and extract multi-bit messages
from generated text remains elusive.
Goals in designing practical watermarking methods. Due
to the requirements of content source tracing, there are four
key goals when designing a watermarking method for texts:
multi-bit capacity, correctness, robustness and efficiency.
â€¢ Multi-bit Capacity: A watermarking method should
allow embedding a multi-bit message into a generated
text. This goal is essential to content source tracing [39].
â€¢ Correctness: A watermarking method should enable
the embedded message to be correctly extracted from a
watermarked text.
â€¢ Robustness: A watermarking method should be robust
against post-processing operations such as word dele-
tion, synonym substitution, and paraphrasing. This goal
ensures the embedded message can be reliably extracted
even if the watermarked text undergoes various forms of
editing or alteration.
â€¢ Efficiency: The efficiency goal requires the computa-
tional overhead of the watermarking method during both
the embedding and extraction process to be small, which
is essential for real-world usage.
Our work. This work focuses on designing a watermarking
scheme that simultaneously meets the four goals. Design-
ing such a scheme is particularly challenging because the
information redundancy in discrete text is much more lim-
ited compared to that in images or videos. To ensure a high
probability of correct extraction, our critical insight is to allo-
cate the information redundancy evenly across each bit of the
message.
To achieve an even distribution of information redundancy
without losing efficiency, we propose pseudo-random segment
assignment as our algorithmâ€™s core component. We partition
the entire message into multiple bit segments for embedding.
During each tokenâ€™s generation, we pseudo-randomly assign a
segment to the token to embed. By packing multiple bits into
a segment before embedding, our design makes the informa-
tion redundancy distributed to each bit more balanced while
avoiding the overhead of enumerating all possible messages
during extraction. Our approach combines the strengths of two
state-of-the-art multi-bit watermarking techniques: the high
correctness of message enumeration-based methods [17,63]
and the efficiency of bit assignment-based methods [68].
We design two techniques to refine our method:
â€¢ Balanced segment assignment. We propose a strategy
to further eliminate the imbalance in pseudo-random
segment assignment. In particular, we employ dynamic
programming to allocate tokens to each segment in a bal-
anced manner. As a result, the probability of extracting
erroneous segments is significantly reduced.
â€¢ Adopting error-correction code. Watermarked text
may be edited by users, potentially distorting the embed-
ded message. As a result, the extraction process might
yield an incorrect message. To enhance the accuracy and
robustness of our method, we utilize error-correction
codes (ECC) to encode our segments before embedding
them into the text. By leveraging ECCâ€™s ability to cor-
rect errors in the embedded segments, our watermark
can tolerate more extensive edits.
Additionally, we derive the provable robustness guarantee
of our watermarking method. We prove that our method can
reliably extract the correct watermark from a watermarked
text as long as the total number of word or token edits (inser-
tion, deletion, substitution) remains within a certain bound.
To validate the performance of our method, we conduct
extensive experiments on multiple benchmark datasets (Open-
Gen [32], C4 news dataset [53], Essays dataset [59]) with
multiple large language models (LLaMA-2-7B [62], Guanaco-
7B [15], and Falcon-7B [3]). We use match rate as the eval-
uation metric, which measures the proportion of generated
texts that can exactly extract embedded message without error.
We have the following observations from the experimental
results. First, our watermarking method could extract the em-
bedded message from a watermarked text with an extremely
high match rate. For instance, our method achieves a 97.6%
match rate when the lengths of the message and watermarked
text are 20 bits and 200 tokens respectively, while Yoo et
2

al. [68] only has a match rate of 49.2%. Second, our water-
mark is robust against manipulations. For instance, under the
same setting, when the attacker applies copy-paste attack, our
method still retains 90% match rate, surpassing the match rate
of Yoo et al. [68] which drops to 32%. In addition, we prove
that our watermark can tolerate edits within an edit distance
of 17 on average for each paragraph under the same setting.
Third, our method also enjoys remarkable efficiency, capable
of extracting a 32-bit message from a sentence in just 0.6
seconds, whereas [63] requires 8,300 seconds. Our results
show that our method also maintains the quality of the text.
In the experiments, we observe a negligible alteration in the
distribution of Perplexity (PPL) between watermarked and
unwatermarked text. These experimental results validate that
our scheme simultaneously achieves the four goals.
Our key contributions are summarized as follows.
â€¢ We propose a new Large Language Model (LLM) multi-
bit watermarking scheme. Our innovation lies in the
design of pseudo-random segment assignment and fur-
ther improvement techniques. Our scheme demonstrates
a substantial enhancement in both the correctness and ro-
bustness of the multi-bit watermarking algorithm while
preserving text quality.
â€¢ We are the first to derive the theoretical robustness guar-
antee under text edit distance for LLM multi-bit water-
marking based on probabilistic analysis. The robustness
bound for each generated paragraph can be efficiently
computed in practice.
â€¢ We validate the effectiveness of our proposed scheme
through extensive experiments. Our results demon-
strate a significant outperformance compared to ex-
isting multi-bit watermarking schemes, particularly in
terms of the correctness of extraction, robustness against
different types of attacks, and efficiency of extrac-
tion. The results also provide evidence of the practi-
cal applicability of our approach in real-world settings.
We release our source code at https://github.com/
randomizedtree/segment-watermark.
2
Background and Related Work
2.1
Zero-bit watermarking
Watermarking digital text is challenging due to the discrete
nature of text [27]. While watermarking on image [45, 52]
and other domains focuses on embedding long bit strings,
most of the research on text watermarking focuses on zero-bit
watermarking, namely only identifying whether the content
is watermarked or not. The main application of zero-bit wa-
termarking is distinguishing between machine-generated text
(watermarked) and human-written text (unwatermarked).
Existing works on zero-bit watermarking. Various research
has been conducted on zero-bit watermarking in text. Early
approaches are mainly rule-based, such as paraphrasing [5]
and synonym substitution [61]. Later, advancements in mod-
ern language models led to improved methods. In [1], they
designed a watermarking framework in which both embed-
ding and extraction are handled by text-to-text adversarially
trained language models. [23, 24] embeds watermarks by
context-aware lexical substitution. Recently, Kirchenbauer
et al. [29] proposed imperceptible watermarks by modifying
the output logits of language models during token genera-
tion. This approach has emerged as a promising approach for
distinguishing language model-generated text from human-
written text. [10,33] proposed distortion-free watermarking
schemes which retain the original LLMâ€™s output distribution.
Zhang et al. [69] trained a message-encoding neural network
for watermark injection, followed by a message-decoding
neural network for watermark extraction. Zhao et al. [70]
proposed a watermarking construction that offers provable
robustness guarantees for editing properties. [16,37] proposed
watermarking schemes that are publicly verifiable. [38, 57]
proposed semantic-based watermarks that enhance robustness
against paraphrasing for zero-bit watermarking.
State-of-the-art zero-bit watermarking [29]. Next, we in-
troduce a state-of-the-art zero-bit watermarking solution for
LLM. The main idea of Kirchenbauer et al. [29] is to bias
a subset of tokens to be more frequently generated during
token generation. Suppose we have an LLM with vocabu-
lary ğ‘‰. Given a prompt as input, the LLM autoregressively
generates a response. At each decoding step ğ‘–(i.e., the LLM
generates the ğ‘–-th token), the LLMâ€™s decoded token is sampled
from logits vector ğ‘£ğ‘–âˆˆR|ğ‘‰|, where |ğ‘‰| represents the size of
ğ‘‰. For instance, in greedy decoding, the ğ‘–-th output token is
argmax ğ‘—ğ‘£ğ‘–ğ‘—.
To embed the watermark, at step ğ‘–, a subset of tokens ğº
is selected from the vocabulary ğ‘‰, i.e., ğºâŠ†ğ‘‰. ğºis called
the green list, while ğ‘‰\ ğºis called the red list. The ratio
of |ğº| to |ğ‘‰| is a fixed hyperparameter. The green/red list
selection is determined by a random seed ğ‘ obtained by a
hash function taking the (ğ‘–âˆ’1)-th token as input. The logits
vector is modified by adding bias term ğ›¿to all tokens in ğº.
The text generated by LLM is watermarked when sampled
from the modified logits vector, because the ğ‘–-th output token
is more likely to belong to the green list ğºseeded with the
(ğ‘–âˆ’1)-th token, than to the red list.
If the ğ‘–-th output token belongs to the green list ğºseeded
with the (ğ‘–-1)-th token, we call it a green token. After gener-
ating the text with a watermark, the addition of bias results in
the majority of tokens being green tokens. For unwatermarked
text, by expectation, if the green list and the red list are equally
partitioned, approximately half of the tokens are green tokens
and the remaining half of them are red tokens. Therefore, the
service provider could leverage this distribution difference
through statistical testing, namely Z-test [8], to detect whether
3

the text is watermarked or not.
2.2
Multi-bit watermarking
Another line of work in LLM watermarking is multi-bit wa-
termarking. Different from zero-bit watermarking, the embed-
ding function of multi-bit watermarking embeds a multi-bit
message into the generated text, whereas its extraction func-
tion extracts the multi-bit message from the given text. While
the applications of zero-bit watermarking are mainly restricted
to detection of LLM-generated text, multi-bit watermarks can
be applied in broader scenarios such as content source tracing.
Multiple works [17,63,68] generalized the idea of select-
ing green tokens and adding bias to them in [29]. Wang et
al. [63] proposed a multi-bit watermarking technique by us-
ing the message content as the seed to randomly select green
list words. Fernandez et al. [17] consolidated multi-bit wa-
termarks for LLMs by adopting more robust statistical tests.
Concurrent with our work, Boroujeny et al. [6] extended the
distortion-free watermark in [33] to the multi-bit setting. An-
other concurrent work Cohen et al [12] proposed a block-
based multi-bit watermarking method. More similar to our
work, Yoo et al. [68] proposed to pseudo-randomly assign
each token a bit position to embed. Next, we provide a de-
tailed discussion on the limitations of the existing multi-bit
watermarking schemes. We also discuss the differences be-
tween our work and other works in LLM watermarking which
also leverage error-correction code.
Limitations of existing message enumeration-based multi-
bit watermark. During embedding, message enumeration
based watermarks [17,63] determines the green list by letting
the hash function to take both the message ğ¾and the previ-
ous token as input. During extraction, the service provider
needs to enumerate all possible messages and compute the
number of green tokens for each candidate. Then the message
candidate that corresponds to the maximal green tokens is
regarded as the message embedded in the text. The underly-
ing principle is that the green token number for the correct
message candidate is likely to be much larger than the green
token number for incorrect candidates. Despite high accuracy,
the heavy computational cost limits the practicality of this ap-
proach in real-world applications. This is due to the necessity
of enumerating all 2ğ‘potential messages, where ğ‘is the bit
length of the message.
Limitations of bit assignment-based multi-bit watermark.
The bit assignment-based multi-bit watermark [68] pseudo-
randomly assigns each token a bit position for embedding.
Specifically, the bit position for the ğ‘–-th token is pseudo-
randomly determined by the (ğ‘–âˆ’1)-th token. This approach
offers high efficiency. However, a significant drawback of this
method is its low correctness. The number of tokens used
to embed each bit can be quite uneven due to imbalanced
token frequencies. As a result, some bit positions may be
represented by very few tokens or even none, making it likely
to extract incorrect results for those positions.
Limitations of distortion-free multi-bit watermark. The
watermark method in [6] achieves the interesting theoretical
property of exactly preserving the output distribution of the
original LLM. However, our analysis reveals its poor perfor-
mance when embedding longer messages in practice. This
issue arises from its distribution interval encoding strategy.
Specifically, the probability of generating the same token for
neighboring messages is (1âˆ’21âˆ’ğ‘)17, where ğ‘is the embeded
message length. This probability is nearly 1 for relatively large
ğ‘. For example, when embedding 20 bits into 200 tokens, the
watermarked text will be exactly same for two neighboring
messages with a probability greater than 99%. This makes it
almost impossible to distinguish between the two neighboring
messages. For more details, please refer to Appendix E.
Limitations of block-based multi-bit watermark. [12] pro-
posed a block-based watermarking method by partitioning
generated text into continuous blocks based on entropy. For
each block, they embed one bit of the message using existing
watermarking schemes such as those in [29]. The major draw-
back of their solution is its poor robustness against real-world
editing. An attacker can simply delete an entire sentence from
the generated text, which will likely cause some blocks to be
lost, leading to extraction failure.
Differences with works adopting ECC. Multiple concurrent
works [9,16,35] in zero-bit LLM watermarking also utilize
ECC in their constructions. However, their settings and tech-
niques are significantly different from ours. [16] achieves the
property of public detectability by embedding a cryptographic
signature into text. The major difference between [16] and our
scheme is that they embed each bit using a fixed number of
continuous tokens. However, this solution is highly vulnerable
to token deletion and insertion, even a single deletion might al-
ter multiple signature bits. [9] presents theoretical guarantees
of high detection probability under bounded edition, while our
method further guarantees high probability of perfect mes-
sage extraction under bounded edition. [35] adopts BCH
code [7] in their scheme for AI-generated code watermarking.
They embed information by applying code transformations;
thus, their method only applies to code LLMs. In contrast, our
scheme is applicable to general AI text generation scenarios.
2.3
Background on Reed-Solomon codes
In this subsection, we briefly introduce the background of
the error-correction code (ECC) used in our watermarking
schemes. Reed-Solomon codes form a large family of error-
correction codes that are constructed using polynomials over
a finite field. They are widely used in the real-world applica-
tions, including digital communication systems [14], storage
devices [54], and cryptography [4]. Reed-Solomon code is
optimal in the sense that it achieves the largest possible mini-
mum distance for linear codes of a given size [60]. Thus we
adopt Reed-Solomon codes [56] in this work.
4

Definition. A Reed-Solomon code is a block code notated as
(ğ‘›, ğ‘˜,ğ‘¡)ğ‘ğ‘š. It takes ğ‘˜symbols from a finite field GF(ğ‘ğ‘š) 1 as
the input message and then outputs ğ‘›symbols belonging to
the same field as the encoded message. As long as no more ğ‘¡
symbols are changed in the encoded message, it is guaranteed
that the ğ‘˜symbols can be fully recovered.
In this paper, following the convention of applying Reed-
Solomon codes [18, 64], we focus on the case where ğ‘= 2,
meaning all symbols are integers in [0,2ğ‘šâˆ’1]. For a detailed
exposition of the construction and fundamental principles of
Reed-Solomon code, readers may refer to [11,36].
3
Problem Formulation
We present the formal definition of multi-bit watermarks for
LLMs.
3.1
Problem definition
Suppose we have an LLM that generates text in response
to an input prompt. Given an arbitrarily bit string ğ¾(called
message) whose length is ğ‘, we aim to embed it into a text
generated by the LLM. In particular, we aim to design two
functions for a multi-bit watermark algorithm, namely Em-
bedding and Extraction. The embedding function aims to
embed the message ğ¾into a text generated by the LLM. The
extraction function aims to extract the message from a given
text. Formally, we have the following definition.
Definition 3.1 (Multi-bit Watermarking). Given an LLM, a
message ğ¾, and a prompt token sequence P = [ğ‘ƒ0 Â·Â·Â· ğ‘ƒğ‘âˆ’1],
a multi-bit watermark algorithm consists of the following two
functions:
S = Embedding(LLM,ğ¾,P)
(1)
ğ¾â€² = Extraction(S).
(2)
where S = [ğ‘†0,Â·Â·Â· ,ğ‘†ğ‘‡âˆ’1] is the generated token sequence
with length ğ‘‡.
3.2
Design goals
We aim to design a multi-bit watermark algorithm to achieve
the following goals.
Correctness. This goal means the algorithm could accurately
extract a message embedded in a watermarked text. Formally,
this requires the following conditional probability to be equal
(or close) to 1.
Pr(Extraction(S) = ğ¾|S = Embedding(LLM,ğ¾,P)).
(3)
Robustness. This goal means the algorithm is resilient against
post-processing operations on watermarked text, such as word
1GF(ğ‘ğ‘š) stands for Galois field (finite field) of order ğ‘ğ‘š.
addition, word deletion, synonym substitution, and paraphras-
ing. Let Sâ€² represent the edited version of the original text S.
The edit distance between S and Sâ€² is bounded by D(S,Sâ€²) â‰¤ğœ‚,
which means S can be transformed into Sâ€² within ğœ‚basic oper-
ations ("insertion", "deletion", and "replacement" of tokens).
Then the robustness goal requires the following conditional
probability is equal (or close) to 1.
Pr(Extraction(Sâ€²) = ğ¾|S = Embedding(LLM,ğ¾,P), ğ·(S,Sâ€²) â‰¤ğœ‚)
(4)
Efficiency. The efficiency goal means the algorithm can em-
bed/extract a message into/from a generated text efficiently.
4
Methodology
In this section, we will discuss the design insights and algo-
rithmic details of our watermarking method, along with the
high-level concepts behind its provable robustness.
4.1
Design insights
4.1.1
Insights from previous works
To further gain insights into designing our multi-bit water-
marking scheme, we dive into the design of several previous
works [12,16,68]. To embed a ğ‘bit message, these works all
partition the message into ğ‘individual bits, and partition the
tokens into ğ‘groups to embed each bit respectively. Specifi-
cally, [12,16] partitions continuous tokens into groups. They
use each group of continuous tokens to embed each individual
bit. However, the major drawback of continuous token parti-
tion is weak robustness. Because removing an entire sentence
may cause the lost of a whole group of tokens, leading to the
extraction error of the corresponding bit.
The algorithm in [68] is different in that it assigns each to-
ken a bit position to embed based on the hash of the previous
token. In this way, tokens are divided in a pseudo-random
manner. Due to the pseudo-randomness in the bit embedded
by each token, this method achieves better robustness against
attacks such as sentence removal. However, this method fails
to achieve high accuracy when embedding longer messages.
Deeper inspection into the bits extracted incorrectly reveals
that these bits are often allocated few tokens or even none
at all. This imbalance in token allocation significantly hin-
ders [68] to achieve high correctness, because when a bit is
embedded using only a few tokens, there is a considerable
probability of extracting the bit incorrectly. In the extreme
case, when no tokens are assigned to a bit, the bitâ€™s value can
only be guessed randomly.
Further investigations reveal that this imbalance is rooted
in the imbalance in natural language token frequency [21,65].
Because the distribution of each tokenâ€™s previous token is
severely imbalanced, the bit positions that frequent tokens
5

Table 1: Example of our watermarking and extraction results. The content in the "Prompt" column comes from datasets. The
"Real Completion" column represents the ground truth completion from datasets. The "Watermarked Text" column represents
text embedded with 20-bit information by our watermarking algorithm. For more examples, see Table 5 in Appendix B.
Prompt
Real Completion
Watermarked Text
Original
bits
Extracted
bits
... The bin floor is where the grain
is stored before being ground into
flour. The sack hoist mechanism is
housed on this floor, driven from
the wooden crown wheel on the
floor below by belt.
The stone floor is where the two pairs
of underdrift French Burr millstones are
located. The stones are 4 feet 4 inches
(1.32 m) and 4 feet 7 inches (1.40 m)
diameter. The wooden crown wheel is
located towards the ceiling. [...continues]
The floor beneath that is the grinding
floor, containing pairs of millstones
capable of producing bushels of flour
per day. This floor is reached by a
spiral stairs from within the dust floor.
The crown wheel is on this level [...continues]
00110100110100111010
00110100110100111010
map to are assigned more tokens. Assume we want to em-
bed 32 bits into the generated text. Ideally, each bit is ex-
pected to be selected with a probability of 1/32. However,
frequent tokens like "the" already exceed this balanced proba-
bility of 1/32 (the frequency of "the" is 3.4% as measured on
the C4 [53] dataset). Other less frequent tokens may still be
mapped to the same bit as "the" with nearly 1/32 probability.
This causes the probability of the bit position corresponding
to hash(â€œğ‘¡â„ğ‘’â€) being chosen to be at least twice as high as in
the uniform case. Consequently, some bit positions are rarely
selected and therefore fewer tokens are used to embed these
bits.
However, we also discover that this problem is not that
severe when embedding shorter messages. For example, if we
only need to pseudo-randomly assign each token to one of the
5 group, the hash result of a popular token with 1% frequency
only has a relatively small influence on the balance of token
allocation. This raises the question: how can we reduce the
number of groups into which we divide tokens while still
embedding the same message?
4.1.2
Key ideas of our watermark design
Our solution is to pack multiple bits together into bit seg-
ments. During embedding, each token is pseudo-randomly
assigned a segment instead of a single bit. This approach
effectively reduces the number of token groups while still
embedding the same message. Note that each token now si-
multaneously embeds every bit in its assigned segment. Con-
sequently, the number of tokens embedding each bit is much
more balanced.
We offer an toy example to illustrate how the bal-
ance is improved. Assume that we intend to embed
a 16-bit message into 200 tokens.
The number of
tokens
pseudo-randomly
allocated
for
each
bit
is
[4,20,16,8,14,14,6,22,16,10,8,14,15,3,7,23].
How-
ever, if each continuous 4 bits are packed together to
form segments, the number of tokens allocated to each
segment is [48,56,48,48]. Take the first segment as an
example. Since the 48 tokens allocated to it simultaneously
embed the first segment of 4 bits, each bit in the first
segment is conceptually embedded by 48/4 = 12 tokens.
Now the number of tokens embedding each bit becomes
[12,12,12,12,14,14,14,14,12,12,12,12,12,12,12,12].
From this example, we can obtain that segment-based
embedding can significantly improve the balance in the
distribution of tokens embedding each bit.
We denote the message we intend to embed as ğ¾, and its
bit length as ğ‘. The watermarked text generated by the LLM
comprises ğ‘‡tokens. Formally, our method divides the orig-
inal message ğ¾into ğ‘˜segments. Without loss of generality,
we assume ğ‘is a multiple of ğ‘˜. For each token, we pseudo-
randomly assign the segment it embeds based on its previous
token. This approach partitions the ğ‘‡tokens into ğ‘˜groups in-
stead of ğ‘groups. Each group of tokens is assigned a segment
with a value in the range [0,2
ğ‘
ğ‘˜âˆ’1] to embed.
Next, we need to embed each segment into its allocated
token group. Inspired by [17,63], we can use the segmentâ€™s
value as an additional input besides the previous token to the
hash function to compute a random seed ğ‘ . Seed ğ‘ is then
used to sample the current tokenâ€™s green token list. During ex-
traction, for each segment, we enumerate the possible values
in [0,2
ğ‘
ğ‘˜âˆ’1] and choose the value that maximizes the count
of green tokens as the segmentâ€™s extracted value.
The underlying principle is that the number of green tokens
associated with the correct value is likely to be much larger
than that of other values. The advantage of correct value
comes from the significant differences among the green lists
pseudo-randomly generated with different seeds. Through
biasing the tokens in green list, each token embedding this
segment is highly likely to belong to the green list of the
segmentâ€™s value. On the other hand, for every other value, the
probability for each generated token to belong to this valueâ€™s
green list is only 1
2. This probability difference allows us to
distinguish between the correct value and other values.
We present a sketch of our segment assignment-based wa-
termark embedding function in Figure 2. Suppose the message
to embed is "01100011" and we divide the message into 4
segments with values [1,2,0,3]. For simplicity, we only show
4 tokens in the LLM vocabulary. After the LLM is provided
with prompt "Matt sat on the" to generate the next token, it
outputs the logits of these 4 tokens as [âˆ’1.9,1.0,2.2,1.7]. To
embed the watermark, the previous token "the" will first be
fed into a hash function to determine the index of segment
that current token should embed. In this case the segment
index is 1, and therefore, the segment value we embed in the
6

0
1
1
0
0
0
1
1
Message:
Segments:
Hash
Matt
sat
on
the
couch
chair
floor
cat
2
-1.9
1
2
0
3
Assigned:
Segment ID 1
couch
Current token:
1
2
2
Previous
token:
2
Logits:
Biased
Logits:
3
Green
List:
couch
floor
1.0
2.2
1.7
-1.9
3.0
2.2
3.7
Figure 2: Simplified example of pseudo-random segment
assignment-based watermark embedding. 1âƒDetermine the
index of segment to embed based on previous token. 2âƒObtain
seed ğ‘ based on previous token and segment value. 3âƒSelect
green list using seed ğ‘ .
current token is 2. In the second step, we obtain the seed
ğ‘ using hash function taking the previous token "the" and
the segment value 2 as input. This seed is used to select the
green token list from the vocabulary. In our example, the
green list contains 2 tokens "floor" and "couch". We add bias
ğ›¿= 2 to all green tokens, resulting in the modified logits vec-
tor [âˆ’1.9,3.0,2.2,3.7]. Finally, we use greedy sampling to
choose "couch" as the next token. Through biasing tokens in
green list, instead of token "chair", "couch" is selected as the
output token.
Our proposed approach is much more efficient during ex-
traction compared with [17,63]. [17,63] have extraction com-
plexity ğ‘‚(2ğ‘) due to the need to enumerate over the whole
potential message space. In our approach, the extraction com-
plexity is only ğ‘‚(ğ‘˜Â·2
ğ‘
ğ‘˜), as the enumeration is conducted for
each ğ‘
ğ‘˜-bits segment individually.
4.1.3
Further improvements
Although our algorithm has shown significant advantages
over existing ones, we aim to enhance its real-world applica-
bility by maximizing its correctness and robustness without
compromising extraction efficiency. To achieve this, we have
developed two additional techniques.
Our first improvement aims to further eliminate the segment
assignment imbalance induced by the imbalance of different
tokenâ€™s frequency. We design a dynamic programming-based
algorithm to compute a frequency-balanced mapping to assign
tokens to different segments based on natural token frequency.
During embedding and extraction, we leverage this mapping
to assign each token a segment to embed. After taking tokensâ€™
natural frequency into consideration, we further alleviate the
imbalance in segment assignment.
Algorithm 1 Embed multi-bit watermark
Input: Autoregressive language model LLM; generate length ğ‘‡;
prompt sequence ğ‘ƒ0 Â·Â·Â· ğ‘ƒğ‘âˆ’1; vocabulary set ğ‘‰; bias ğ›¿; set of
Reed-Solomon code F ; message ğ¾with ğ‘bits ; minimal code
rate ğ‘…ğ‘; minimal recover rate ğ‘…ğ‘Ÿ; a hash function hash; secret
key ğ‘ ğ‘˜
Output: Generated sequence ğ‘†0 Â·Â·Â·ğ‘†ğ‘‡âˆ’1
Initialize S =""
Select optimal ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘šusing Algorithm 3 under ğ‘…ğ‘Ÿand ğ‘…ğ‘
Obtain
ECC-encoded
segment
sequence:
ğ¸
=
( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘š.encode(ğ¾)
Calculate pseudo-random token-to-segment mapping ğ‘€: [0, |ğ‘‰|âˆ’
1] â†’[0, Ë†ğ‘›âˆ’1] with ğ‘ ğ‘˜
for ğ‘–= 0,1,Â·Â·Â· ,ğ‘‡âˆ’1 do
Compute logits, ğ‘£ğ‘–= LLM(ğ‘ƒ0 Â·Â·Â· ğ‘ƒğ‘âˆ’1,ğ‘†0 Â·Â·Â·ğ‘†ğ‘–âˆ’1)
Compute segment index ğ‘= ğ‘€[ğ‘†ğ‘–âˆ’1]
Compute random seed ğ‘ = hash(ğ‘ ğ‘˜,ğ‘†ğ‘–âˆ’1, ğ¸[ğ‘])
Use seed ğ‘ to select green list ğºâŠ‚ğ‘‰, where |ğº| = |ğ‘‰|
2
Bias the logits:
for ğ‘¤= 0,1,Â·Â·Â· , |ğ‘‰| âˆ’1 do
if ğ‘¤âˆˆğºthen
Ë†ğ‘£ğ‘–[ğ‘¤] = ğ‘£ğ‘–[ğ‘¤] + ğ›¿
end if
end for
Append ğ‘†ğ‘–to S by sampling from logits Ë†ğ‘£ğ‘–
end for
return S
Additionally, our algorithm still faces a challenge: when
the attackerâ€™s modifications are substantial enough to distort
the value embedded in a segment, the embedded message may
no longer be correctly extracted. To address this issue, our
second improvement integrates Reed-Solomon code [56] into
our watermarking scheme. We opt for Reed-Solomon code
in light of its effectiveness in correcting burst errors. This
property naturally aligns with the needs of our watermarking
scheme in that when extraction error occurs in our scheme,
they often take the form of burst errors, where only one or
two segments are incorrect, but multiple bits within those
segments may be wrong.
Based on our segment partition, the input message can be
viewed as a sequence of ğ‘˜segments (each of which is an
integer in [0,2
ğ‘
ğ‘˜âˆ’1]). We use Reed-Solomon code scheme
to encode these ğ‘˜segments into ğ‘›segments. Instead of em-
bedding the original ğ‘˜segments, we embed these ğ‘›segments
into the generated text. The property of Reed-Solomon code
ensures that if no more than ğ‘¡= âŒŠğ‘›âˆ’ğ‘˜
2 âŒ‹segments are extracted
incorrectly from the text, the final message decoded from
the ğ‘›segments will be guaranteed correct. Adopting Reed-
Solomon code simultaneously improves the correctness and
robustness of our method.
We illustrate the concrete technique of our multi-bit water-
marking in the following section. We present the high-level
workflow of our multi-bit watermarking algorithm in Figure 1.
7

Algorithm 2 Extract multi-bit watermark
Input: Text length ğ‘‡; text sequence ğ‘†0 Â·Â·Â·ğ‘†ğ‘‡âˆ’1; vocabulary set
ğ‘‰; Reed-Solomon code scheme ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘š; pseudo-random token-
to-segment mapping ğ‘€: [0, |ğ‘‰| âˆ’1] â†’[0, Ë†ğ‘›âˆ’1]; hash function
hash; secret key ğ‘ ğ‘˜;
Output: Extracted message ğ¾â€² with ğ‘bits
Initialize COUNT as Ë†ğ‘›Ã—2 Ë†ğ‘šmatrix with 0
Define ğ¸â€² as integer vector with length Ë†ğ‘›
for ğ‘–= 0,1,Â·Â·Â· ,ğ‘‡âˆ’1 do
Compute segment index ğ‘= ğ‘€[ğ‘†ğ‘–âˆ’1]
for ğ‘—= 0,1,Â·Â·Â· ,2 Ë†ğ‘šâˆ’1 do
Compute random seed ğ‘ = hash(ğ‘ ğ‘˜,ğ‘†ğ‘–âˆ’1, ğ‘—)
Use seed ğ‘ to select set ğºâŠ‚ğ‘‰, where |ğº| = |ğ‘‰|
2
if ğ‘†ğ‘–âˆˆğºthen
COUNT[ğ‘][ ğ‘—] += 1
end if
end for
end for
for ğ‘= 0,1,Â·Â·Â· , Ë†ğ‘›âˆ’1 do
ğ¸â€²[ğ‘] â†argmax ğ‘—âˆˆ[0,2 Ë†ğ‘šâˆ’1] COUNT[ğ‘][ ğ‘—]
end for
ğ¾â€² = ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘š.decode(ğ¸â€²)
return ğ¾â€²
4.2
Design details of multi-bit watermarking
We first illustrate the details of balanced segment assignment
used in our watermark embedding and extraction function.
Then we discuss the procedure of embedding a message ğ¾
with ğ‘bits into a text generated by an LLM. Finally, we
introduce how to extract the message from given text.
Balanced segment assignment. Balanced segment assign-
ment can be transformed into the following problem: given
the frequency ğ‘ğ‘–of token ğ‘–appearing in natural text, we need
to pseudo-randomly map each ğ‘–âˆˆ[0,Â·Â·Â· , |ğ‘‰|âˆ’1] to a segment
ğ‘—âˆˆ[0,Â·Â·Â· , Ë†ğ‘›âˆ’1], and make the probability of each segment
to be chosen as close to uniform as possible.
A strawman way to reformulate this problem is to divide
ğ‘0,Â·Â·Â· , ğ‘|ğ‘‰|âˆ’1 into Ë†ğ‘›subsets and minimize the variance of the
sum of each subset. However, such a problem can be reduced
to the partition problem [22,42], which is NP-hard.
To that end, we propose an alternative version of the prob-
lem. Specifically, assume that the IDs of |ğ‘‰| tokens are
pseudo-randomly shuffled using private key sk, ranging in
{0,Â·Â·Â· , |ğ‘‰| âˆ’1}; we partition the vocabulary ğ‘‰into Ë†ğ‘›groups,
with tokens in each group having continuous IDs. We denote
the frequency of token with ID ğ‘–after shuffle as ğ‘â€²
ğ‘–. Our target
is to minimize the variance of the sum within each group,
thereby ensuring the plan to achieve optimal balance.
Through derivation, the goal is equivalent to minimizing
the sum of the square of each groupâ€™s sum. Denote the sum
of each group ğ‘–as Sumğ‘–. We aim to minimize Ã Ë†ğ‘›âˆ’1
ğ‘–=0 Sum2
ğ‘–.
We propose to solve this problem by dynamic program-
ming. For the tokens with ID in [0,ğ‘–] divided into ğ‘—groups,
denote ğ·(ğ‘–, ğ‘—) as the minimal squared sum of these groupsâ€™ in-
ternal sum. Denote the starting token ID of the ğ‘—âˆ’th group as
â„“. The first â„“tokens are partitioned into ğ‘—âˆ’1 groups. ğ·(ğ‘–, ğ‘—)
must transition from some ğ·(â„“âˆ’1, ğ‘—âˆ’1). Thus, we have:
ğ·(ğ‘–, ğ‘—) = minâ„“=ğ‘—âˆ’1,Â·Â·Â· ,ğ‘–ğ·(â„“âˆ’1, ğ‘—âˆ’1) + (Ãğ‘–
ğ‘¢=â„“ğ‘â€²
ğ‘¢)2
The computation of each ğ·(ğ‘–, ğ‘—) requires ğ‘‚(|ğ‘‰|) complex-
ity. Thus, the complexity of computing ğ·(|ğ‘‰| âˆ’1, Ë†ğ‘›) will
eventually become ğ‘‚( Ë†ğ‘›Â· |ğ‘‰|2). Since this dynamic program-
ming only needs to be done once for each Reed-Solomon
code scheme, the computation overhead is acceptable. It can
be further speeded up by leveraging parallel computing.
Design of Embedding function. Our Embedding function
consists of the following steps.
Step I: In the first step, we aim to select an ECC scheme
that has enough payload and satisfies the minimal code rate
ğ‘…ğ‘and minimal recover rate ğ‘…ğ‘Ÿ. We first filter for all schemes
(ğ‘›, ğ‘˜,ğ‘¡)2ğ‘šâˆˆF satisfying ğ‘˜ğ‘š= ğ‘, ğ‘˜
ğ‘›â‰¥ğ‘…ğ‘and ğ‘¡
ğ‘›â‰¥ğ‘…ğ‘Ÿ. Among
these candidate schemes, we choose the code (ğ‘›, ğ‘˜,ğ‘¡) with
the lowest ğ‘›to embed. For simplicity, we use ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘što
denote the selected code scheme. Algorithm 3 in Appendix C
presents the complete ECC scheme selection process.
After we determine the code scheme to use, we compute the
pseudo-random token-to-segment mapping ğ‘€: [0, |ğ‘‰|âˆ’1] â†’
[0, Ë†ğ‘›âˆ’1] with a secret key ğ‘ ğ‘˜using the above-mentioned
balanced segment assignment algorithm. Secret key ğ‘ ğ‘˜is used
to pseudo-randomly shuffle the tokens, therefore protecting
our mapping from being discovered by the attacker.
Step II: In the second step, we use the previously selected
Reed-Solomon code scheme ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘što encode ğ¾, resulting
in an ECC-encoded sequence ğ¸(referred to as the encoded
message) with a length of Ë†ğ‘›, where each element is within the
range [0,2 Ë†ğ‘šâˆ’1].
Step III: In the last step, we embed the encoded message ğ¸
into text generated by an LLM. The process of embedding ğ¸
into a text is as follows: at the ğ‘–-th step of token generation,
we query the token-to-segment mapping ğ‘€with previous
generated token ğ‘†ğ‘–âˆ’1 to obtain the segment index ğ‘. We de-
note the ğ‘-th segment as ğ¸[ğ‘]. Then we compute a random
seed ğ‘ = hash(ğ‘ ğ‘˜,ğ‘†ğ‘–âˆ’1,ğ¸[ğ‘]) with secret key ğ‘ ğ‘˜, the previ-
ous token ğ‘†ğ‘–âˆ’1 and ğ¸[ğ‘]. Secret key ğ‘ ğ‘˜is used to strengthen
the security of our scheme, similar to [29]. Without knowing
the secret key, a potential attacker can only randomly guess
whether a token belongs to the previous tokensâ€™ green list or
red list. We utilize seed ğ‘ to pseudo-randomly select the green
list ğºwith length |ğ‘‰|
2 from vocabulary ğ‘‰.
At the ğ‘–-th step, after the LLM outputs the logits vector ğ‘£ğ‘–,
we add bias term ğ›¿to the logits of all tokens in ğº. Then we
use the LLM decoding algorithm to sample the output token
ğ‘†ğ‘–from modified logits Ë†ğ‘£ğ‘–. The entire embedding procedure
of our watermarking method is described in Algorithm 1.
Design of Extraction function. Let ğ¸â€² denote the segment
sequence directly extracted from the token string ğ‘†. During ex-
traction, we first initialize the matrix COUNT counting green
tokens for each potential value of each segment as zero. For
the ğ‘–-th token, we first obtain its corresponding segmentâ€™s
8

index ğ‘through querying the token-to-segment mapping ğ‘€
using the previous token. Then for each possible value of
ğ‘-th segment ğ‘—âˆˆ[0,2 Ë†ğ‘šâˆ’1], we compute its corresponding
seed ğ‘ with secret key ğ‘ ğ‘˜, previous token ğ‘†ğ‘–âˆ’1 and value ğ‘—.
Seed ğ‘ is subsequently used for selecting the green list ğº. If
ğ‘–-th token ğ‘†ğ‘–belongs to the green list ğºof value ğ‘—, we add
1 to COUNT[ğ‘][ ğ‘—]. After we enumerate all the tokens, for
each position ğ‘, we determine the value of ğ¸â€²[ğ‘] by finding
ğ‘—which maximizes COUNT[ğ‘][ ğ‘—].
After we decided ğ¸â€², we use ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘što decode ğ¸â€² to ob-
tain ğ¾â€². The entire extraction procedure of our watermarking
method is described in Algorithm 2.
Table 1 illustrates an example of our watermark pipeline.
The example show that the embedded message can be accu-
rately extracted from the watermarked text, while the water-
marked text itself maintains good quality.
4.3
Theoretical robustness analysis
In this section, we demonstrate the robustness of our water-
marking scheme against editing attempts (insertion, deletion,
or substitution of a token). This represents the first LLM multi-
bit watermarking scheme that provides a non-trivial provable
robustness bound under edit distance [44] for every generated
sentence. Some other works [12,20,70] also studied the prov-
able robustness of LLM watermarking. However, [20, 70]
only apply to zero-bit watermarking. Concurrent work Cohen
et al. [12] proved that if a sufficient number of entropy-based
blocks remain after editing, a 1 âˆ’ğ›¿ratio of bits in the mes-
sage can be recovered. However, it is challenging to bound
how many entropy-based blocks remain after editting within
some edit distance bound. Additionally, their guarantee is less
compelling because it cannot prove perfect extraction.
The edit distance between two strings is defined as the
minimum number of operations (insertion, deletion, or sub-
stitution of a token) required to transform one string into the
other. Before presenting the formal theorem, it is essential to
first introduce one notation.
Definition 4.1. Define ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) as the probability that
exactly ğ‘–segments in the first ğ‘˜segments are extracted incor-
rectly. The condition is ğ‘¥tokens are added to the groups of the
first ğ‘˜segments, and ğ‘¦tokens are deleted from them, initial
allocated token numbers for each segment is c = (ğ‘1,Â·Â·Â· ,ğ‘ğ‘›)
and initial green token numbers for each segment is d =
(ğ‘‘1,Â·Â·Â· ,ğ‘‘ğ‘›).
We provide our main theorem as follows:
Theorem 4.1. For text paragraph ğ‘†generated by Algo-
rithm 1 before any editing. Denote embedding informa-
tion ğ¾âˆˆ{0,1}ğ‘˜ğ‘šwith token number ğ‘‡, error-correction
code (ğ‘›, ğ‘˜,ğ‘¡)2ğ‘š, allocated token number for each segment
(ğ‘1,Â·Â·Â· ,ğ‘ğ‘›) and green token number for each segment
(ğ‘‘1,Â·Â·Â· ,ğ‘‘ğ‘›). Attacker modifies ğ‘†within edit distance bud-
get ğœ‚to obtain ğ‘†â€² such that ğ·(ğ‘†,ğ‘†â€²) â‰¤ğœ‚. We have
Pr(Extraction(ğ‘†â€²) â‰ ğ¾) â‰¤1âˆ’
ğ‘¡âˆ‘ï¸
ğ‘–=0
ğ‘(ğ‘›)
ğ‘–
(2ğœ‚,2ğœ‚,c,d)
(5)
Through the theorem, we demonstrate that when the at-
tacker modifies the watermarked text within an edit distance
budget, with quite a small probability, the extracted informa-
tion will be manipulated. For example, when ğ‘‡= 200, ğœ‚= 15,
ğ‘= 20, ğ‘˜= 5, ğ‘š= 4, ğ‘›= 6, c = [30,35,35,30,35,35], d =
[25,31,31,26,32,30], the upper bound of extraction error
rate is 0.08%. Therefore, it is almost impossible for the at-
tacker to distort our watermark within this ğœ‚â‰¤15 budget. Due
to the space limitation, the proof is available in Appendix A.
We define the robust bound for a generated paragraph with
allocated token number for each segment c = (ğ‘1,Â·Â·Â· ,ğ‘ğ‘›) and
green token number for each segment d = (ğ‘‘1,Â·Â·Â· ,ğ‘‘ğ‘›) as
follows:
Definition 4.2. The robust bound ğµunder error rate ğ›¼is the
largest ğœ‚that Theorem 4.1 can guarantee the extraction error
rate does not exceed ğ›¼. Formally, we have:
ğµ= max{ğœ‚|1âˆ’
ğ‘¡âˆ‘ï¸
ğ‘–=0
ğ‘(ğ‘›)
ğ‘–
(2ğœ‚,2ğœ‚,c,d) â‰¤ğ›¼}
(6)
The robust bound ğµcan be determined by performing a
binary search to find the maximum value of ğœ‚. The algorithm
for computing the robust bound ğµfor a specific paragraph
is detailed in Algorithm 5 in Appendix C. The complexity
of Algorithm 5 is ğ‘‚(ğ‘›Â· ğœ‚4
ğ‘šğ‘ğ‘¥+ ğ‘¡Â· logğœ‚ğ‘šğ‘ğ‘¥), dominated by
the computation of the PI array. In practice, we typically use
ğ‘›â‰¤10 and ğœ‚ğ‘šğ‘ğ‘¥= 40, allowing ğµto be efficiently computed
within a few seconds on a modern CPU.
4.4
Detect machine-generated text
In real-world applications, such as content auditing, it is often
crucial to initially determine whether the text is watermarked
and subsequently extract the message embedded in the water-
marked text. It would be meaningless to extract information
from natural text that does not even carry a watermark.
Our method can also detect watermarked text from human-
written text in a manner similar to [30]. The intuition is that
our multi-bit watermarking method also injects token bias
into generated text. This allows us to leverage the statistical
difference between natural text and machine-generated text
for detection purposes.
Although the detector has no knowledge about the message
embedded in the watermark, the COUNT matrix is still com-
putable. Due to added token bias, for segment ğ‘it is quite
likely that the segment value that has the most green tokens
is the correct value. Therefore, we can approximate the total
number of green tokens in the paragraph by using the sum of
9

the maximum green token numbers for each segment. From
Figure 3 we can see that all of the blue dots are close to the red
line ğ‘¦= ğ‘¥. This indicates that the sum of max statistics we use
is very close to the true number of green tokens. Algorithm
4 in Appendix C gives a precise description of our machine
text detection algorithm.
100
125
150
175
200
Green token number
100
125
150
175
200
Sum of max
Figure 3: Green token number and sum of maximum number
of green tokens for different watermarked paragraphs. The
two statistics for machine text detection are similar showed
in the figure.
5
Experiments
In this section, we present a comprehensive set of experiments
designed to rigorously evaluate the correctness, robustness
and efficiency of our watermarking method for LLMs.
5.1
Experiment setup
Datasets. Following [29, 68] on watermarking texts gen-
erated by LLMs, we utilize the following datasets for our
evaluation: OpenGen [32], C4 news dataset [53], and Essays
dataset [59]. In particular, OpenGen is composed of 3,000
two-sentence chunks randomly sampled from the WikiText-
103 [41] validation set. The C4 news dataset consists of 15GB
news crawled from the internet. The essays dataset consists
of homework essays written by students from the IvyPanda
essay database [26]. Unless otherwise mentioned, we use the
OpenGen dataset as it is the most popular one.
Large language models. We conduct our major experi-
ments with state-of-the-art public LLMs: LLaMA-2-7B [62],
Guanaco-7B [15], and Falcon-7B [3]. By default, we use
LLaMA-2-7B [62] as it is more popular than the other two.
LLM decoding algorithms. For the LLM decoding algo-
rithm, we use greedy sampling by default. We consider Multi-
nomial sampling and Nucleus sampling [25] when studying
the impact of decoding algorithms on our method.
Metrics and evaluation methods. We use match rate to
measure the proportion of generated text that can exactly
extract embedded watermark message without error. Several
previous work [40,66â€“68] in watermarking adopted another
metric: bit accuracy (the ratio of correctly extracted message
bits). For the comparison of these 2 metrics, see Appendix F.
We use edit distance to quantify the attackerâ€™s change to the
watermarked text. Edit distance (Levenshtein distance) [44]
measures the difference between two texts defined as the
minimum number of single-token edits (insertions, deletions,
or substitutions) required to change one text into the other.
Additionally, to measure theoretical robustness, we define
the metric robust bound for a dataset as the average robust
bound for each generated text. The robust bound for each
paragraph is calculated using Algorithm 5 in Appendix C.
We set the error probability threshold in Definition 4.2 as
ğ›¼= 10âˆ’3. For text quality, following the work in [29], we use
perplexity (PPL) computed by a large oracle model GPT-3.5
(text-davinci-003) [50].
Parameter settings.
In our experiments, we adopt a de-
fault setting with bias ğ›¿= 6. We investigate token generation
for a max generation length of ğ‘‡= 200 tokens, embedding
a 20-bit message , i.e., ğ‘= 20, which we randomly sample
for each paragraph. For selecting the Reed-Solomon code in
Algorithm 1, we determine a minimum code rate ğ‘…ğ‘= 0.6,
minimum recover rate ğ‘…ğ‘Ÿ= 0.15. For the default setting of em-
bedding 20-bit message, we opt for the Reed-Solomon code
scheme with (ğ‘›, ğ‘˜,ğ‘¡)2ğ‘š= (6,4,1)25. Throughout this section,
the term â€œbit lengthâ€ refers to the length of the message that
we intend to embed into a text using Algorithm 1, i.e. ğ‘. We
conduct extensive ablation studies to evaluate the impact of
each hyperparameter on our method.
Hardware. All of our experiments are conducted on a server
with 80 Intel Xeon @2.1GHz CPUs, and 4 V100-32GB GPUs.
5.2
Main results
Our method achieves the correctness goal. To assess the
adaptability of our method under diverse settings, we carry
out experiments using three modelsâ€”Llama2, Guanaco, and
Falconâ€”and three datasets: OpenGen, C4, and Essays. We
evaluate bit lengths of 12, 16, 20, 24, 28, and 32 while keeping
other hyperparameters at their default settings. The results
are depicted in Figure 4. It is observed that the match rates
for a given bit length are relatively consistent across different
models and datasets, demonstrating the versatility of our wa-
termarking method. Notably, there is a discernible trend where
the match rate diminishes as the bit length increases. This
trend is natural as when the bit length grows, the information
redundancy available for embedding each bit becomes less.
In summary, the proposed solution has yielded remarkable
results. In all cases, we have attained a match rate exceeding
90% for all three datasets.
Our method achieves the robustness goal. We evaluate the
theoretical robustness of our method across different settings.
We conduct the experiment under the same setting as the
previous correctness experiment. The results are illustrated
in Figure 5. We find that the theoretical robustness bound for
different models and different datasets are also quite similar
under the same bit length. The results also show that, as the
bit length increases, the robust bound decreases. This result
10

12
16
20
24
28
32
Bit length
50
60
70
80
90
100
Match rate(%)
Llama2
Guanaco
Falcon
(a) OpenGen
12
16
20
24
28
32
Bit length
50
60
70
80
90
100
Match rate(%)
Llama2
Guanaco
Falcon
(b) C4
12
16
20
24
28
32
Bit length
50
60
70
80
90
100
Match rate(%)
Llama2
Guanaco
Falcon
(c) Essays
Figure 4: Match rate of our method on different datasets, LLMs, and bit lengths.
12
16
20
24
28
32
Text length
0
10
20
30
40
Robust bound
Llama2
Guanaco
Falcon
(a) Opengen
12
16
20
24
28
32
Text length
0
10
20
30
40
Robust bound
Llama2
Guanaco
Falcon
(b) C4
12
16
20
24
28
32
Bit length
0
10
20
30
40
Robust bound
Llama2
Guanaco
Falcon
(c) Essays
Figure 5: Robust bound of our method on different datasets, LLMs, and bit lengths.
0
5
10
15
20
25
30
Perplexity
Human text
Llama2-wo watermark
Guanaco-wo watermark
Falcon-wo watermark
Llama2-with watermark
Guanaco-with watermark
Falcon-with watermark
Figure 6: Our method maintains the quality of texts generated
by LLMs. The perplexity of texts generated by LLMs is simi-
lar with and without our watermark.
is not surprising, as it is rooted in theoretical analysis. When
the bit length increases, the number of ECC-coded segments
ğ‘›or the segment size ğ‘šalso increases. Consequently, the
RHS of Inequality 5 escalates with the increase of ğ‘›or ğ‘š.
When ğ‘›or ğ‘šincreases, under the same ğœ‚, ğ‘(ğ‘›)
0
defined in
Section 4.3 decreases. According to Theorem 4.1, the RHS
of Inequality 5 rises with the increase of ğ‘›or ğ‘š. Therefore,
the robust bound will diminish with the increase of bit length.
Our method maintains the quality of texts generated by
LLMs.
We also explore the influence of our watermark
on the quality of text generated by an LLM. Experiments
are conducted on OpenGen dataset using 3 different mod-
elsâ€”Llama2, Guanaco, Falcon, and different bit lengths of
12, 16, 20, 24, 28, and 32. Other hyperparameters are under
the default settings. Figure 6 uses box plots to compare the
perplexity distribution of human-written text with that of texts
generated by three different models (Llama2, Guanaco, and
Falcon), both with and without watermarking. Note that a
lower perplexity indicates higher text quality. The results in-
dicate that watermarking only slightly harms the quality of
AI-generated text, but the effect is minor. Prior to the applica-
tion of our watermark, the perplexity values associated with
text generated by Falcon predominantly fall within the inter-
val of [6,10]. Following the integration of our watermark, a
very slight elevation in perplexity occurs, with the generated
text now primarily falling within the range of [6.5,10.5].
Figure 11 in Appendix B shows that the change of embed-
ded bit length has negligible impact on text quality for all
models, as indicated by the almost flat curves. These results
suggest that our watermarking method subtly affects the qual-
ity of the generated text, ensuring the utility of watermarked
AI-generated content in practical applications.
Our method outperforms baselines [17,63,68]. To compare
our approach with baselines [12, 17, 63, 68] 2, we measure
match rate and extraction time with bit lengths set to 12, 16,
20, 24, 32 separately, while other parameters are kept at their
default values. We do not compare with [6] since they are not
open-sourced. Additionally, their method is not practical when
embedding 20-bit messages, see our analysis in Appendix E.
The experimental results for baseline comparisons are
shown in Table 2. For match rate, our method achieves results
similar to those reported in Fernandez et al. [17], especially at
larger bit lengths, where they were significantly better than the
findings of Yoo et al. [68]. In terms of extraction time, our re-
sults are very close to those of [68]. For [17,63], the extraction
time increases exponentially with the increase of bit length,
because their methods require to enumerate over 2ğ‘possible
messages. For a bit length of ğ‘= 24, our method achieves a
match rate of 96.0%, markedly surpassing the 30.4% achieved
by [68]. Additionally, our extraction process consumes less
than 1 second, in contrast to the extraction time exceeding 100
seconds as reported by [17]. For bit length ğ‘= 32, we do not
run the entire experiment for [17,63] due to their infeasible
time overhead.
2 [12] did not provide source code, we implemented their algorithm by
ourselves.
11

Table 2: Compare our method with existing baselines, on match rate and extraction time (per text paragraph).
Bit length ğ‘
12
16
20
24
32
Match rate (%)
Time (s)
Match rate (%)
Time (s)
Match rate (%)
Time (s)
Match rate (%)
Time (s)
Match rate (%)
Time (s)
Fernandez et al. [17]
99.6
0.12
99.6
0.34
99.2
5.04
98.0
110
NA
29000 (Estimated)
Wang et al. [63]
99.6
0.16
98.8
0.58
98.4
3.14
97.2
35.5
NA
8300 (Estimated)
Yoo et al. [68]
86.4
0.01
73.6
0.01
49.2
0.01
30.4
0.01
8.4
0.01
Cohen et al. [12]
93.2
0.01
88.8
0.02
78.4
0.02
65.6
0.03
27.2
0.04
Ours
98.8
0.04
98.0
0.06
97.6
0.10
96.0
0.18
94.0
0.6
12
16
20
24
28
32
Bit length
50
60
70
80
90
100
Match rate(%)
Reed-Solomon Code
BCH code
(a)
20
24
28
32
36
Bit length
75
80
85
90
95
100
Match rate(%)
w balanced-alloc
wo balanced-alloc
(b)
Figure 7: (a) compares adopting Reed-Solomon code and
BCH code. (b) compares with and without adopting balanced
segment assignment.
Impact of using other ECC schemes. We also explore the
impact of using ECC other than Reed-Solomon code in our
watermark, specifically switching to the widely adopted bi-
nary BCH code [7]. Experiments are conducted on the default
model and dataset, for bit lengths of 12, 16, 20, 24, 28, and 32,
with other hyperparameters at their default settings. Figure
7a shows the results of this experiment. We observe that us-
ing Reed-Solomon consistently results in a higher match rate
across all cases. More importantly, as the bit length increases,
the performance gap between Reed-Solomon and BCH codes
becomes significant. For example, when ğ‘= 12, using Reed-
Solomon code achieves match rate of 98.8% while using BCH
achieves 95.2%. But when ğ‘= 32, the match rate of using
Reed-Solomon code drops about five percent to 94.0% while
using BCH code drops about 35% percent to 60.0%. This
demonstrates that Reed-Solomon code is much more suitable
for our watermarking scheme than BCH code. The essential
reason is that binary BCH codes excel at correcting errors in
individual bits, while Reed-Solomon codes are more effective
at correcting burst errors. Our scheme packs multiple bits
into one segment and embeds each segment into a group of
tokens. Thus, when errors occur during decoding, they often
take the form of burst errors, where only one or two segments
are incorrect, but multiple bits within those segments may
be wrong. This makes Reed-Solomon a better choice for our
scenario compared to BCH.
Impact of adopting balanced segment assignment. To ex-
plore the effect of our designed balanced segment assignment
algorithm, we compare it with the original segment assign-
ment algorithm. Experiments are conducted on the default
model and dataset, for bit lengths of 20, 24, 28, 32 and 36,
with other hyperparameters under the default settings. Figure
7b shows the results of this experiment. We observe that using
balanced segment assignment consistently results in a higher
match rate across all cases. Furthermore, as the bit length
increases, the performance gap between adopting balanced
segment assignment and the original segment assignment be-
comes significant. For example, when ğ‘= 20, the gap is 2%.
But when ğ‘= 32, the gap raises to about 5%. This demon-
strates the effectiveness of balanced segment assignment in
improving the performance of our scheme. Making the token
assignment for different segments more balanced reduces the
probability of certain segment being allocated fewer tokens
and therefore being decoded incorrectly. This enhances the
accuracy of watermark extraction.
5.3
Impact of hyperparameters
Impact of model sizes and LLM decoding algorithms.
To further showcase the generalizability of our method, we
conduct experiments across different model sizes of Llama2
model (7B, 13B, and 70B) with other hyperparameters under
default settings. We also conduct experiments using differ-
ent LLM decoding algorithms including greedy sampling,
multinomial sampling, and nucleus sampling. For the tem-
perature in sampling, we use 0.7 following prevalent works
in LLMs [49]. The results are presented in Table 3 and Ta-
ble 4 in Appendix B. Our method yields consistently similar
performance across these varied conditions, indicating that it
possesses robust generalization capabilities.
Impact of ğ‘‡. To demonstrate the impact of text generation
length ğ‘‡on the match rate and robustness, we conduct exper-
iments with ğ‘‡set at 100, 150, 200, 250 and 300 tokens, while
all other parameters are kept at their default settings. Figure 9
in Appendix B presents the outcomes of these experiments.
As can be observed from Figure 9a, with all other hyperpa-
rameters held constant, an increase in ğ‘‡generally leads to
a higher match rate. This effect can be attributed to the fact
that a larger ğ‘‡value results in a higher average number of
tokens allocated to each segment, which reduces the error
probability on each segment since the probability that the
correct candidate has the most green tokens increases.
Additionally, Figure 9b shows that a larger ğ‘‡enhances
robustness. In accordance with the conclusion of Theorem
4.1, under the same ğœ‚, the RHS of Inequality 5 decreases with
the increase of ğ‘‡. Therefore the robust bound will increase
with the increase of ğ‘‡.
Impact of ğ›¿. To demonstrate the impact of bias ğ›¿on match
rate, robustness, and text quality (measured by perplexity),
12

Without Attack
Word Delete
Word Add
Synonym Replace
Copy-Paste
Paraphrasing
Homoglyph
0
20
40
60
80
100
Match rate (%)
Ours
Our wo ECC
Yoo et al.
Cohen et al.
Figure 8: A comparison of our watermark algorithm with Yoo et al. [68], Cohen et al. [12], and our algorithm without error-
correction code under different attacks.
we conduct experiments with ğ›¿ranging from 2 to 10, while
keeping other hyperparameters at their default settings. The
results are presented in Figure 10 in Appendix B. It can be
observed that as ğ›¿increases, both match rate and robustness
improve, but text quality deteriorates. Theoretically, when
other hyperparameters are fixed, an increase in ğ›¿increases the
total number of green tokens for each segment. This further
decreases the probability of extraction errors on each segment,
because for each segment, the probability for incorrect value
corresponding to max green tokens decreases. Thus when
ğ›¿is relatively small, increasing ğ›¿improves match rate. On
the other hand, when ğ›¿increases, the green token number
assigned to each segment d will increase by expectation. This
will increase ğ‘(ğ‘›)
0
in Theorem 4.1. Thus, the RHS of Inequal-
ity 5 decreases, thereby increasing the robust bound. However,
with the increase of ğ›¿, the introduced bias in tokens sampling
also becomes stronger and the text quality degrades, which
is consistent with the observation in [29]. In conclusion, the
change in ğ›¿brings a tradeoff between correctness, robustness,
and text quality.
5.4
Robustness results
In the real world, a user may attempt to edit the generated
text to evade the watermark or improve the utility of the text.
We investigate six types of attacks previously studied in the
literature [29,68,69]: (1) Word Deletion Attack [69] (2) Word
Addition Attack (3) Synonym Replacement Attack [69] (4)
Paraphrasing Attack [69] (5) Copy-paste Attack [68] (6) Ho-
moglyph Attack [29]. Following [68,69], we set attack types
(1) (2) (3) (5) to change 10% of the tokens. We compare
our algorithm with Yoo et al. [68], Cohen et al. [12] and our
algorithm without error-correction code under these attacks.
We do not compare with Fernandez et al. [17] and Wang et
al. [63] here due to their inefficiency as shown in Table 2.
In our paper, we consider an attacker who aims to distort
the watermark while preserving the semantics and readabil-
ity of the resulting texts. Therefore, we do not consider the
emoji attack proposed in [29], as it compromises readabil-
ity. Furthermore, the attacker employs LLMs to expedite text
generation. Consequently, we do not consider scenarios in-
volving the manual rewriting of texts for watermark removal,
as the additional human effort deviates from the attackerâ€™s
objectives of automating text generation.
Figure 8 shows the results of the attack experiments. We ob-
serve that our algorithm still persists high accuracy compared
with other baselines under different attacks. For example, un-
der the copy-paste attack, our method achieves a match rate of
90.0%, while Yoo et al. [68] only achieves 32.4%. And under
the word deletion attack, our method significantly outperforms
Yoo et al. [68] by 60%. These results clearly demonstrate that
our method achieves strong robustness under different attacks.
The major reason for our method achieving much better ro-
bustness compared to Yoo et al. [68] is our design of segment
assignment. Segment assignment reduces the probability of
extracting incorrect messages by mitigating imbalance.
The experimental results also demonstrate the importance
of our design of adopting error-correction code. For example,
under homoglyph attack, our method achieves an accuracy
of 86.4%, while not adopting ECC can only achieve 73.6%.
Adopting ECC also contributes to about 10% match rate gain
under the synonym replacement attack and word deletion
attack. This robustness advantage is even more significant
under the most challenging paraphrasing attack. Adopting
ECC improves match rate by 18%, and the match rate of our
method still achieves 80% under paraphrasing attack. The
rationale behind ECC improving robustness is as follows:
These attacks reduce the number of green tokens in each
group, leading to incorrect values being extracted from some
segments. By adopting ECC, we can tolerate errors in some
segments during extraction, which significantly increases the
difficulty of distorting our embedded message.
6
Conclusion
In this work, we proposed a watermarking method tailored
for AI-generated texts. Our method achieves four goals si-
multaneously: correctness, robustness, multi-bit capacity, and
efficiency. The effectiveness of our method lies in the design
of pseudo-random segment assignment and two improvement
techniques. Furthermore, we establish the provable robustness
of our watermarking method against adversarial edits. Em-
pirical results highlight our watermarking methodâ€™s superior
performance in both correctness and efficiency compared to
existing methods.
13

Discussion: Research Ethics
Our research focuses on enhancing the auditing of LLM gener-
ated content, preventing and tracing malicious LLM users gen-
erating content like fake news and plagiarism. We strictly fol-
low ethical guidelines, avoiding deceptive practices, unautho-
rized disclosures, live system experiments, or any actions that
could jeopardize the well-being of our team. Consequently,
our work does not introduce new ethical concerns beyond
those already associated with LLMs.
Discussion: Open Science Policy
Our study adheres to open science principles and fully sup-
ports artifact evaluation by guaranteeing the availability, func-
tionality, and reproducibility of our work. We anonymously
share our code at https://github.com/randomizedtree/
segment-watermark. We are committed to submitting our
work for artifact evaluation.
References
[1] Sahar Abdelnabi and Mario Fritz. Adversarial water-
marking transformer: Towards tracing text provenance
with data hiding. In 2021 IEEE Symposium on Security
and Privacy (SP), pages 121â€“140. IEEE, 2021.
[2] David Ifeoluwa Adelani, Haotian Mai, Fuming Fang,
Huy H Nguyen, Junichi Yamagishi, and Isao Echizen.
Generating sentiment-preserving fake online reviews
using neural language models and their human-and
machine-based detection. In Advanced Information Net-
working and Applications: Proceedings of the 34th Inter-
national Conference on Advanced Information Network-
ing and Applications (AINA-2020), pages 1341â€“1354.
Springer, 2020.
[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz
Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,
MÃ©rouane Debbah, Ã‰tienne Goffinet, Daniel Hesslow,
Julien Launay, Quentin Malartic, et al.
The falcon
series of open language models.
arXiv preprint
arXiv:2311.16867, 2023.
[4] Scott Ames, Carmit Hazay, Yuval Ishai, and Muthura-
makrishnan Venkitasubramaniam. Ligero: Lightweight
sublinear arguments without a trusted setup. In Proceed-
ings of the 2017 acm sigsac conference on computer
and communications security, pages 2087â€“2104, 2017.
[5] Mikhail J Atallah, Victor Raskin, Christian F Hempel-
mann, Mercan Karahan, Radu Sion, Umut Topkara, and
Katrina E Triezenberg. Natural language watermark-
ing and tamperproofing. In International workshop on
information hiding, pages 196â€“212. Springer, 2002.
[6] Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, and Brian
Mark. Multi-bit distortion-free watermarking for large
language models. arXiv preprint arXiv:2402.16578,
2024.
[7] Raj Chandra Bose and Dwijendra K Chaudhuri. On a
class of error correcting binary group codes. Information
and control, 3(1):68â€“79, 1960.
[8] George Casella and Roger L Berger. Statistical infer-
ence. Cengage Learning, 2021.
[9] Patrick Chao, Edgar Dobriban, and Hamed Hassani. Wa-
termarking language models with error correcting codes.
arXiv preprint arXiv:2406.10281, 2024.
[10] Miranda Christ, Sam Gunn, and Or Zamir. Undetectable
watermarks for language models. In The Thirty Seventh
Annual Conference on Learning Theory, pages 1125â€“
1139. PMLR, 2024.
[11] Jr Clark, C George, and J Bibb Cain. Error-correction
coding for digital communications. Springer Science &
Business Media, 2013.
[12] Aloni Cohen, Alexander Hoover, and Gabe Schoenbach.
Watermarking language models for many adaptive users.
Cryptology ePrint Archive, Paper 2024/759, 2024.
[13] Ingemar Cox, Matthew Miller, Jeffrey Bloom, and Chris
Honsinger. Digital watermarking. Journal of Electronic
Imaging, 11(3):414â€“414, 2002.
[14] William C Cox, Jim A Simpson, Carlo P Domizioli,
John F Muth, and Brian L Hughes. An underwater opti-
cal communication system implementing reed-solomon
channel coding. In OCEANS 2008, pages 1â€“6. IEEE,
2008.
[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. Qlora: Efficient finetuning of quan-
tized llms. Advances in Neural Information Processing
Systems, 36, 2024.
[16] Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed
Mahloujifar, Mohammad Mahmoody, and Mingyuan
Wang. Publicly detectable watermarking for language
models. arXiv preprint arXiv:2310.18491, 2023.
[17] Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien
Chappelier, and Teddy Furon. Three bricks to consol-
idate watermarks for large language models. In 2023
IEEE International Workshop on Information Forensics
and Security (WIFS), pages 1â€“6. IEEE, 2023.
[18] William A Geisel. Tutorial on reed-solomon error cor-
rection coding, volume 102162. National Aeronautics
and Space Administration, Lyndon B. Johnson Space
Center, 1990.
14

[19] Github. Copilot. https://github.com/features/
copilot, 2023. Accessed: January 12, 2024.
[20] Noah Golowich and Ankur Moitra. Edit distance ro-
bust watermarks for language models. arXiv preprint
arXiv:2406.02633, 2024.
[21] Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng,
Wanying Xie, Jie Zhou, and Dong Yu.
Token-level
adaptive training for neural machine translation. arXiv
preprint arXiv:2010.04380, 2020.
[22] Brian Hayes.
Computing science: The easiest hard
problem. American Scientist, 90(2):113â€“117, 2002.
[23] Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu,
and Chenguang Wang. Protecting intellectual property
of language generation apis with lexical watermark. In
Proceedings of the AAAI Conference on Artificial Intel-
ligence, volume 36, pages 10758â€“10766, 2022.
[24] Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu,
Fangzhao Wu, Jiwei Li, and Ruoxi Jia. Cater: Intel-
lectual property protection on text generation apis via
conditional watermarks. Advances in Neural Informa-
tion Processing Systems, 35:5431â€“5445, 2022.
[25] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. The curious case of neural text degeneration.
arXiv preprint arXiv:1904.09751, 2019.
[26] IvyPanda.
IvyPanda.
https://ivypanda.com/
essays/, 2024. Accessed: January 19, 2024.
[27] Neil F Johnson, Zoran Duric, and Sushil Jajodia. In-
formation hiding: steganography and watermarking-
attacks and countermeasures: steganography and wa-
termarking: attacks and countermeasures, volume 1.
Springer Science & Business Media, 2001.
[28] Rabimba Karanjai. Targeted phishing campaigns us-
ing large scale language models.
arXiv preprint
arXiv:2301.00665, 2022.
[29] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan
Katz, Ian Miers, and Tom Goldstein. A watermark for
large language models. In Proceedings of the 40th In-
ternational Conference on Machine Learning, pages
17061â€“17084. PMLR, 2023.
[30] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli
Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando,
Aniruddha Saha, Micah Goldblum, and Tom Goldstein.
On the reliability of watermarks for large language mod-
els, 2023. arXiv:2306.04634.
[31] Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama,
Jonathan Huang, Rachel Hornung, Hartwig Adam,
Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al.
Videopoet: A large language model for zero-shot video
generation. arXiv preprint arXiv:2312.14125, 2023.
[32] Kalpesh Krishna, Yixiao Song, Marzena Karpinska,
John Wieting, and Mohit Iyyer. Paraphrasing evades
detectors of ai-generated text, but retrieval is an effective
defense. Advances in Neural Information Processing
Systems, 36, 2024.
[33] Rohith
Kuditipudi,
John
Thickstun,
Tatsunori
Hashimoto, and Percy Liang. Robust distortion-free
watermarks for language models.
Transactions on
Machine Learning Research, 2024.
[34] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong,
Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee
Kim. Who wrote this code? watermarking for code
generation. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 4890â€“4911. Association for
Computational Linguistics, 2024.
[35] Boquan Li, Mengdi Zhang, Peixin Zhang, Jun Sun,
and Xingmei Wang. Resilient watermarking for llm-
generated codes.
arXiv preprint arXiv:2402.07518,
2024.
[36] Shu Lin and Juane Li. Fundamentals of Classical and
Modern Error-Correcting Codes. Cambridge University
Press, 2021.
[37] Aiwei Liu, Leyi Pan, Xuming Hu, Shuâ€™ang Li, Lijie Wen,
Irwin King, and Philip S Yu. An unforgeable publicly
verifiable watermark for large language models. arXiv
preprint arXiv:2307.16230, 2023.
[38] Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie
Wen. A semantic invariant robust watermark for large
language models. arXiv preprint arXiv:2310.06356,
2023.
[39] Jeffrey Lubin, Jeffrey A Bloom, and Hui Cheng. Robust
content-dependent high-fidelity watermark for tracking
in digital cinema. In Security and Watermarking of
Multimedia Contents V, volume 5020, pages 536â€“545.
SPIE, 2003.
[40] Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang,
and Peyman Milanfar. Distortion agnostic deep wa-
termarking. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition, pages
13548â€“13557, 2020.
[41] Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843, 2016.
15

[42] Stephan Mertens. The easiest hard problem: Number
partitioning. arXiv preprint cond-mat/0310317, 2003.
[43] Microsoft. New Bing. https://copilot.microsoft.
com/, 2023. Accessed: January 16, 2024.
[44] Gonzalo Navarro. A guided tour to approximate string
matching. ACM computing surveys (CSUR), 33(1):31â€“
88, 2001.
[45] Nikos Nikolaidis and Ioannis Pitas. Digital image wa-
termarking: an overview. In Proceedings IEEE Interna-
tional Conference on Multimedia Computing and Sys-
tems, volume 1, pages 1â€“6. IEEE, 1999.
[46] Aaron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
Wavenet: A generative model for raw audio.
arXiv
preprint arXiv:1609.03499, 2016.
[47] OpenAI.
ChatGPT.
https://openai.com/blog/
chatgpt, 2023. Accessed: January 10, 2024.
[48] OpenAI. Gpt-4 technical report, 2023. arXiv:2303.
08774.
[49] OpenAI.
Default temperature of OpenAI.
https:
//platform.openai.com/docs/api-reference/
making-requests, 2024. Accessed: August 31, 2024.
[50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
Training language models to follow instructions with
human feedback. Advances in Neural Information Pro-
cessing Systems, 35:27730â€“27744, 2022.
[51] Julien Piet, Chawin Sitawarin, Vivian Fang, Norman
Mu, and David Wagner. Mark my words: Analyzing and
evaluating language model watermarks. arXiv preprint
arXiv:2312.00273, 2023.
[52] Vidyasagar M Potdar, Song Han, and Elizabeth Chang.
A survey of digital image watermarking techniques. In
INDINâ€™05. 2005 3rd IEEE International Conference
on Industrial Informatics, 2005., pages 709â€“716. IEEE,
2005.
[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J Liu. Exploring the limits of transfer learn-
ing with a unified text-to-text transformer. The Journal
of Machine Learning Research, 21(1):5485â€“5551, 2020.
[54] Vinayak Ramkumar, Myna Vajha, Srinivasan Babu Bal-
aji, M Nikhil Krishnan, Birenjith Sasidharan, and P Vijay
Kumar. Codes for distributed storage. In Concise Ency-
clopedia of Coding Theory, pages 735â€“762. Chapman
and Hall/CRC, 2021.
[55] Priyanka Ranade, Aritran Piplai, Sudip Mittal, Anupam
Joshi, and Tim Finin. Generating fake cyber threat intel-
ligence using transformer-based models. In 2021 Inter-
national Joint Conference on Neural Networks (IJCNN),
pages 1â€“9, 2021.
[56] Irving S Reed and Gustave Solomon. Polynomial codes
over certain finite fields. Journal of the society for in-
dustrial and applied mathematics, 8(2):300â€“304, 1960.
[57] Jie Ren, Han Xu, Yiding Liu, Yingqian Cui, Shuaiqiang
Wang, Dawei Yin, and Jiliang Tang. A robust semantics-
based watermark for large language model against para-
phrasing. arXiv preprint arXiv:2311.08721, 2023.
[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and BjÃ¶rn Ommer. High-resolution image
synthesis with latent diffusion models, 2021. arXiv:
2112.10752.
[59] Christoph Schuhmann.
https://huggingface.
co/datasets/ChristophSchuhmann/
essays-with-instructions, 2023.
Accessed:
January 10, 2024.
[60] Richard Singleton. Maximum distance q-nary codes.
IEEE Transactions on Information Theory, 10(2):116â€“
118, 1964.
[61] Umut Topkara, Mercan Topkara, and Mikhail J Atallah.
The hiding virtues of ambiguity: quantifiably resilient
watermarking of natural language text through synonym
substitutions. In Proceedings of the 8th workshop on
Multimedia and security, pages 164â€“174, 2006.
[62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023.
[63] Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai
Lin, Fandong Meng, Jie Zhou, and Xu Sun. Towards
codable watermarking for injecting multi-bits informa-
tion to llms. In The Twelfth International Conference
on Learning Representations (ICLR) 2024.
[64] James Westall and James Martin. An introduction to
galois fields and reed-solomon coding. School of Com-
puting Clemson University Clemson, pages 29634â€“1906,
2010.
[65] Benoist Wolleb, Romain Silvestri, Giorgos Vernikos,
Ljiljana Dolamic, and Andrei Popescu-Belis. Assessing
the importance of frequency versus compositionality
for subword-based tokenization in nmt. arXiv preprint
arXiv:2306.01393, 2023.
16

[66] Xi Yang, Jie Zhang, Kejiang Chen, Weiming Zhang, Ze-
hua Ma, Feng Wang, and Nenghai Yu. Tracing text
provenance via context-aware lexical substitution. In
Proceedings of the AAAI Conference on Artificial Intel-
ligence, volume 36, pages 11613â€“11621, 2022.
[67] KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun
Kwak. Robust multi-bit natural language watermarking
through invariant features. In Proceedings of the 61st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2092â€“2115,
2023.
[68] KiYoon Yoo, Wonhyuk Ahn, and Nojun Kwak. Advanc-
ing beyond identification: Multi-bit watermark for large
language models. In Proceedings of the 2024 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 4031â€“
4055, 2024.
[69] Ruisi
Zhang, Shehzeen Samarah Hussain, Paarth
Neekhara, and Farinaz Koushanfar. REMARK-LLM: A
robust and efficient watermarking framework for gener-
ative large language models. In 33rd USENIX Security
Symposium (USENIX Security 24), pages 1813â€“1830.
USENIX Association, 2024.
[70] Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li,
and Yu-Xiang Wang. Provable robust watermarking for
AI-generated text. In The Twelfth International Confer-
ence on Learning Representations (ICLR), 2024.
A
Proof of Robustness Theorem 4.1
We provide the high level ideas of our proof and the formal
proof of our robustness theorem in this section. The proof
relies on the following assumption.
Assumption 1. Assume â„ğ‘ğ‘ â„(Â·) is a pseudo-random function,
and the attacker cannot predict â„ğ‘ğ‘ â„(ğ‘¥) given an arbitrary
input ğ‘¥.
Proof Sketch.
Consider a scenario where the attackerâ€™s
modifications to the text ğ‘†are confined within a prede-
termined edit distance. Such alterations result in bounded
changes to the COUNT matrix. We define a â€˜wrong segmentâ€™
as a segment that decodes incorrectly. By leveraging the
properties of the ECC scheme characterized by parameters
(ğ‘›, ğ‘˜,ğ‘¡)2ğ‘š, our objective is to demonstrate that the probability
of the COUNT matrix having more than ğ‘¡wrong segments
is smaller than a limited probability ğ›¼. This probability is
effectively bounded as per the right-hand side (RHS) of In-
equality 5. To compute the RHS, we first need to compute
ğ‘(ğ‘›)
ğ‘–
. We derive ğ‘(ğ‘›)
ğ‘–
by considering the cases where the ğ‘›-th
segment is extracted correctly and incorrectly. Subsequently,
ğ‘(ğ‘›)
ğ‘–
can be computed based on ğ‘(ğ‘›âˆ’1)
ğ‘–âˆ’1
and ğ‘(ğ‘›)
ğ‘–âˆ’1, forming a
recurrence relation for calculating ğ‘(ğ‘›)
ğ‘–
. The proof details are
as follows.
Proof. The following notations are crucial for our proof.
Definition A.1. Denote B(ğ‘›, ğ‘) as the binomial distribution,
where Pr(ğ‘‹= ğ‘˜|ğ‘‹âˆ¼B(ğ‘›, ğ‘)) =  ğ‘›
ğ‘˜
 ğ‘ğ‘˜(1âˆ’ğ‘)ğ‘›âˆ’ğ‘˜.
Definition A.2. Denote H(ğ‘,ğ¾,ğ‘›) as the hypergeometric
distribution, where Pr(ğ‘‹= ğ‘˜|ğ‘‹âˆ¼H(ğ‘,ğ¾,ğ‘›)) = (ğ¾
ğ‘˜)(ğ‘âˆ’ğ¾
ğ‘›âˆ’ğ‘˜)
(ğ‘
ğ‘›)
.
Definition
A.3.
Define
Bigram(ğ‘†)
=
{(ğ‘†0,ğ‘†1), (ğ‘†1,ğ‘†2),Â·Â·Â· , (ğ‘†ğ‘‡âˆ’2,ğ‘†ğ‘‡âˆ’1)}.
We claim that each operation of insertion, deletion, and
edition can remove at most two existing elements and add
at most two elements in Bigram(ğ‘†). We consider insertion,
deletion, and edition separately.
For insertion at position ğ‘–, the token ğ‘†âˆ—is inserted.
(ğ‘†ğ‘–âˆ’1,ğ‘†ğ‘–) is removed while (ğ‘†ğ‘–âˆ’1,ğ‘†âˆ—), (ğ‘†âˆ—,ğ‘†ğ‘–) are added.
For deletion at position ğ‘–, the token ğ‘†ğ‘–is deleted. (ğ‘†ğ‘–âˆ’1,ğ‘†ğ‘–),
(ğ‘†ğ‘–,ğ‘†ğ‘–+1) are removed while (ğ‘†ğ‘–âˆ’1, ğ‘†ğ‘–+1) are added. For
edition at position ğ‘–, the token ğ‘†ğ‘–is changed to ğ‘†âˆ—. (ğ‘†ğ‘–âˆ’1,ğ‘†ğ‘–),
(ğ‘†ğ‘–,ğ‘†ğ‘–+1) are removed while (ğ‘†ğ‘–âˆ’1,ğ‘†âˆ—),(ğ‘†âˆ—,ğ‘†ğ‘–+1) are added.
Therefore, after at most ğœ‚such operations, the attacker
can remove at most 2ğœ‚existing elements and add at most
2ğœ‚existing elements in Bigram(ğ‘†). This indicates that for
ğ·(ğ‘†,ğ‘†â€²) â‰¤ğœ‚, the number of tuples added ğ‘¥and the numbers
of tuples deleted ğ‘¦are both bounded by 2ğœ‚.
Using error-correction code (ğ‘›, ğ‘˜,ğ‘¡)2ğ‘š, we can correct at
most ğ‘¡incorrect segments. Based on the definition of ğ‘(ğ‘˜)
ğ‘–
, to
compute the probability of correct extraction, we can sum the
corresponding probabilities ğ‘(ğ‘›)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) for ğ‘–âˆˆ{0,1,Â·Â·Â· ,ğ‘¡}.
Based on the analysis of Bigram(ğ‘†), we have:
Pr(Extraction(ğ‘†â€²) â‰ ğ¾) â‰¤1âˆ’
ğ‘¡âˆ‘ï¸
ğ‘–=0
ğ‘(ğ‘›)
ğ‘–
(2ğœ‚,2ğœ‚,c,d)
(7)
Since we leveraged ğ‘(ğ‘˜)
ğ‘–
to derive an upper bound for the
probability of incorrect extraction, now the challenge is how
to derive and compute ğ‘(ğ‘˜)
ğ‘–
. We start from defining and ana-
lyzing the case of a single segment.
Definition A.4. Define ğ‘“(ğ‘¥, ğ‘¦,ğ‘,ğ‘‘) as the probability that a
segment is decoded correctly, given its allocated token number
ğ‘, green token number ğ‘‘, number of tokens ğ‘¥added to the
corresponding group and number of tokens ğ‘¦deleted from
the group.
Now we derive the probability expression of ğ‘“(ğ‘¥, ğ‘¦,ğ‘,ğ‘‘).
This function illustrates the probability of a single segment
decoding correctly after a certain number of token changes.
17

Editing causes some allocated tokens to be removed and some
tokens to be added.
Let ğ‘‹be the added green token number of the correct
value. For each added token, the probability that it is a green
token of the correct value is 1
2. For ğ‘¥added tokens, we have
ğ‘‹âˆ¼B(ğ‘¥, 1
2).
Let ğ‘Œbe the deleted green token number of the correct
value. For deleting ğ‘¦tokens from a set of ğ‘tokens, among
which ğ‘‘is the green token number of the correct value, we
have ğ‘Œâˆ¼H(ğ‘,ğ‘‘, ğ‘¦).
Let ğ‘‘â€² be the green token number of the correct value after
adding x tokens and deleting y tokens. We have ğ‘‘â€² = ğ‘‘+ ğ‘‹âˆ’ğ‘Œ.
ğ‘“(ğ‘¥, ğ‘¦,ğ‘,ğ‘‘) can be derived as follows:
ğ‘“(ğ‘¥, ğ‘¦,ğ‘,ğ‘‘) =
ğ‘‘+ğ‘¥
âˆ‘ï¸
ğ‘‘â€²=0
Pr(ğ‘‘+ ğ‘‹âˆ’ğ‘Œ= ğ‘‘â€²|ğ‘‹âˆ¼B(ğ‘¥, 1
2),ğ‘Œâˆ¼H(ğ‘,ğ‘‘, ğ‘¦))
Â·Pr(ğ‘‘â€² > 2ğ‘šâˆ’1 value candidates|initially ğ‘tokens,
added ğ‘¥tokens and deleted ğ‘¦tokens)
(8)
Under Assumption 1, the green token number of each
incorrect value candidate can be viewed as sampled from
B(ğ‘+ğ‘¥âˆ’ğ‘¦, 1
2).
Pr(ğ‘‘â€² > one incorrect valueâ€™s green token number|
initially ğ‘tokens, added ğ‘¥tokens and deleted ğ‘¦tokens)
= 1âˆ’Pr(ğ‘â‰¥ğ‘‘â€²|ğ‘âˆ¼B(ğ‘+ğ‘¥âˆ’ğ‘¦, 1
2))
(9)
There are in total 2ğ‘šâˆ’1 incorrect values. The green list of
these values are independent with each other. Thus, we have:
Pr(ğ‘‘â€² > 2ğ‘šâˆ’1 value candidates|initially ğ‘tokens,
added ğ‘¥tokens and deleted ğ‘¦tokens)
= (1âˆ’Pr(ğ‘â‰¥ğ‘‘â€²|ğ‘âˆ¼B(ğ‘+ğ‘¥âˆ’ğ‘¦, 1
2)))2ğ‘šâˆ’1
(10)
Substituting Equation 10 into Equation 8, we have:
ğ‘“(ğ‘¥, ğ‘¦,ğ‘,ğ‘‘) =
ğ‘‘+ğ‘¥
âˆ‘ï¸
ğ‘‘â€²=0
Pr(ğ‘‘+ ğ‘‹âˆ’ğ‘Œ= ğ‘‘â€²|ğ‘‹âˆ¼B(ğ‘¥, 1
2),ğ‘Œâˆ¼H(ğ‘,ğ‘‘, ğ‘¦))
(1âˆ’Pr(ğ‘â‰¥ğ‘‘â€²|ğ‘âˆ¼B(ğ‘+ğ‘¥âˆ’ğ‘¦, 1
2)))2ğ‘šâˆ’1
(11)
After deriving function ğ‘“illustrating the decoding probabil-
ity distribution of a single segment after editing, we consider
the distribution of the decoding correctness of multiple seg-
ments illustrated by ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d). We derive the recursive
formula to compute ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d). The probability density
function (PDF) of ğ‘(ğ‘˜)
ğ‘–
is crucial for computing our final
robust bound.
We start from the special case of ğ‘˜= 1,ğ‘–= 0. It is obvi-
ous that ğ‘(1)
0 (ğ‘¥, ğ‘¦,c,d) is the probability of the first segment
decoding to the correct value. We have:
ğ‘(1)
0 (ğ‘¥, ğ‘¦,c,d) = ğ‘“(ğ‘¥, ğ‘¦,ğ‘1,ğ‘‘1)
(12)
Additionally, ğ‘(1)
1 (ğ‘¥, ğ‘¦,c,d) represents the probability of
the complementary event of the previous event:
ğ‘(1)
1 (ğ‘¥, ğ‘¦,c,d) = 1âˆ’ğ‘(1)
0 (ğ‘¥, ğ‘¦,c,d)
(13)
Denote the number of tokens added to the ğ‘˜-th segmentâ€™s
group as ğ‘‹â€². For adding ğ‘¥tokens to the first ğ‘˜segments, we
have ğ‘‹â€² âˆ¼B(ğ‘¥, 1
ğ‘˜).
Denote the number of tokens deleted from the ğ‘˜-th seg-
mentâ€™s group as ğ‘Œâ€². For deleting ğ‘¦tokens from (ğ‘1,Â·Â·Â· ,ğ‘ğ‘›)
tokens, we have ğ‘Œâˆ¼H(Ãğ‘˜
ğ‘—=1 ğ‘ğ‘—,ğ‘ğ‘˜, ğ‘¦).
For the probability ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) that exactly ğ‘–segments
in the first ğ‘˜segments decode incorrectly, if ğ‘–â‰¥1, there are
two cases: the first case is that the ğ‘˜-th segment decodes in-
correctly and exactly ğ‘–âˆ’1 segments in the first ğ‘˜âˆ’1 segments
decode incorrectly; the second case is that the ğ‘˜-th segment
decodes correctly and exactly ğ‘–segments in the first ğ‘˜âˆ’1
segments decode incorrectly.
The probability of the first case is ğ‘(ğ‘˜âˆ’1)
ğ‘–âˆ’1
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’
ğ‘¦ğ‘˜,c,d)(1âˆ’ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜)).
The probability of the second case is ğ‘(ğ‘˜âˆ’1)
ğ‘–
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’
ğ‘¦ğ‘˜,c,d) ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜).
Therefore, for ğ‘–â‰¥1, we have the recursive formula for
ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d):
ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d)
=
ğ‘¥
âˆ‘ï¸
ğ‘¥ğ‘˜=0
Pr(ğ‘‹= ğ‘¥ğ‘˜|ğ‘‹âˆ¼B(ğ‘¥, 1
ğ‘˜))
ğ‘¦
âˆ‘ï¸
ğ‘¦ğ‘˜=0
Pr(ğ‘Œ= ğ‘¦ğ‘˜|ğ‘Œâˆ¼H(
ğ‘˜
âˆ‘ï¸
ğ‘—=1
ğ‘ğ‘—,ğ‘ğ‘˜, ğ‘¦))
(ğ‘(ğ‘˜âˆ’1)
ğ‘–âˆ’1
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’ğ‘¦ğ‘˜,c,d)(1âˆ’ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜))+
ğ‘(ğ‘˜âˆ’1)
ğ‘–
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’ğ‘¦ğ‘˜,c,d) ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜))
(14)
If ğ‘–= 0, the only possibility is all the first ğ‘˜segments decode
correctly. Similarly, we have:
ğ‘(ğ‘˜)
0
(ğ‘¥, ğ‘¦,c,d)
=
ğ‘¥
âˆ‘ï¸
ğ‘¥ğ‘˜=0
Pr(ğ‘‹= ğ‘¥ğ‘˜|ğ‘‹âˆ¼B(ğ‘¥, 1
ğ‘˜))
ğ‘¦
âˆ‘ï¸
ğ‘¦ğ‘˜=0
Pr(ğ‘Œ= ğ‘¦ğ‘˜|ğ‘Œâˆ¼H(
ğ‘˜
âˆ‘ï¸
ğ‘—=1
ğ‘ğ‘—,ğ‘ğ‘˜, ğ‘¦))
ğ‘(ğ‘˜âˆ’1)
0
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’ğ‘¦ğ‘˜,c,d) ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜)
(15)
In conclusion, we can leverage Equation 14 and Equa-
tion 15 to recursively compute every ğ‘(ğ‘˜)
ğ‘–
for ğ‘–â‰¤ğ‘¡, ğ‘˜â‰¤ğ‘›.
18

By substituting each ğ‘(ğ‘›)
ğ‘–
in Equation 7, we obtain an upper
bound of the probability of error extraction after editing the
text within an edit distance of ğœ‚.
â–¡
B
Some Experimental Results
In this section, we show some experiment results not included
in the main body of the paper due to space limitations. Fig-
ure 9 shows the match rate and robust bound of watermarked
text under different ğ‘‡. Figure 10 shows the match rate, robust
bound, perplexity of watermarked text under different ğ›¿. Fig-
ure 11 shows the perplexity of texts generated by different
LLMs across different bit lengths. Table 3 shows the match
rate and robust bound of watermarked text for LLMs with
different sizes. Table 4 shows the match rate and robust bound
of watermarked text generated by different LLM decoding
algorithms. Table 5 shows examples of watermarked text.
100
150
200
250
300
Text length
50
60
70
80
90
100
Match rate (%)
(a)
100
150
200
250
300
Text length
0
5
10
15
20
25
30
Robust bound
(b)
Figure 9: Impact of text length ğ‘‡on match rate and robust
bound.
2
3
4
5
6
7
8
9
10
0
20
40
60
80
100
Match rate(%)
(a)
2
3
4
5
6
7
8
9
10
0
5
10
15
20
25
Robust bound
(b)
2
3
4
5
6
7
8
9
10
0
5
10
15
20
25
Perplexity
(c)
Figure 10: Impact of ğ›¿on match rate, robust bound, and per-
plexity.
Table 3: Match rate and robust bound of our method for LLMs
with different sizes.
Model size
LLaMA-2-7b
LLaMA-2-13b
LLaMA-2-70b
Match rate
97.6
98.4
98.0
Robust bound
17.3
16.9
17.2
C
Pseudo-code of our procedures
Algorithm 3 illustrates our procedure of selecting the optimal
Reed-Solomon code scheme. Algorithm 4 illustrates our pro-
Table 4: Impact of LLM decoding algorithm on match rate
and robust bound.
Greedy
sampling
Multinomial
sampling
Nucleus
sampling
Match rate
97.6
98.0
97.2
Robust bound
17.3
17.5
16.8
12
16
20
24
28
32
Bit length
0
5
10
15
20
Perplexity
Llama2
Guanaco
Falcon
Figure 11: The perplexity of texts generated by LLMs with our
watermark is similar across different bit lengths and models.
cedure for detecting whether a piece of text is watermarked
by our watermark algorithm. Algorithm 5 illustrates our algo-
rithm for computing the provable bound for each watermarked
paragraph. Algorithm 6 illustrates our algorithm for comput-
ing the PI array used in Algorithm 5.
D
Real-World User Generated Text Tracing
We also evaluate our watermarking method under a setting
that is close to real-world AI-generated text auditing applica-
tions: we consider a chatbot service with 1 million different
users. There are many pieces of text of interest distributed
over the Internet, with a proportion of them generated by this
service and others by human written or by other LLM services.
The service provider wants to be able to distinguish which
pieces of text are generated by their service, and concurrently
traces the original user associated with each of these texts.
Algorithm 3 Select Reed-Solomon scheme
Input: set of Reed-Solomon scheme F ; embed bit number
ğ‘; minimal code rate ğ‘…ğ‘; minimal recovery rate ğ‘…ğ‘Ÿ
Output: Optimal Reed-Solomon scheme ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘š
Initialize candidate schemes set C as empty set
for (ğ‘›, ğ‘˜,ğ‘¡)2ğ‘šâˆˆF do
if ğ‘˜Â·ğ‘š= ğ‘, code rate ğ‘˜
ğ‘›â‰¥ğ‘…ğ‘, recover rate ğ‘¡
ğ‘›â‰¥ğ‘…ğ‘Ÿthen
C = {(ğ‘›, ğ‘˜,ğ‘¡)2ğ‘š}ÃC
end if
end for
( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘šâ†(ğ‘›, ğ‘˜,ğ‘¡)2ğ‘šâˆˆC with minimal ğ‘›
return ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘š
19

Table 5: Examples of our watermark embedding and extraction results. The content in the "Prompt" column comes from datasets
we use. The "Real Completion" column represents the ground truth completion from datasets. The "Watermarked Text" column
represents text embedded with watermark information through our watermarking algorithm. The "Orig bits" column stands for
the original message embedded in the watermarked text, whereas the "Extr bits" column represents the message extracted from
the watermarked text using our watermarking algorithm.
Prompt
Real Completion
Watermarked Text
Orig
bits
Extr
bits
... In the heavy fighting that
followed, Kokusho and around 100
of his men were killed, ending
that attack. Meanwhile, Kawaguchiâ€™s
2nd Battalion, under Major Masao
Tamura, assembled for their
planned assault against Hill 80
from the jungle south of the ridge.
Marine observers spotted Tamuraâ€™s
preparations and called in artillery
fire. At about 22:00, a barrage from
twelve guns hit Tamuraâ€™s position.
In response, two companies of Tamuraâ€™s
troops numbering about 320 men charged
up Hill 80 with fixed [...continues]
At 03:40, artillery shells landed among
Tamuraâ€™s troops, signaling Keatingâ€™s
artillery concentrations and starting
their attack off late. As the Marines
concentrated artillery fires on his men,
their commanding officer realized they
would not make the ridge [...continues]
11000
10011
01000
00111
11000
10011
01000
00111
... The ship was scheduled to leave
for Australia on Monday. The last
time there was a norovirus outbreak
on the ship was back in 2012.
According to Yahoo, health
officials conducted a series of
tests, and they confirmed that
the ill-
-ness was norovirus, but the outbreak
does seem to be going away. The
norovirus usually lasts for one
to three days, and those infected
may experience stomach pains,
vomiting, diarrhea and nausea.
Princess Cruises released a
statement saying that those
who were infected
were [...continues]
-nesses were caused by the contagious
norovirus, a highly contagious viral
gastroenteritis that can cause
diarrhea, nausea, and vomiting. They
also said that the outbreak does not
mean that the ship is unsafe and that
there are no reports of anyone with
fevers or any other [...continues]
110010
101001
111100
001101
110010
101001
111100
001101
... Retail is another major employer
in the county, with 5,280 people
employed in April 2010. Nearly $2
billion annually is spent on retail
purchases in the city. The Uptown
Meridian offers over one hundred
shopping venues, including
department stores, specialty shops,
restaurants, eateries, and United
Artists Theatres.
Phase I of the construction of Meridian
Crossroads, a shopping center in the
Bonita Lakes area, was completed in
November 2007, providing a major boost
to retail in the area. Also, the shopping
district on North Hills Street has
continued to expand, and in March 2007,
additional retail and office space was
opened [...continues]
Unprecedented investments have
been made recently to revitalize
downtown Meridian, with more
than a hundred new condominiums
and commercial developments
and $250 million invested since
2013 to build retail spaces
and renovate properties. As
of 2019, the total sales tax
revenue for the county [...continues]
00001000
11001000
11111011
01000010
00001000
11001000
11111011
01000010
... The episode was directed
by Andrew Overtoom for the
animation, and was written
by Walt Dohrn, Paul Tibbitt,
and Merriwether Williams.
Dohrn and Tibbitt also
served as storyboard directors,
and Carson Kugler, William Reiss,
and Erik Wiese worked as
storyboard artists.
It originally aired on Nickelodeon
in the United States on September
21, 2001. In this episode, SpongeBob
reads a "bad word" off a dumpster
behind the Krusty Krab, but does
not know what it means. Patrick
explains to him that it is a
"sentence enhancer" which
is used [...continues]
The main plot revolves around
Patrickâ€™s decision to give up
his nickname, "Patrick Star",
as he believes it does not accurately
reflect his personality. As a
result of this decision, he becomes
increasingly obsessed with gaining
a new nickname, leading several
of the showâ€™s characters [...continues]
00001000
11001000
11111011
01000010
00001000
11001000
11111011
01000010
Table 6: Real-world user generated text tracking
Precision
Recall
User match rate
Text from Service (%)
99.6
100
97.6
For this experiment, we assume there are 750 texts, where
250 are human-written natural text, 250 are generated by
OpenAI, and 250 are generated by their service. All of the
texts have a length of approximately 200 tokens.
As 220 = 1,048,576, we can use a 20-bit string to represent
a userâ€™s unique ID. Thus, we use our Algorithm 1 to embed
20 bit long ID of each user into the text generated by this
service. For other hyperparameters, we all follow the default
experiment settings. For detection of service generated text
we adopt Algorithm 4, and use threshold ğ‘= 8.
The experimentâ€™s results are shown in Table 6. Our detec-
tion method achieves high precision and recall on discerning
service generated text and other text. Note that here, other text
includes both human-written text and OpenAI generated text.
This shows that our detection algorithm not only can distin-
guish service generated text from human-written text, but also
can distinguish service generated text from text generated by
other LLMs.
After our detection method accurately identifies service
generated text, our watermarkâ€™s extract functionality has
strong performance. Achieving a user match rate of nearly
98% is remarkably high considering there are 1 million users
as possible candidates. Randomly guessing can only achieve
0.0001% match rate under this setting.
20

Algorithm 4 Detect zero-bit watermark
Input: Text length ğ‘‡; text sequence ğ‘†0 Â·Â·Â·ğ‘†ğ‘‡âˆ’1; vocabu-
lary set ğ‘‰; Reed-Solomon code scheme ( Ë†ğ‘›, Ë†ğ‘˜, Ë†ğ‘¡)2 Ë†ğ‘š; pseudo-
random token-to-segment mapping ğ‘€: [0, |ğ‘‰| âˆ’1] â†’
[0, Ë†ğ‘›âˆ’1]; hash function hash; secret key ğ‘ ğ‘˜; threshold
Z
Output: True or False
Initialize COUNT as Ë†ğ‘›Ã—2 Ë†ğ‘šmatrix with 0
for ğ‘–= 0,1,Â·Â·Â· ,ğ‘‡âˆ’1 do
Compute segment index ğ‘= ğ‘€[ğ‘†ğ‘–âˆ’1]
for ğ‘—= 0,1,Â·Â·Â· ,2 Ë†ğ‘šâˆ’1 âˆ’1 do
Compute random seed ğ‘ = hash(ğ‘ ğ‘˜,ğ‘†ğ‘–âˆ’1, ğ‘—)
Use seed ğ‘ to select set ğ‘„âŠ‚ğ‘‰, where |ğ‘„| = |ğ‘‰|
2
if ğ‘†ğ‘–âˆˆğ‘„then
COUNT[ğ‘][2ğ‘—] += 1
else
COUNT[ğ‘][2ğ‘—+1] += 1
end if
end for
end for
Initialize ğ‘ ğ‘¢ğ‘š= 0
for ğ‘= 0,1,Â·Â·Â· , Ë†ğ‘›âˆ’1 do
ğ‘ ğ‘¢ğ‘š+= maxğ‘—âˆˆ[0,2 Ë†ğ‘šâˆ’1] COUNT[ğ‘][ ğ‘—]
end for
Calculate z value: ğ‘§=
ğ‘ ğ‘¢ğ‘šâˆ’1
2 ğ‘‡
1
2
âˆš
ğ‘‡
return ğ‘§> ğ‘
E
Analysis of Boroujeny et al. [6]
Through our examination, [6] only works well when the bit
length of message is small. With the bit length growing, this
method has increasing difficulty in distinguishing between
neighboring messages.
Suppose the bit length of the message to embed is ğ‘. For
any neighboring message ğ‘€= ğ‘šand ğ‘€â€² = ğ‘š+1, and any bina-
rized token ğ‘–with the probability of being 1 as ğ‘ğ‘–(1), we have
ğ›¿ğ‘€= ğ‘€ğ›¿= ğ‘šÂ· 2âˆ’ğ‘and ğ›¿ğ‘€â€² = ğ‘€â€²ğ›¿= (ğ‘š+1) Â· 2âˆ’ğ‘. Without
losing generalizability, we first assume ğ‘ğ‘–(1)+2âˆ’ğ‘(ğ‘š+1) â‰¤1.
According to Equation (21) in [6], we have:
ğ´1,ğ‘–(ğ‘€) = {0 â‰¤ğ‘¦ğ‘–< ğ‘šÂ· 2âˆ’ğ‘}
Ã˜
{ ğ‘ğ‘–(1) + ğ‘šÂ· 2âˆ’ğ‘â‰¤ğ‘¦ğ‘–â‰¤1}
ğ´2,ğ‘–(ğ‘€) = {ğ‘šÂ· 2âˆ’ğ‘â‰¤ğ‘¦ğ‘–< ğ‘ğ‘–(1) + ğ‘šÂ· 2âˆ’ğ‘}
ğ´1,ğ‘–(ğ‘€â€²) = {0 â‰¤ğ‘¦ğ‘–< (ğ‘š+1) Â· 2âˆ’ğ‘}
Ã˜
{ ğ‘ğ‘–(1) + (ğ‘š+1) Â· 2âˆ’ğ‘â‰¤ğ‘¦ğ‘–â‰¤1}
ğ´2,ğ‘–(ğ‘€â€²) = {(ğ‘š+1) Â· 2âˆ’ğ‘â‰¤ğ‘¦ğ‘–< ğ‘ğ‘–(1) + (ğ‘š+1) Â· 2âˆ’ğ‘}
(16)
According to Algorithm 3 in [6], the ğ‘–-th binarized token
generated with message ğ‘€= ğ‘š(denoted as ğ‘¤ğ‘€
ğ‘–) and with
message ğ‘€â€² = ğ‘š+1 (denoted as ğ‘¤ğ‘€â€²
ğ‘–
) will be
ğ‘¤ğ‘€
ğ‘–
= ğŸ™[ğ‘¦ğ‘–âˆˆğ´2,ğ‘–(ğ‘€)]
ğ‘¤ğ‘€â€²
ğ‘–
= ğŸ™[ğ‘¦ğ‘–âˆˆğ´2,ğ‘–(ğ‘€â€²)]
(17)
Algorithm 5 Compute robust bound for watermarked text
Input: Text token number ğ‘‡; Reed-Solomon scheme
(ğ‘›, ğ‘˜,ğ‘¡); initial allocated token numbers for each segment
c; initial green token numbers for each segment d; robust
edit distance upper bound ğœ‚ğ‘šğ‘ğ‘¥; maximum error rate ğ›¼
Output: Robust bound for text: ğœ‚
ğœ‚1 = 0
ğœ‚2 = ğœ‚ğ‘šğ‘ğ‘¥
Initialize array PI[ğ‘¡+1][2ğœ‚ğ‘šğ‘ğ‘¥+1][2ğœ‚ğ‘šğ‘ğ‘¥+1]
Compute PI[ğ‘–][ğ‘¥][ğ‘¦] = ğ‘(ğ‘›)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) for ğ‘–âˆˆ[0,ğ‘¡],ğ‘¥, ğ‘¦âˆˆ
[0,2ğœ‚ğ‘šğ‘ğ‘¥] using Algorithm 6
while ğœ‚2 âˆ’ğœ‚1 > 1 do
ğœ‚ğ‘šğ‘–ğ‘‘= âŒŠğœ‚1+ğœ‚2
2
âŒ‹
Initialize ğ‘ƒğ¸as 1
for ğ‘–= 0,1,Â·Â·Â· ,ğ‘¡do
ğ‘ƒğ¸âˆ’= PI[ğ‘–][2ğœ‚ğ‘šğ‘–ğ‘‘][2ğœ‚ğ‘šğ‘–ğ‘‘]
end for
if ğ‘ƒğ¸â‰¤ğ›¼then
ğœ‚1 = ğœ‚ğ‘šğ‘–ğ‘‘
else
ğœ‚2 = ğœ‚ğ‘šğ‘–ğ‘‘
end if
end while
return ğœ‚1
To make ğ‘¤ğ‘€
ğ‘–
â‰ ğ‘¤ğ‘€â€²
ğ‘–
, we need ((ğ‘¦ğ‘–âˆˆğ´2,ğ‘–(ğ‘€)) âˆ§(ğ‘¦ğ‘–âˆ‰
ğ´2,ğ‘–(ğ‘€â€²))) âˆ¨((ğ‘¦ğ‘–âˆ‰ğ´2,ğ‘–(ğ‘€)) âˆ§(ğ‘¦ğ‘–âˆˆğ´2,ğ‘–(ğ‘€â€²))). According
to 16, only when ğ‘¦ğ‘–âˆˆ[ğ‘šÂ· 2âˆ’ğ‘, (ğ‘š+ 1) Â· 2âˆ’ğ‘) Ã[ğ‘ğ‘–(1) + ğ‘šÂ·
2âˆ’ğ‘, ğ‘ğ‘–(1) + (ğ‘š+1) Â·2âˆ’ğ‘) will ğ‘¤ğ‘€
ğ‘–
â‰ ğ‘¤ğ‘€â€²
ğ‘–
hold. As ğ‘¦ğ‘–is sam-
pled uniformly from [0,1], the probability of sampling differ-
ent binarized tokens for neighboring messages at any given
position ğ‘–is ((ğ‘š+1) Â·2âˆ’ğ‘âˆ’ğ‘šÂ·2âˆ’ğ‘) + (ğ‘ğ‘–(1) + (ğ‘š+1) Â·2âˆ’ğ‘âˆ’
ğ‘ğ‘–(1) âˆ’ğ‘šÂ·2âˆ’ğ‘) = 21âˆ’ğ‘.
For conditions where ğ‘ğ‘–(1) + 2âˆ’ğ‘(ğ‘š+ 1) > 1, it can be
readily confirmed that the probability is also 21âˆ’ğ‘.
Based on the description in [6], one normal token will be
divided into 17 binarized tokens. Therefore, the probability of
generating exactly the same ğ‘˜-tokens text under neighboring
message ğ‘€and ğ‘€â€² is (1âˆ’21âˆ’ğ‘)17Ã—ğ‘˜.
When the bit length ğ‘increases to over 20 bits for practical
usage, the probability of 2 neighboring messages generating
exactly the same long text will become extremely high. Take
bit length ğ‘= 20 and token length ğ‘˜= 200 as an example. The
probability of 2 neighboring messages generating exactly the
same 200-tokens text will become (1âˆ’2âˆ’19)17Ã—200 â‰ˆ0.9935.
Even if we already know the correct message is either ğ‘€or
ğ‘€â€², as these 2 different messages will generate exactly the
same 200 tokens text with extremely high probability, we
cannot effectively discern whether the message is ğ‘€or ğ‘€â€²
given the text no matter which method we use.
21

Algorithm 6 Compute ğ‘(ğ‘›)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) table
Input: Total segments number ğ‘›; robust edit distance up-
per bound ğœ‚ğ‘šğ‘ğ‘¥; initial allocated token numbers for each
segment c; initial green token numbers for each segment d
Output: ğ‘(ğ‘›)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) for ğ‘–âˆˆ[0,ğ‘¡] ğ‘¥, ğ‘¦âˆˆ[0,2ğœ‚ğ‘šğ‘ğ‘¥]
Initialize all ğ‘(ğ‘˜)
ğ‘–
to zero for ğ‘˜= 1,2,...,ğ‘›and ğ‘–= 0,1,...,ğ‘¡
ğ‘“â†Precomputed result table using Equation 11
for ğ‘¥= 0,1,...,2ğœ‚ğ‘šğ‘ğ‘¥do
for ğ‘¦= 0,1,...,2ğœ‚ğ‘šğ‘ğ‘¥do
ğ‘(1)
0 (ğ‘¥, ğ‘¦,c,d) = ğ‘“(ğ‘¥, ğ‘¦,ğ‘1,ğ‘‘1)
ğ‘(1)
1 (ğ‘¥, ğ‘¦,c,d) = 1âˆ’ğ‘(1)
0 (ğ‘¥, ğ‘¦,c,d)
end for
end for
for ğ‘˜= 2,...,ğ‘›do
for ğ‘¥= 0,1,...,2ğœ‚ğ‘šğ‘ğ‘¥do
for ğ‘¦= 0,1,...,2ğœ‚ğ‘šğ‘ğ‘¥do
for ğ‘¥ğ‘˜= 0,1,...,ğ‘¥do
for ğ‘¦ğ‘˜= 0,1,..., ğ‘¦do
ğ‘ğ‘¥ğ‘˜= Pr(ğ‘‹= ğ‘¥ğ‘˜|ğ‘‹âˆ¼B(ğ‘¥, 1
ğ‘˜))
ğ‘ğ‘¦ğ‘˜= Pr(ğ‘Œ= ğ‘¦ğ‘˜|ğ‘Œâˆ¼H(Ãğ‘˜
ğ‘—=1 ğ‘ğ‘—,ğ‘ğ‘˜, ğ‘¦))
ğ‘(ğ‘˜)
0
(ğ‘¥, ğ‘¦,c,d) += ğ‘ğ‘¥ğ‘˜Â· ğ‘ğ‘¦ğ‘˜Â· (ğ‘(ğ‘˜âˆ’1)
0
(ğ‘¥âˆ’
ğ‘¥ğ‘˜, ğ‘¦âˆ’ğ‘¦ğ‘˜,c,d) Â· ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜))
for ğ‘–= 1,...,ğ‘¡do
ğ‘(ğ‘˜)
ğ‘–
(ğ‘¥, ğ‘¦,c,d)
+=
ğ‘ğ‘¥ğ‘˜
Â·
ğ‘ğ‘¦ğ‘˜
Â·
(ğ‘(ğ‘˜âˆ’1)
ğ‘–âˆ’1
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’ğ‘¦ğ‘˜,c,d) Â· (1 âˆ’
ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜)) + ğ‘(ğ‘˜âˆ’1)
ğ‘–
(ğ‘¥âˆ’ğ‘¥ğ‘˜, ğ‘¦âˆ’
ğ‘¦ğ‘˜,c,d) Â· ğ‘“(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜,ğ‘ğ‘˜,ğ‘‘ğ‘˜))
end for
end for
end for
end for
end for
end for
return ğ‘(ğ‘›)
ğ‘–
(ğ‘¥, ğ‘¦,c,d) table
F
Differences between Match Rate and Bit Ac-
curacy
In our work we focus on using match rate as our evalu-
ation metric for watermark correctness. Several previous
work [40, 66â€“68] in watermarking adopted another metric,
bit accuracy (the ratio of correctly extracted bits). However,
in many cases, the gap between match rate and bit accuracy
may be large. For example, if we use Yoo et al. [68] to embed
20 bits into a 200 length paragraph, it achieves an bit accuracy
of 94.7%, while the match rate is only 49%. Consequently,
achieving high bit accuracy does not necessarily ensure a cor-
respondingly high match rate. In practical applications, where
the extracted message is critical, even minor extraction errors
can be unacceptable. Thus, using the match rate as a met-
ric provides a better reflection of performance in real-world
scenarios.
22
