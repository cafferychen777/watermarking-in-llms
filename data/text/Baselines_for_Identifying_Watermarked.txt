Baselines for Identifying Watermarked Large Language Models
Leonard Tang 1 Gavin Uberti 1 Tom Shlomi 1
Abstract
We consider the emerging problem of identifying
the presence and use of watermarking schemes
in widely used, publicly hosted, closed source
large language models (LLMs). We introduce a
suite of baseline algorithms for identifying wa-
termarks in LLMs that rely on analyzing distri-
butions of output tokens and logits generated by
watermarked and unmarked LLMs. Notably, wa-
termarked LLMs tend to produce distributions
that diverge qualitatively and identifiably from
standard models. Furthermore, we investigate the
identifiability of watermarks at varying strengths
and consider the tradeoffs of each of our identifi-
cation mechanisms with respect to watermarking
scenario. Along the way, we formalize the spe-
cific problem of identifying watermarks in LLMs,
as well as LLM watermarks and watermark de-
tection in general, providing a framework and
foundations for studying them.
1. Introduction
Recent progress in large language models (LLMs) has re-
sulted in a rapid increase in the ability of models to produce
convincingly human-like text, sparking worries that LLMs
could be used to spread disinformation, enable plagiarism,
and maliciously impersonate people. As such, researchers
have begun to develop methods to detect AI generated text.
These include watermarking algorithms, which subtly mod-
ify the outputted text to allow for better detection, given that
the detector has sufficient access to watermarking parame-
ters. Differing from previous work, where the focus is on
determining if text has been produced by a watermarked
model, here we study the problem of if a language model
has been watermarked. Critically, our black-box algorithms
only require querying the model and do not necessitate any
knowledge of underlying watermarking parameters.
1Department
of
Computer
Science,
Harvard
Uni-
versity.
Correspondence
to:
Leonard
Tang
<leonard-
tang@college.harvard.edu>.
2. Related Work
Generated Text Detection Via Statistical Discrepancies
Recent methods such as DetectGPT and GPTZero distin-
guish between machine-generated and human-written text
by analyzing their statistical discrepancies (Tian, 2023;
Mitchell et al., 2023). DetectGPT compares the log probabil-
ity computed by a model on unperturbed text and perturbed
variations, leveraging the observation that text sampled from
a LLM generally occupy negative curvature regions of the
model’s log probability function. GPTZero instead uses
perplexity and burstiness to distinguish human from ma-
chine text, with lower perplexity and burstiness indicating
a greater likelihood of machine-generated text. However,
these heuristics do not generalize and are often fallible.
Detection by Learning Classifiers
Several papers have
proposed to train classifiers to distinguish between AI and
human generated text. During the initial GPT-2 release,
OpenAI trained a RoBERTa classifier to detect GPT-2 gen-
erated text with 95% accuracy (Solaiman et al., 2019). More
recently, OpenAI fine-tuned a GPT model on a dataset of
machine-generated and human texts focusing on the same
topic, with a true positive identification rate of 26% (Ope-
nAI, 2023). Similarly, Guo et al. (2023) collected the Hu-
man ChatGPT Comparison Corpuse (HC3) and fine-tuned
RoBERTa for the detection task.
Notably, the capabilities of such classifiers decrease as
machine-generated text becomes increasingly human-like.
Sadasivan et al. (2023) show theoretically that for suffi-
ciently advanced language models, machine-generated text
detectors offer only a marginal improvement over random
classifiers. Moreover, such methods are prone to adversarial
attacks and are not robust to out-of-distribution text.
Watermarking Large Language Models
An alterna-
tive to detecting of machine-generated text using statisti-
cal discrepancies and learned classifiers is the concept of
watermarks. Watermarks are hidden patterns in machine-
generated text that are imperceptible to humans, but al-
gorithmically identifiable as synthetic. Natural language
watermarks long predate the development of LLMs, relying
on methods such as synonym substitution, as well as syn-
tactic and semantic transformations (Topkara et al., 2005).
While these schemes are capable of preserving syntactic and
arXiv:2305.18456v1  [cs.LG]  29 May 2023

Baselines for Identifying Watermarked Large Language Models
semantic meaning, they often break stylistic constraints.
More recently, Kirchenbauer et al. (2023) propose a water-
marking scheme that minimizes degradation in the quality
of generated text, while being efficient to detect in text. In
unpublished work, Aaronson (2023) introduces a conceptu-
ally similar watermarking scheme. At any given inference
step, both watermarking approaches modify the output to-
ken probabilities of the underlying model with an algorithm
using a secret key, hashing, and pseudorandom function
properties. We broadly refer to both of these watermarks
as Kirchenbauer watermarks, which we develop a subset of
our identification mechanisms against.
3. A Framework for Language Model
Watermarks and Watermark Detection
Before introducing our identification algorithms, we first
outline a framework and core terminology for the problem
of identifying watermarks in LLMs.
3.1. Large Language Models
Definition 3.1 (Vocabulary). A vocabulary in the context
of LLM watermarking is a set of tokens T along with an
encoder E and decoder D that encode and decode between
sequences of tokens and text. For the vocabulary to be valid,
we require D(E(x)) = x for any string x.
Notably, vocabulary encoders are sensitive to concatenation.
That is, it is not always the case that E(x1x2) = E(x1)E(x2)
for strings x1, x2. As an example relevant to our identifi-
cation algorithms, consider how numerals are tokenized in
Google’s Flan-T5 model. Suppose that x = "5" and
E(x) is the token with index 755. However, E(xx) is not
token 755 repeated twice – rather, it is a single token with
index 3769.
Definition 3.2 (Large Sequence Model). A large sequence
model S over a vocabulary T is a map from a finite sequence
of tokens T ∗to a set of logits over all tokens L ∈R|T |,
along with a sampler R : R|T | →∆T that randomly out-
puts a token based on the output logits.
However, this definition does not capture a characteristic of
LLM behavior that is critical for watermark identification.
In all publicly hosted LLMs, the distribution over logits is
highly uneven. Thus, we define a LLM as follows:
Definition 3.3 (Large Language Model). A large sequence
model is a large language model if an adversary, only know-
ing the training data, is able to guess the sampled token with
probability much greater than 1/|T |.
3.2. Watermarks on Large Language Models
With an understanding of relevant LLM mechanics, we now
define a LLM watermark as follows:
Definition 3.4 (Watermark). A watermark Ws with secret
key s over a vocabulary T is a map Ws : LT →LT , where
LT is the set of LLMs with vocabulary T .
Definition 3.5 (Principled Watermark). Let LA, LB be large
language models that are identical up to token permutation.
If, for any pair of such LLMs and all keys s, Ws(LA) is
identical to Ws(LB) up to the same token permutation, then
each Ws is a principled watermark.
All existing LLM watermarks that we are aware of satisfy
this definition. However, watermarks that obey this defini-
tion are not necessarily useful. For instance, the identity
watermark is a valid principled watermark, but is not al-
gorithmically detectable within a sequence of text. We
therefore introduce the following notion of watermark de-
tectability in text:
Definition 3.6 (Detectability). A watermark Ws is (p, P)-
detectable for a model L, some expression P, and p ∈
(0, 0.5], if there exists a detector Ds : T n × T n →{0, 1}
that runs in P(n) time and correctly distinguishes between
sequences of length n generated by L and Ws(L) with
probability at least 1
2 + p.
Critically, detectability of watermarks in text is different
from the notion of identifying models that have been water-
marked, the central aim of this work.
A watermark should ideally not materially change the qual-
ity of LLM text generation. While quality is somewhat
subjective, if it is impossible to distinguish watermarked
text from standard text generated by a LLM, then the water-
mark must not affect any perceivable metric of quality. We
use this observation to craft the following definition:
Definition 3.7 (Strong Quality-Preserving Watermark). Let
Ws be a watermark where s has length m and L is a LLM, p
and q are polynomials, n ≤p(m), and q(m) ≥2. Consider
a p(m)-time adversary A which takes in text and classifies
it as watermarked or benign. Ws is quality-preserving if,
for all L, m, n, A, p, and q, A is correct with probability at
most 1/2 + 1/q(m) when given texts of length n generated
by L and Ws(L), over the randomness of LLM generation
and the choice of s.
This definition is more than sufficient for a watermark to pre-
serve the quality of a text. None of the existing watermarks
discussed here satisfy this standard of quality preservation,
despite being relatively quality-preserving in practice.
For a deterministic LLM, strong quality-preservation and
detectability conflict. The only way to be strong quality-
preserving is to almost never modify the benign output, in

Baselines for Identifying Watermarked Large Language Models
which case the watermark is undetectable. The same is not
true for non-deterministic language models.
Theorem 3.8. Assuming the existence of one-way functions,
there exists a detectable watermark which is strong quality-
preserving for non-deterministic LLMs.
Proof. Let fs be a pseudorandom function, which exists as
one-way functions exist. Consider the watermark Ws from
Kirchenbauer et al. (2023) that generates a pseudorandom
number r ∈[0, 1] by applying fs to the previous tokens.
The next token is then chosen by using r to select the next
token from the LLM logits.
This is strong quality-preserving, as otherwise an adversary
that could distinguish a watermarked from unmarked lan-
guage model could be used to distinguish fs from a random
function. Since Ws is deterministic for any given seed, it
can be detected by rerunning the watermarked LLM and
observing if it returns the same output.
Such a watermark is detectable, and perfectly preserves
quality, though it fails the desideratum that watermarks
should still be detectable after the text is modified slightly.
We will not formalize this desideratum in this paper.
Though other watermarks are less sensitive to changes to
the text, all known watermarks are vulnerable to attacks
that preserve generated text quality while evading detection
(Sadasivan et al., 2023). As such, watermarkers might have
an incentive to hide their watermarking algorithms or even
the fact that they use a watermark.
Definition 3.9 (Measurable Watermark). Consider the fol-
lowing game played by an polynomial time (in |s|) distin-
guisher A who has black-box access to a language model
that is possibly watermarked. Suppose we have two gener-
ated texts from a model L and watermarked model Ws(L).
The adversary wins if it can determine which text is wa-
termarked. The watermark is measurable if there exists A
such that the probability of the adversary winning is at least
1/2 + 1/p(|s|) where p is some polynomial.
Detectability is at odds with immeasurability. The easier it
is for a detector with access to underlying watermark seed to
detect the watermark, the easier it is for a detector without
access to the seed to detect it. This conflict is provable. In
fact, for watermarks with a given detectability, there is a
single adversary that can detect all of them.
Theorem 3.10. Let R be a sequence model which always
samples uniformly from {0, 1}. Denote Rn as output text
generated from R with length n, and Ws(R)n similarly.
There exists an adversary A such that, for any water-
mark Ws and detector D : {0, 1}n →{0, 1} such that
Pr[D(Rn) = 0]/2 + Pr[D(Ws(R)n) = 1]/2 ≥
1
2 + p,
A can, with probability at least 1 −∆, distinguish R and
Ws(R) in O(n log( n
p∆log( 1
∆))).
Proof. Consider the distribution of next-token probabilities
for 0 in a text.
If the detector correctly distinguishes between positive and
negative distributions is at most 1
2 +p, we can use the bound
from Sadasivan et al. (2023) to bound the total variation
distance between Rn and W(R)n:
1
2 + p ≤1
2 + TV(Rn, W(R)n) −TV(Rn, W(R)n
2
⇒TV(Rn, W(R)n) ≥1 −
p
1 −p
Suppose that the adversary has the ability to not only sample
the generator, but also obtain its probabilities for the next
token. Consider the average variation distance from uniform
of the next token from the watermarked generator, over a
uniformly random in (0, . . . , n −1) number of uniformly
randomly generated previous tokens. By the subadditivity
of the total variation measure, the average variation distance
must be at least 1−√1−p
n
. Since it is bounded in [0, 0.5],
at least 2−2√1−p
n
of the sampled probabilities must be at
least 1−√1−p
n
. To ensure that with probability 1 −∆/2 the
adversary has sampled at least one such probability, it must
take at least m samples, with (1 −2−2√1−p
n
)m ≤∆/2, and
so
m ≤
log(∆/2)
log(1 −2−2√1−p
n
)
Since the adversary cannot sample probabilities, it must
repeatedly sample a certain token. Let k be the number
of samples it takes from each particular token, and let the
adversary classify the sample depending on whether the
proportion of ‘0’ generations differs from 1/2 by at least q.
Using the two-sided Chernoff bounds, we can get the proba-
bility of any particular sample from the uniform generator
being misclassified. We then use union bounds to get the to-
tal probability of the uniform generator being misclassified,
and use it to get bounds on k and q:
2m exp(−k((0.5 + q) log(1 + 2q)
+(0.5 −q) log(1 −2q))) ≤∆
We use a similar method to obtain the probability that a
token with variation from uniform v =
1−√1−p
n
avoids
detection:
exp(−k((0.5 + q) log(0.5 + q
0.5 + v )
+(0.5 −q) log(0.5 −q
0.5 −v ))) ≤∆/2

Baselines for Identifying Watermarked Large Language Models
To get the q which requires the fewest samples, we set these
bounds to be equal. Doing this, we get a detection algo-
rithm polynomial that takes O(n log(
n
pv∆log( 1
∆))) which
correctly classifies both the random and any watermarked
generator with probability at least 1 −∆.
The fallout of this theorem is that, when the natural dis-
tribution of a language model acting on a fixed prompt is
known, it cannot be watermarked undetectably. This forms
a theoretical foundation for our identification mechanisms.
4. Understanding Language Model Output
and Probability Distributions
A watermark can be characterized and detected by how it
affects the logits distribution of the underlying LLM. As
such, our algorithms for watermark detection are centered
heavily on analyzing shifts in language model output as well
as logit and probability distributions. Therefore, it is critical
to gain intuition for how these distributions usually behave.
Figure 1. Example of a 10,000-sample RNG distribution generated
by Alpaca-LoRA. Clearly, the distribution is far from uniform and
exhibits idiosyncratic generations resulting from the training set.
4.1. Random Bit Generation
A simple case casts LLMs as random bit generators. Ide-
ally, a LLM can generate bits uniformly at random when
prompted, and so the identification mechanism in Theorem
3.10 would apply. We attempted random bit generation with
OpenAI models. For each model, we use the following
prompt:
"""Choose two digits, and generate a
uniformly random string of those digits.
Previous digits should have no influence
on future digits:"""
This prefix was followed by a fixed sequence of 20 ‘0’s and
‘1’s, produced by a Python random number generator.
We let each model generate 100 tokens. For each token, if
‘0’ and ‘1’ were both a top-5 likely token, the probability
of generating ‘0’ was recorded. This procedure was re-
peated across multiple generations. A graph of the recorded
probabilities for each model is displayed in Figure 2.
These distributions fail tests for normality and unsurpris-
ingly, the corresponding generated bits are far from uniform.
Surprisingly, the qualitative output probability distributions
of each model are strikingly different. Ada (2a) produces
a roughly monotonically increasing distribution, babbage
(2b) produces a roughly truncated normal distribution, curie
(2c) produces a distribution with probability mass concen-
trated around 0 and 1, and davinci (2d) produces a trimodal
distribution with peaks around 0, 0.5, and 1.
4.2. Ranked Probability Lorenz Curves
Inspired by tools from econometrics, we use the Lorenz
curve as a means of understanding language model behavior.
Specifically, we examine the output token probabilities of a
model and construct ranked probability Lorenz curves. The
x-axis of a ranked probability Lorenz curve lists the tokens
sorted from lowest to highest probability, and the y-axis
of the curve displays the probabilities of each token. Due
to the sorted construction of the x-axis, the ranked token
Lorenz curve is monotonically increasing. Figure 3 displays
an example of these Lorenz curves.
The Lorenz curve is an effective tool for understanding the
effects of a watermark from Kirchenbauer et al. (2023).
Such a watermark adds a constant term δ to a randomly
selected subset of green list token logits. In the ranked token
Lorenz curve, this is notably reflected by a smoothing effect,
as seen on the right of Figure 3. This indicates that a portion
of low-probability tokens have experienced a δ-increase.
To rigorize this notion of smoothness, one can compute the
Gini coefficient G of the Lorenz curve:
G =
Pn
i=1
Pn
j=1 |xi −xj|
2n2x
Here xi, xj are the probabilities of i-th and j-th tokens on
the curve, indexed by the ordered ranking, and x is the
average probability. Traditionally in economics, G is used
to measure the inequality of a distribution. High G suggests
more inequality, reflected in unmarked distributions, while
low G suggests less inequality and a smoother distribution,
suggesting the presence of a watermark.

Baselines for Identifying Watermarked Large Language Models
(a) Ada probabilities of generating 0.
(b) Babbage probabilities of generating 0.
(c) Curie probabilities of generating 0.
(d) Davinci probabilities of generating 0.
Figure 2. Comparison of OpenAI engine behavior on a simple
random bit generation task. The x-axis displays the retrieved
probability of generating 0, and the y-displays the frequency of
each probability bucket over multiple generations.
Recovering Logits from Sampling
In practice, exact log-
its may not be available for analysis, for example when
interacting with ChatGPT. In this case, we approximate to-
ken probabilities by sampling a large number of tokens from
a language model, and calculating empirical probabilities.
4.3. Random Number Generation
In the case of a publicly hosted API, oftentimes logit data
is not directly accessible. As a suitable approximation, we
instead consider the distribution of tokens from a small
subset of the original vocabulary. This enables us to ana-
lyze the shifting behavior of a LLM before and applying a
watermark, without requiring access to output logits.
Specifically, we treat LLMs as random number generators,
asking them to generate integers from 1 to 100, inclusive.
Figure 1 displays an example 10,000-sample distribution
from Alpaca-LoRA using the following prompt:
"""Below is an instruction that
describes a task. Write a response that
appropriately completes the request.
### Instruction:
Generate a random number between
1 and 100.
### Response:"""
While this is a natural task to restrict the output token set
of a model, it is certainly not the only task that would do
so. For example, asking a LLM to provide a synonym for
a given input word, such as “intelligent”, that starts with a
specific letter, such as “c”, would also severely restrict the
output distribution to a subset resembling something like
{“clever”, “canny”, “crafty”, “calculating”, “cunning”}.
A key benefit of the random number generation task over
other alternatives, however, is that the output space for any
model is fairly consistent between models, generating inte-
gers between 1 and 100, regardless of model capacity. While
the distribution of numbers is certainly expected to change
across models, the range of outputs is relatively more stable.
5. Baseline Mechanisms for Identifying
Watermarked Large Language Models
Here, we introduce three simple watermark detection algo-
rithms based on analyzing exact and approximate probabil-
ity and logit distributions. Critically, our algorithms do not
require any access to information governing the underlying
watermark generation procedure, such as a hash function or
random number generator. We hope these algorithms can
serve as sound baselines for future work in this field.

Baselines for Identifying Watermarked Large Language Models
(a) Lorenz curve for a unmarked Flan-T5-XXL language
model. Most of the probability mass is concentrated in a few
top tokens, as visualized by the sharp spike towards the right
of the Lorenz curve.
(b) Lorenz curve for a Flan-T5-XXL model affected by a
Kirchenbauer watermark with parameters γ = 0.5 and δ = 100.
Notice that the Lorenz curve is slightly smoother under this
setting, due to the δ application on low-probability tokens.
Figure 3. Examples of ranked probability Lorenz curves of the first token generated by Flan-T5-XXL under different Kirchenbauer
watermarking strengths. The dashed line represents a perfectly uniform distribution. In both watermarking settings, the majority of the
probability mass is concentrated in the top few tokens.
Our three proposed algorithms vary in their access to exact
versus sampled logits, generalizability across watermark-
ing schemes, and statistical robustness. Depending on the
objective and identification constraints, such as efficient
computation, interpretable test statistic, availability of log-
its, and robustness to random shifts in the data, a different
algorithm will be ideal.
5.1. Measuring Divergence of RNG Distributions
The first algorithm is centered on the simple idea of measur-
ing divergence in “random” number distributions generated
by a LLM, as alluded to in §4.3. In particular, we make use
of the Two-sample Kolmogorov–Smirnov test to determine
whether an empirical random number distribution of a wa-
termarked LLM shifts from an empirical random number
distribution of an unmarked LLM.
Given a specific LLM, we first generate a 1000 number
empirical distribution Fu,n as described in §4.3 using an
unmarked model. We watermark the LLM and produce an
empirical distribution Fw,m in the same fashion. We then
compute the Kolmogorov-Smirnov statistic as follows:
Dn,m = supx |Fu,n(x) −Fw,m(x)|
Here n and m are the sizes of each sample, and n = m =
1000 specifically in our case.
The null hypothesis is that the samples are drawn from the
same distribution, i.e.:
H0: Fu,n and Fw,m are drawn from the same underlying
distribution
We reject H0 at significance level α if:
Dn,m > c(α)
q
n+m
n·m
Here c(α) =
q
−ln
  α
2

· 1
2.
5.2. Mean Adjacent Token Differences
From the Lorenz curve and Gini measure discussed in §4.2,
a natural extension is to analyze the average increase in logit
value between adjacent tokens. That is, we compute:
I =
Pn−1
i=1 ℓi+1 −ℓi
n −1
Here, ℓi is the logit at index i on the Lorenz curve, and n is
the total number of tokens in the vocabulary.
Note that for a Kirchenbauer-watermarked LLM with logit
perturbation δ and green list G with proportion γ, we have

Baselines for Identifying Watermarked Large Language Models
(a) Logit distribution produced by an unmarked Alpaca-LoRA
model. The tokens are indexed by the original tokenizer ordering.
While there is certainly a wide variation in logit values, there is
no distinct separation.
(b) Logit distribution produced by a Alpaca-LoRA model with a
γ = 0.15 and δ = 40 Kirchenbauer watermark. There is a
distinct logit separation of size δ into two bands, one for the
original logits, and one for logits perturbed by δ. The width of
the top band corresponds to γ.
Figure 4. Logit separation induced by Kirchenbauer watermarks.
an average logit increase of:
IW =
Pn−1
i=1 (ℓi+1 −ℓi)1[i ∈G]
n −1
= γ(n −1)δ + Pn−1
i=1 (ℓi+1 −ℓi)
n −1
Taking the difference with the average logit increase of an
unmarked model, IU, we have:
IW −IU = γ(n −1)δ
n −1
= γδ
Taking the above IW −IU as inspiration, a simple identifi-
cation procedure is to periodically compute I and observe
how it varies over time. Notice that IW −IU directly varies
with γ and δ; that is, the strength of the watermark directly
influences its detectability. For a strong watermark, varia-
tions in I will be obvious, while weaker watermarks will
manifest subtler differences in I.
5.3. Robustly Identifying Small-δ Watermarks
While §5.2 introduces a metric that will successfully detect
a Kirchenbauer watermark for small δ, it is sensitive to
general logit distribution perturbations introduced by other
scenarios, such as routine model updates. An identification
method robust to general distribution shifts should rely on
shift characteristics specific to a Kirchenbauer watermark.
Notably, a Kirchenbauer watermark will induce perceptible
band separations in logit space. Figure 4 demonstrates an
example of this phenomenon. Inspired by this observation,
we draw an analogue between the separation of logit values
into bands and the bimodality of logit frequencies. Under
this reframing, testing for bimodality is equivalent to testing
for the existence of a band gap.
However, though this approach is robust to other distribution
shifts, it does not yet consider small-δ perturbations. To
handle such situations, we introduce the δ-Amplification
algorithm.
Algorithm 5.1 (δ-Amplification). Suppose we have a po-
tentially watermarked LLM L. We wish to detect if it is
watermarked. We prompt L repeatedly as follows:
[Random string sampled from training
datasets]. Now write me a story:
Take the produced logits and average them across repeti-
tions. If the resulting frequency of averaged logits is bi-
modal, conclude that Ws(L) is watermarked.
To recover the underlying Kirchenbauer watermark param-
eters, we estimate δ by measuring the distance between the
peaks, and γ by measuring their respective masses.

Baselines for Identifying Watermarked Large Language Models
Table 1. Tradeoffs between proposed watermarking identification algorithms. An ideal watermarking is not specific to Kirchenbauer
and can detect general watermarks; does not require access to logits, which is common in publicly hosted models; is sensitive to
small δ and parameter values; is robust against other distribution shifts not induced by watermarks; and can be performed in a single
snapshot of time without reference to previous distributions or tests.
DETECTION METHOD
GENERAL WATERMARKS
LOGIT-FREE
δ-SENSITIVE
SHIFT-ROBUST
SINGLE-SHOT
RNG DIVERGENCE
√
√
√
×
×
MEAN ADJACENT
×
×
√
×
×
δ-AMPLIFICATION
×
×
√
√
√
Critically, as watermarks (Kirchenbauer et al., 2023; Aaron-
son, 2023) only use a fixed-size previous token window
(rumored to be 5-tokens in OpenAI models) to determine
green list indices, the green list partition across all prompts
is the same under this algorithm, as every prompt ends in a
fixed “Now write me a story:” suffix. Therefore, the output
logit distributions all experience the same δ mask.
However, the model is still influenced by earlier tokens in
the prompt, and thus exhibits differing logit values across
prompts. Intuitively then, averaging distributions across dif-
ferent prompts reduces the variation of logits, but maintains
the same effect of the δ perturbation. The averaged distri-
bution thus amplifies the effects of a small-δ watermark.
Figure 5 demonstrates an example of this effect.
We test for bimodality via the Hartigan dip test (Hartigan
& Hartigan, 1985). For a distribution with probability dis-
tribution function f, this test computes the largest absolute
difference between f and the unimodal distribution which
best approximates it.
D(f) = inf
g∈U
sup
x |f(x) −g(x)|
Here U is the set of all unimodal distributions over x. The
corresponding p-value is calculated as the probability of
achieving a Dip score at least as high as D from the nearest
unimodal distribution.
5.4. Tradeoffs Between Detection Algorithms
The algorithms proposed above are all effective in different
senses. §5.1 introduced a RNG divergence approach to
watermark detection that is not specific to Kirchenbauer
watermarks, does not require access to logits and can thus be
used directly on black-box public APIs, and is also sensitive
to small δ watermarks.
§5.2 introduced an adjacent token metric for analyzing
Kirchenbauer watermarks that is sensitive to small-δ.
Finally, we extended this approach in §5.3 to robustly han-
dle small-δ watermarks, while preserving the single-shot
criteria. Moreover, the δ-Amplification approach lent itself
nicely to statistical testing, specifically of bimodality. It is
also the only identification method that can be performed in
a single shot. That is, it does not require comparing behavior
between a watermarked and unmarked model, and equiva-
lently between a single model across multiple snapshots in
time, as is the case for the previous two algorithms.
Each method has merit depending on the specific identifica-
tion setting, but will also sacrifice certain desiderata. Table
Table 2. δ-Amplification algorithm produces the corresponding bimodality test dip and p-values for a small-δ watermarked Alpaca-LoRA
model when using prompt prefixes randomly sampled from the Pile and OpenWebText datasets. Greater diversity in prompt prefix task
and content increasingly induces bimodality and thus watermark identification. Notice that when only using Pile prompts, δ-Amplification
is only identify a watermarked model at strength δ = 7, compared to δ = 5 when using both OWT and Pile prompts.
δ
P-VALUE (OWT & PILE)
DIP (OWT & PILE)
p (PILE)
DIP (PILE)
0
0.886
0.0017
0.908
0.0016
1
1.0
0.00094
0.999
0.00097
2
0.991
0.0013
1.0
0.00092
3
0.900
0.0016
1.0
0.00093
4
0.204
0.0025
1.0
0.00093
5
0.0
0.00497
1.0
0.00093
6
0.0
0.0087
0.947
0.0015
7
0.0
0.012
0.0
0.0048
8
0.0
0.017
0.0
0.013
9
0.0
0.023
0.0
0.025
10
0.0
0.033
0.0
0.037

Baselines for Identifying Watermarked Large Language Models
(a) Distribution of logit values prior to δ-Amplification.
(b) Distribution of logit values after applying δ-Amplification,
averaging across 140 prompts.
Figure 5. Distribution of Alpaca-LoRA logit values before and after δ-Amplification for the “Now write me a story:” prompt. The x-axis
is the logit value and y-axis is the frequency. Without δ-Amplification, it is not clear whether the distribution of logit values exhibits
bimodality. Bimodality emerges only after applying δ-Amplification, enabling watermark identification.
1 summarizes these tradeoffs.
5.5. Monitoring
To detect watermarks in publicly hosted models, we set up
monitoring scripts with our identification mechanisms that
periodically query these models and compute the relevant
tests and metrics. We are most interested in identifying
watermarks in OpenAI models that are not API-accessible,
as we believe that UI-based versions of these models will be
most susceptible to dishonest usage (e.g. students cheating
will primarily use ChatGPT and not a Python script access-
ing API). As such, we also set up an agent to interact and
monitor the UI-based version of these models.
6. Results
We perform experiments on our identification mechanisms
using the Flan-T5-XXL and Alpaca-LoRA models due to
their strong instruction-following capabilities but differing
Byte-Pair Encoding and digit tokenization methods.
Table
3
displays
the
p-values
resulting
from
the
Kolmogorov-Test method. For each model and watermark
strength, the method is performed across 30 independent
instances of 1000-sample distributions generated from a
Kirchenbauer-watermarked model. Specifically, we perform
a test between each of the 30 distributions and a distribution
generated by an unmarked model. Under this procedure,
any model with distributions producing an average p-value
less than 0.05 would be considered watermarked. The p-
values are highest when comparing an unmarked distribution
against an unmarked distribution, as expected. Notably, the
p-values are extremely low for a majority of watermark
strengths for both Flan-T5-XXL and Alpaca-LoRA.
The results of the δ-Amplification method and correspond-
ing bimodality test are in Table 2. Concretely, we sample a
diverse range of prompt prefixes from Pile and OpenWeb-
Text via HuggingFace datasets and run tests on the logit
value distributions from these generations. Notably, diver-
sity in prompt prefix task and content enables uncorrelated
variance in δ, thus most effectively eliminating logit vari-
ance post-averaging.
In particular, we observe that at δ ≥5, our method produces
p-values less than 0.05, thus successfully identifying the
presence of a watermark in the model. Critically, increasing
the number of varied prefix prompts also increases identifi-
cation potency. Namely, averaging logit distributions only
across Pile prompts identifies Kirchenbauer-watermarked
models only at strength δ ≥7, while averaging across
both Pile and OpenWebText prompts identifies watermarked
models at strength δ ≥5.
As such, both algorithms serve as strong baselines for wa-
termark identification.

Baselines for Identifying Watermarked Large Language Models
Table 3. Kolmogorov-Smirnov test results on 1000-sample “RNG“
distributions from models watermarked at varying strengths. A
test is performed between 30 distributions generated from the
model in each row against a random distribution generated from
an unmarked model. The reported p-values are averaged across
these 30 samples. Note that the first row of each section, where
γ = δ = 0, is the Kolmogorov-Smirnov test result between an
unmarked model against an unmarked model.
MODEL
γ
δ
AVERAGE P-VALUE
FLAN-T5-XXL
0
0
0.80
FLAN-T5-XXL
0.1
1
3.54E-7
FLAN-T5-XXL
0.1
10
1.22E-9
FLAN-T5-XXL
0.1
50
6.75E-7
FLAN-T5-XXL
0.1
100
8.11E-8
FLAN-T5-XXL
0.25
1
0.002
FLAN-T5-XXL
0.25
10
6.47E-9
FLAN-T5-XXL
0.25
50
2.59E-7
FLAN-T5-XXL
0.25
100
1.33E-6
FLAN-T5-XXL
0.5
1
0.00024
FLAN-T5-XXL
0.5
10
0.057
FLAN-T5-XXL
0.5
50
0.054
FLAN-T5-XXL
0.5
100
0.054
FLAN-T5-XXL
0.75
1
0.42
FLAN-T5-XXL
0.75
10
0.22
FLAN-T5-XXL
0.75
50
0.34
FLAN-T5-XXL
0.75
100
0.23
ALPACA-LORA
0
0
0.63
ALPACA-LORA
0.1
1
1.40E-10
ALPACA-LORA
0.1
10
4.31E-37
ALPACA-LORA
0.1
50
4.31E-36
ALPACA-LORA
0.1
100
1.48E-35
ALPACA-LORA
0.25
1
3.10E-13
ALPACA-LORA
0.25
10
1.93E-10
ALPACA-LORA
0.25
50
4.52E-14
ALPACA-LORA
0.25
100
2.14E-11
ALPACA-LORA
0.5
1
4.17E-13
ALPACA-LORA
0.5
10
0.0089
ALPACA-LORA
0.5
50
0.066
ALPACA-LORA
0.5
100
0.06
ALPACA-LORA
0.75
1
0.00015
ALPACA-LORA
0.75
10
0.52
ALPACA-LORA
0.75
50
0.40
ALPACA-LORA
0.75
100
0.48
7. Conclusion
In this work, we develop a theoretical framework for un-
derstanding the watermark identification problem in large
language models. We then provide three black-box base-
line algorithms – measuring divergence of RNG distribu-
tions, mean adjacent token differences in logits, and δ-
Amplification – for identifying watermarks, which all fun-
damentally rely on the analysis of the distributions of model
outputs, logits, and probabilities. Each algorithm trades off
in different practical aspects, including identification gen-
eralizability, logit-free analysis, sensitivity to watermarks,
robustness against general distribution shifts, and single-
shot testing. Ultimately, we monitor publicly hosted models
in an attempt to detect watermarks. Since we are the first
to consider the problem of identifying watermarks in large
language models, we hope that our framework and baselines
serve as strong foundations for future work in this direction
from the community.
References
Aaronson, S. My AI safety projects at OpenAI. Talk given
to the Harvard AI Safety Team, March 2023.
Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding,
Y., Yue, J., and Wu, Y. How close is chatgpt to human
experts? comparison corpus, evaluation, and detection.
arXiv preprint arXiv:2301.07597, 2023.
Hartigan, J. A. and Hartigan, P. M. The dip test of unimodal-
ity. The annals of Statistics, pp. 70–84, 1985.
Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I.,
and Goldstein, T. A watermark for large language models.
arXiv preprint arXiv:2301.10226, 2023.
Langley, P. Crafting papers on machine learning. In Langley,
P. (ed.), Proceedings of the 17th International Conference
on Machine Learning (ICML 2000), pp. 1207–1216, Stan-
ford, CA, 2000. Morgan Kaufmann.
Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., and
Finn, C. Detectgpt: Zero-shot machine-generated text
detection using probability curvature.
arXiv preprint
arXiv:2301.11305, 2023.
OpenAI.
New AI classifier for indicating AI-written
text.
https://openai.com/blog/new-ai-
classifier-for-indicating-ai-written-
text, 2023.
Sadasivan, V. S., Kumar, A., Balasubramanian, S., Wang,
W., and Feizi, S. Can ai-generated text be reliably de-
tected? arXiv preprint arXiv:2303.11156, 2023.
Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-
Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,
Kreps, S., et al. Release strategies and the social impacts
of language models. arXiv preprint arXiv:1908.09203,
2019.
Tian, E. GPTZero. https://gptzero.me/, 2023.
Topkara, M., Taskiran, C. M., and Delp III, E. J. Natural
language watermarking. In Security, Steganography, and
Watermarking of Multimedia Contents VII, volume 5681,
pp. 441–452. SPIE, 2005.
