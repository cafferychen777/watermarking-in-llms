Multi-Bit Distortion-Free Watermarking for Large Language Models
Massieh Kordi Boroujeny 1 Ya Jiang 2 Kai Zeng 1 Brian Mark 1
Abstract
Methods for watermarking large language models
have been proposed that distinguish AI-generated
text from human-generated text by slightly alter-
ing the model output distribution, but they also dis-
tort the quality of the text, exposing the watermark
to adversarial detection. More recently, distortion-
free watermarking methods were proposed that
require a secret key to detect the watermark. The
prior methods generally embed zero-bit water-
marks that do not provide additional information
beyond tagging a text as being AI-generated. We
extend an existing zero-bit distortion-free water-
marking method by embedding multiple bits of
meta-information as part of the watermark. We
also develop a computationally efficient decoder
that extracts the embedded information from the
watermark with low bit error rate.
1. Introduction
The emergence of large language models (LLMs) has ush-
ered in the dawn of general artificial intelligence (Dong et al.,
2023; Tamkin et al., 2021; Brown et al., 2020; Touvron et al.,
2023; Zhang et al., 2022; Chowdhery et al., 2022; Ye et al.,
2023) due to their remarkable capabilities in understanding
and generating human-like text. LLMs can be fine-tuned
for a wide range of natural language processing tasks, such
as text summarization, translation, question-answering, and
more. Their versatility allows for the development of more
generalized AI systems (Wu et al., 2023b; Ge et al., 2023;
Mialon et al., 2023; Li & Liang, 2021). The natural lan-
guage generation capabilities of these models contribute to
more natural and human-like interactions between machines
and humans, making AI systems more accessible and user-
friendly (Chan et al., 2023; Shen et al., 2023; Wu et al.,
2023a; Vemprala et al., 2023).
Unfortunately, along with these attractive features, LLMs
1Department of Electrical and Computer Engineering, George
Mason University, Fairfax, VA, USA 2Department of Computer
Science, George Mason University, Fairfax, VA, USA. Correspon-
dence to: Massieh Kordi Boroujeny <mkordibo@gmu.edu>.
.
also create opportunities for malicious use, such as mis-
information spreading, inappropriate content generation,
unethical activities engagements, academic cheating, etc.
(Yang & Menczer, 2023; Lapid et al., 2023; Bommasani
et al., 2021; Goldstein et al., 2023). To bolster AI account-
ability, it is crucial to be able to ascertain the provenance of
a given text, i.e., whether the text was generated by an LLM
or crafted by a human, and if the text was generated by an
LLM, which LLM was used.
Post-hoc detectors, which were proposed as an initial ap-
proach to address this concern, are based on the use of statis-
tical outliers (Lavergne et al., 2008; Beresneva, 2016; Tian,
2023; Mitchell et al., 2023) or training a binary classifier
over the human-generated and LLM-generated texts (Ope-
nAI, 2023; Bakhtin et al., 2019; Fagni et al., 2021). How-
ever, these methods tend to be rendered ineffective as LLM-
generated texts have become increasingly similar to the
human-generated texts with advances in LLMs.
Another approach to AI accountability is to embed a wa-
termark in a generated text. Watermarks are embedded by
means of intentional and hopefully imperceptible modifica-
tions to an LLM. Recently, Kirchenbauer et al. (Kirchen-
bauer et al., 2023a) presented the first watermarking scheme
for LLMs. However, in their watermarking scheme the dis-
tribution of the generated watermarked text deviates from
that of the original LLM distribution, which results in dis-
tortion of the text quality.
A good watermarking scheme should satisfy the following:
I. Distortion-free: The watermarked text should have the
same output distribution as the original LLM.
II. Low probability of false alarm: Human-generated text
should be detected as AI-generated with negligible
probability.
III. High probability of correct detection: AI-generated
text should be detected as such with high probability.
To achieve these desirable properties,
watermarking
schemes were further developed in (Aaronson, 2023; Christ
et al., 2023; Kuditipudi et al., 2023). (Christ et al., 2023)
proposed a watermarking scheme that claims to have prop-
erties I–III. Their method relies on binarization of the token
1
arXiv:2402.16578v1  [cs.CL]  26 Feb 2024

Multi-Bit Distortion-Free Watermarking for LLMs
Table 1. Comparison between other LLM watermarking schemes
and this work.
A: (Kirchenbauer et al., 2023a), (Kirchenbauer et al., 2023b),(Liu
et al., 2023); B: (Christ et al., 2023),(Aaronson, 2023),(Kuditipudi
et al., 2023); C: (Wang et al., 2023), (Abdelnabi & Fritz, 2021),
(Yoo et al., 2023), (Fernandez et al., 2023).
A
B
C
This work
high detection rate
✓
✓
✓
✓
low false alarm
✓
✓
✓
✓
distortion-free
✗
✓
✗
✓
multi-bit watermark
✗
✗
✓
✓
low BER
✗
✗
✗
✓
efficient decoding
✗
✗
✗
✓
set and token generation based on the value of a pseudoran-
dom function (PRF). (Aaronson, 2023) proposed a similar
method, which satisfies the above three properties but does
not involve binarization.
The above-mentioned references are considered zero-bit wa-
termarking schemes in the sense that the watermark does
not embed any information beyond differentiating an AI-
generated text from a human-generated one. In practice, it
is crucial to encode meta-information such as the language
model name, model version, and generation time within
the watermark. For example, encoding meta-information
in the watermark supports forensic analysis in case of mis-
use. It helps trace back the origin of content and assists
in determining whether a specific model or version was in-
volved. Moreover, the incorporation of meta-information
in the watermark aligns with the need for responsible and
accountable AI practices.
Therefore, we extend the above list with the following addi-
tional properties:
IV. Multi-bit embedding: The watermark encodes multiple
bits of meta-information.
V. High probability of correct decoding: The bit error
rate (BER) in decoding the embedded information bits
should be low.
VI. Efficient decoding: The decoding algorithm should be
efficient and not require exhaustive search over all the
possible embedded information bits.
In this paper, we develop the first efficient multi-bit
distortion-free watermarking scheme that satisfies all of
the above properties I–VI. Table 1 compares our proposed
scheme with the state-of-the-art approaches.
The remainder of the paper is organized as follows. In Sec-
tion 2, we show how distortion-free watermarking schemes
can be represented by a distortion-free mapping rule. In Sec-
tion 3, we apply this generalization to the scheme of (Christ
et al., 2023) and correct a flaw in their design of the decoder.
In Section 4, we extend the distortion-free mapping rule to
a multi-bit distortion-free mapping rule and then develop a
new multi-bit distortion-free watermarking algorithm. Sec-
tion 5 presents numerical results from our simulation study.
Concluding remarks are given in Section 6.
2. Preliminaries
Random variables are denoted by boldfaced letters, e.g., w,
whereas, specific realizations of a random variable are de-
noted by non-boldfaced letters, e.g., w. In this case, w = w
means that the random variable w gets value w. For brevity,
in conditional probability expressions, we abbreviate expres-
sions such as w = w by simply w. We use uppercase letters
to denote vectors and calligraphic font to denote sets. We
use X[i:j] := (xi, xi+1, . . . , xj) and X[n] := (x1, . . . , xn)
to denote subsequences of a sequence {xn}.
Definition 2.1 (Language Model). A vocabulary V =
{v1, v2, . . . , v|V|} is a set of tokens. Given a generated
sequence of tokens W[t−1] ∈Vt−1 and a prompt α =
W[−(Np−1):0] ∈VNp, where Np is the length of the prompt,
a Language Model M is specified by a conditional distribu-
tion
pt,i = pM(vi | W[t−1], α)
:= PM

wt = vi | W [t−1] = W[t−1], α
	
,
(1)
where i = 1, . . . , |V| for the tth token wt ∈V. We use
Dt = (pt,1, pt,2, . . . , pt,|V|) to denote this conditional prob-
ability distribution over the set V and define Dt[vj] = pt,j,
for j = 1, . . . , |V|.
Note that wt is a nonstationary discrete-time random pro-
cess with conditional probability distribution given by (1).
The response of a language model M to a prompt α is a ran-
dom vector M(α) := W [L], where L is a random variable,
with conditional distribution given by
P

M(α) = W[ℓ]
	
=
ℓ
Y
t=1
pM(wt | W[t−1], α)
= pM(W[ℓ−1] | α) · pM(wℓ| W[ℓ−1], α), ℓ= 1, 2, . . . ,
(2)
where pM(W[0] | α) = 1, and pM(· | W[0], α) = pM(· | α).
This random response is generated by sampling from Dt
until a special terminating token done ∈V is generated.
Therefore, M(α) = W[L] implies that w[L] = done. When
we talk about altering a language model, we mean Dt →D′
t,
where the prompt α and the sequence generated tokens so
far, i.e., W[t−1], are fixed. The entropy of the response of a
language model M to a prompt α is defined as
H(α) := EM(α){−ln P{M(α)}}
(3)
2

Multi-Bit Distortion-Free Watermarking for LLMs
For further discussion of H(α) see Appendix A.1.
Having established a language model definition, we now
define a distortion-free watermarking algorithm.
Definition 2.2. A watermarking algorithm is distortion-
free if for any prompt α and a sequence of watermarked
generated text W[ℓ], we have
P{wt = wt | W [t−1] = W[t−1], α}
= pM(wt | W[t−1]; α) = Dt[wt],
for all t ∈{1, . . . , ℓ}.
In other words, a watermarking algorithm is distortion-free
if the watermarked text has the same distribution Dt as the
non-watermarked text.
Following (Christ et al., 2023), our proposed watermarking
algorithms use pseudorandom functions (PRFs) to generate
random numbers. PRFs are defined as follows,
Definition 2.3 (PseudoRandom Functions (PRF)). Let
F = {Fsk : {0, 1}l1(λ) →{0, 1}l2(λ)|sk ∈{0, 1}λ} be a
family of functions. F is a PRF if Fsk is efficiently com-
putable and for all probabilistic polynomial-time distinguish-
ers D,
Psk←{0,1}λ{DFsk(·)(1λ) = 1} −Pf{Df(·)(1λ) = 1}

≤negl(λ),
where f : {0, 1}l1(λ) →{0, 1}l2(λ) denotes a random func-
tion. A function g(λ) is negligible, denoted by negl(λ), if
g(λ) ∈O

1
poly(λ)

for every poly(·).
The basic approach to distortion-free watermarking is to
embed the watermark in the correlation between {wt}ℓ
t=1
and a specially crafted i.i.d. sequence {yt}ℓ
t=1 (Kuditipudi
et al., 2023; Christ et al., 2023; Aaronson, 2023), where ℓ
is the length of the generated text. In (Christ et al., 2023;
Kirchenbauer et al., 2023a), yt ∼Uniform[0, 1]. In the
watermarking process, the token wt is generated such that it
has high correlation with yt, for t = 1, . . . , ℓ. On the other
hand, for the human-generated text or non-watermarked text,
the token wt is independent of yt, for t = 1, . . . , ℓ. For
a given sequence of tokens W = W[ℓ] and a sequence of
generated random numbers Y = Y[ℓ], the detection algo-
rithm is a statistical test ψ(W, Y ) that checks whether the
correlation between the two sequences exceeds a threshold.
In the watermarking process, the correlation between {wt}
and {yt} is introduced using a watermarking mapping rule.
Definition 2.4 (Watermarking mapping rule). For a ran-
dom variable y defined over the sample space Ωwith distri-
bution Py, a watermarking mapping rule Γ(Ω, V; Py) is a
process of mapping a partition of a sample space Ωinto the
Ai
Aj
Ak
Ai
Ω
V
vi
vj
vk
Figure 1. Watermarking mapping rule Γ(Ω, V).
token set V of the language model M. This process is done
in two steps: First, a partition U = {A1, . . . , A|V|} on the
sample space Ωis formed. Then each partition part Aj is
mapped to vj, for j = 1, . . . , |V| (see Appendix A.2).
The distortion-free property of a watermarking algorithm
following a watermarking mapping rule Γt(Ω, V; Py) is
characterized as follows.
Proposition 2.5. A watermarking algorithm following a
watermarking mapping rule Γt(Ω, V; Py) is distortion-free
if and only if for every prompt α and the past generated
tokens W[t−1],
P{yt ∈Aj,t} = pM(vj | W[t−1], α) = Dt[vj].
(4)
3. Zero-bit Distortion-free Watermarking
In this section, we review the zero-bit distortion-free water-
marking scheme of (Christ et al., 2023) and address a prob-
lematic issue with the statistical test used in their scheme.
We then propose a modification to the random initialization
in this method and develop an alternative zero-bit distortion-
free watermarking algorithm, which we extend further to a
multi-bit algorithm in Section 4.
3.1. Binarization of language model
Watermarking in (Christ et al., 2023) follows a watermark-
ing mapping rule and is done on binary tokens. Language
model M with token set V is converted into a language
model Mb with binary token set Vb = {0, 1}. This con-
version is done as follows: First, each token v ∈V is
represented as a distinct binary string in {0, 1}log |V|, where
log(·) denotes the base-2 logarithm. In this way, sampling
log |V| times from the language model Mb is equivalent
to sampling one token from language model M. Hence-
forth, we will assume binarization has been applied (see
Appendix B.1 for further details).
3.2. Watermarking without random initialization
The watermarking algorithm decides the value for each
binary token according to a watermarking mapping rule
3

Multi-Bit Distortion-Free Watermarking for LLMs
0
1
pi(1)
0 ≤yi ≤pi(1)
pi(1) ≤yi ≤1
0
1
Vb
Figure 2. Watermarking mapping rule in (Christ et al., 2023).
as shown in Figure 2.
Given a prompt αb and the past
generated tokens W b
[i−1], the watermarking mapping rule
Γi(Ω, Vb; Py) is specified as follows:
Ω= [0, 1],
yi ∼Uniform[0, 1],
Γi(Ω) = {A1,i = [pi(1), 1], A2,i = [0, pi(1))},
Γi(A1,i) = 0,
Γi(A2,i) = 1,
(5)
where pi(1) is the probability of the i-th token being 1
according to Mb. According to (5),
P{yi ∈A1,i} = pi(0), P{yi ∈A2,i} = pi(1).
By Proposition 2.5, a watermarking scheme following
this watermarking mapping rule is distortion-free. The
random variable Yi can be generated using a PRF Fsk :
{0, 1}poly1(λ) →{0, 1}poly2(λ), with a secret key sk ∈
{0, 1}λ, shared between the encoder and the decoder. Here,
λ is the security parameter of the watermarking algorithm
(see (Christ et al., 2023)).
Unlike (Christ et al., 2023), we will not use the index i as
the the input to this PRF, as this choice of input will make
the watermark very prone to a simple deletion attack, and
removing one word in a watermarked text can render the
detection process useless. Instead we use the context ngram
Si,h = W b
[i−h:i−1] as the input to the PRF for the i-th token.
Usually h is chosen such that ⌊h/ log |V|⌋≤8. In our
work, we set ⌊h/ log |V|⌋= 5. Here, poly1 is chosen such
that the ngram Si,h is not too long for the PRF, and if it
is too short it will be padded. On the other hand, let z be
the integer representation of the output of PRF; then taking
z
2poly2(λ) results in a real number in [0, 1]. In our notation,
we assume these steps are included and Fsk(i) ∈[0, 1].
The watermarking encoding scheme can be summarized as
follows:
yi = Fsk(Si,h), wb
i =
 1,
0 ≤yi < pi(1),
0,
pi(1) ≤yi ≤1.
(6)
For W b = W b
[ℓ] and Y = Y[ℓ], the decoder uses a statistical
test ψ(W b, Y ) to check whether the correlation between Y
and W b exceeds a certain threshold. The null hypothesis is
H0 : text is non-watermarked while the alternative hypothe-
sis is H1 : text is watermarked. First, for each binary token
a score value is calculated. This score value depends on the
binary token value, wb, and the uniform random number
generated, y. Define C(W b, Y ) as the sum of the score
values for all (wb
i, yi), i = 1, . . . , ℓ:
C(W b, Y ) :=
ℓ
X
i=1
s(wb
i, yi).
(7)
The p-value for the observed value z of a random variable
z is defined as the probability of observing a value at least
as extreme as the observed value z under H0, i.e.,
p-value(z) = P{z > z | H0}.
(8)
The p-value for C(W b, Y ) is compared to a threshold FPR,
where FPR is the maximum tolerable false positive rate
for detecting a non-watermarked text as watermarked. If
p-value(C(W b, Y )) ≤FPR, the text is detected as water-
marked (cf. Algorithm 4 in (Christ et al., 2023)). This
leads to a critical region Dc = {C(W b, Y ) ≥θ}, such that
H0 is rejected if C(W b, Y ) ∈Dc, and a region of accep-
tance Dc
c = {C(W b, Y ) < θ}, where H0 is accepted if
C(W b, Y ) ∈Dc
c. In other words, for a non-watermarked
text W b
NW and the constructed Y, the threshold θ is cho-
sen such that, P{C(W b
NW, Y) ≥θ} ≤FPR, for all W b
NW
with length ℓ. This choice of threshold θ, on the other
hand, affects the false negative rate. If in addition to FPR,
there is also a maximum tolerable false negative rate FNR,
this bound will yield a minimum length of watermarked
text such that both false positive rate and false negative
rate are bounded by FNR and FPR, respectively (see Ap-
pendix B.2.1).
As in (Christ et al., 2023), the score function is calculated
as follows,
s(wb
i, yi) =
 ln 1
yi ,
wb
i = 1,
ln
1
1−yi ,
wb
i = 0.
(9)
This score function is designed such that, given a prompt αb
and the past generated tokens W b
[i−1], the expected value of
the score function for the i-th token, if it is watermarked, is
greater than its expected value if it is non-watermarked (see
Appendix B.2.2).
Using the central limit theorem (CLT), we can approximate
the distribution of C(W b
W, Y ) (see Appendix B.2.3). There
can be correlation between the s(wb
i, yi)’s. This correlation
can stem from the correlation between the tokens generated
by an LLM or from similar ngrams with length h appearing
throughout the text. Not much can be done for the first cause
of correlation, however, by adopting the idea from (Fernan-
dez et al., 2023), in the detection process we can eliminate
the second cause. Namely, we consider any ngram “context
plus current token,” i.e., wb
[i−h:i], only once. In other words,
if for different values of i, the corresponding context plus
4

Multi-Bit Distortion-Free Watermarking for LLMs
current token is repeated before, we will not include its cor-
responding score value in C(W b, Y ). Therefore, to simplify
our derivation, in applying the CLT we shall assume that
the s(wb
i, yi)’s are independent. Our experimental studies
have shown that this assumption does not adversely affect
the detection algorithm.
The threshold θ and the critical region Dc are chosen such
that for a non-watermarked text W b
NW and the constructed
Y , P{C(W b
NW, Y) > θ} ≤FPR. Using (59), for a given L
and FPR, we can derive θ as follows,
θ = ¯F −1
EL(1)(FPR) = Q−1(L, FPR),
(10)
where ¯FErL(1)(x) is the tail distribution function of a ran-
dom variable x ∼ErL(1) and ErL(λ) denotes the Erlang
distribution with shape parameter k and rate λ:
¯FErL(1)(x) = P{x > x} = Γ(L, x)
Γ(L)
= Q(L, x),
(11)
where Γ(L, x) is the upper incomplete gamma function
and Q(L, x) is the regularized gamma function (see Ap-
pendix B.2.3).
We have derived an approximation for Lmin (see Ap-
pendix B.2.4):
Lmin ≈f 2
1 (FPR, FNR)
ζb(αb)2
(12)
where
f1(FPR, FNR) =
r
2 ln
1
2FPR +
r
4.4 ln
1
2FNR
(13)
The approximation in (12) is similar to the lower bound
approximation in (Aaronson, 2023), i.e., O(
1
ζ(α)2 ln 1
η) with
FPR=FNR= η. Using (38), we can express Lmin in (12) in
terms of the average conditional entropy, conditioned on the
past tokens, per token of response of the language model M
to the prompt α, i.e., ζ(α) as
log |V|f 2
1 (FPR, FNR)
ζ(α)2
≈Lmin
log |V|.
(14)
Based on (14), the number of required tokens to achieve a
desired probability of error for a watermarking scheme, on
a binarized language model Mb, is log |V| times than what
it would have been under M. The exact and approximate
values of Lmin derived by the numerical method mentioned
here and by using (12), for different values of ζ(αb) for
vocabulary size |V| = 50272, are shown in Figure 3, where
the approximate values are depicted by dashed lines. In
Figure 3, the bounds on false positive rate and false nega-
tive rate are considered equal. As the average conditional
entropy per token for the generated text increases, fewer
tokens are required to achieve the same false negative and
positive rates.
Figure 3. Exact and approximate (dashed lines) Lmin, |V| =
50272.
3.3. Watermarking with random initialization
(Christ et al., 2023) proposed to initiate the watermarked
text with a chunk of tokens R that is randomly sampled
from language model Mb. The empirical entropy for R is
defined as
He(Mb, αb, R) =
n
X
i=1
−ln pi(wb
i).
(15)
After sampling tokens from language model Mb until
He(Mb, αb, R) exceeds a threshold λ for the sampled
set of tokens R = wb
[m], the watermarking encoding
starts. The sampled binary tokens in R and the context
ngram Si,h = W b
[i−h:i−1] are used as the input to a PRF
Fsk : {0, 1}poly1(λ) →{0, 1}poly2(λ), with a secret key
sk ∈{0, 1}λ, to generate a random variable Yi. Following
the same steps as previous section, the watermarking encod-
ing can be summarized using (6), with yi = Fsk(R, Si,h).
Therefore, following this procedure for a prompt αb, a se-
quence of uniform random numbers, Y [n+1:L] and a wa-
termarked text (R, W b
[n+1:L]) will be generated, where the
initial chunk of tokens, R, is sampled from the language
model Mb and the rest of the tokens are decided based on the
watermarking encoding rule in (6) with yi = Fsk(R, Si,h).
We then obtain the encoder in Algorithm 1.
The initial random chunk R, including its length, is ran-
dom but does not depend on the output of the PRF. If the
accumulated empirical entropy never surpasses λ, then the
text generated by Algorithm 1 will contain no watermark.
Given a prompt αb and a fixed initial chunk R, the condi-
tional distribution for response of a language model Mb,
can be derived from (2), with M and α replaced by Mb and
(αb, R), respectively. In other words, (αb, R) is treated as
the prompt (see Appendix B.3.1).
5

Multi-Bit Distortion-Free Watermarking for LLMs
Algorithm 1 Zero-bit encoder with random initialization
Input: A prompt α and a secret key sk
Output: Watermarked text W b
[L]
1: t ←1; H ←0;
2: while done ̸= wb
t−1 do
3:
pt(1) ←pMb(1 | W b
[t−1], αb); pt(0) ←1−pt(1)
4:
if H < λ then
5:
Sample wb
t with (pt(0), pt(1));
6:
H ←H −ln pt(wb
t)
7:
if H ≥λ and t ≥h then
8:
R ←W b
[t]; St,h ←W b
[t−h,t−1]
9:
end if
10:
else
11:
Establish mapping rule Γt(Ω, V);{Eq. (5)}
12:
yt ←Fsk(R, St,h); wt ←1[yt ∈A2,t];
13:
end if
14:
t ←t + 1;
15: end while
As in the previous section, we can assume the decoder has
access to the reconstructed sequence Y[n+1:L]. For a given
text W b, a statistical test ψ(W b, Y ) is used to test hypothe-
sis H0 against H1. First, an initial chunk of W b with length
m, is considered as R = W b
[m]. Then for that specific initial
chunk R, Y (R) = Y[m+1:L] is constructed and we define
C(W b, Y ; m) :=
L
X
i=m+1
s(wb
i, yi).
(16)
Again, in calculating C(W b, Y ; m), any ngram “context
plus current token” is considered only once. The estimated
n in the decoder is defined as
n∗: = arg min
h ≤m≤L−1

p-valuem(C(W b, Y ; m))
	
=
min
h≤m≤L−1 Q(L −m, C(W b, Y ; m)).
(17)
Then the global p-value(C(W b, Y )), defined as,
p-value(C(W b, Y ))
:= 1 −(1 −p-valuen∗(C(W b, Y ; n∗)))L−h,
(18)
is calculated (see Appendix B.3.2).
If the global
p-value(C(W b, Y )) ≤FPR, the text is detected as water-
marked, where similar to the previous section, FPR is the
maximum tolerable false positive rate. We then obtain the
watermark detector given in Algorithm 2.
As in the previous section, we can derive a lower bound
Lmin on the number of required watermarked tokens such
that the false positive rate and false negative rate are
bounded by FPR and FNR, respectively. At first we start
with an estimate of Lmin as Lmin = L0, for a small value
Algorithm 2 Zero-bit detector with random initialization
Input: Text W b
[L] and a secret key sk
Output: true or false;
1: for m ←h, . . . , L −1 do
2:
Am = ϕ; C(W b
[L], Y ; m) ←0; R ←W b
[m]
3:
for i ←m + 1, . . . , L do
4:
if W b
[i−h:i] /∈Am then
5:
Add W b
[i−h:i] to Am; yi ←Fsk(R, W b
[i−1])
6:
vi ←wb
i · yi + (1 −wb
i) · (1 −yi)
7:
s(wb
i, yi) ←ln 1
vi
8:
C(W b
[L], Y ; m) ←C(W b
[L], Y ; m) + s(wb
i, yi)
9:
end if
10:
end for
11:
pm ←Q(|Am|, C(W b
[L], Y ; m))
12: end for
13: n∗←arg minh≤m≤L−1 pm {Eq. (17)}
14: if 1 −(1 −pn∗)L−h ≤FPR then
15:
return true Else return false
16: end if
L0. Then for this estimate Lmin = L0, we derive β and
θn given in (72) and (73). Using (80), we derive an upper
bound for false negative rate. If the derived upper bound
on false negative rate is less than FNR, the estimated value
of Lmin, is correct. However, if the derived upper bound
on false negative rate is greater than FNR, we increase the
estimate Lmin, i.e Lmin ←Lmin+1, and repeat this process
until for Lmin = L∗, the upper bound in (80) is bounded by
FNR, and we derive Lmin = log |V|
l
L∗
log |V|
m
.
4. Multi-Bit Distortion-free Watermarking
In this section we develop a multi-bit distortion-free wa-
termarking scheme based on the zero-bit schemes from
Section 3. We first extend Definition 2.4 to a multi-bit
watermarking mapping rule.
Definition 4.1 (Multi-bit watermarking mapping rule).
For a random variable y defined over the sample space
Ωwith distribution Py, a multi-bit watermarking mapping
rule Γ(Ω, V, M; Py) maps a partition of a sample space
Ωinto the token set V of the language model M. This
process is done in two steps: First a partition U(M) =
{A1(M), . . . , A|V|(M)}, depending on the message M ∈
M, on the sample space Ωis formed. Then each partition
part Aj(M) is mapped to vj for j = 1, . . . , |V|.
The multi-bit watermarking mapping rule is depicted in
Figure 1. The mapping rules in red and black depict two
distinct mapping rules based on two different embedded
messages. We assume M = {0, 1, . . . , 2m −1}, such
that each message M conveys m bits of information. Let
Γ(Ω, M) denote the partition on Ωdepending the embedded
6

Multi-Bit Distortion-Free Watermarking for LLMs
0
1
δM
pi(1) + δM
0
1
Vb
Figure 4. Multi-bit watermarking mapping rule in DISC.
message M, i.e. Γ(Ω, M) = U(M). On the other hand,
Γ(Ai(M)), denoted the mapped token, i.e. Γ(Ai(m)) = vi.
Finally, Γ(Ω, V, M) denotes the the process of partition-
ing the sample space based on the embedded message M
and then mapping the partition parts into the corresponding
tokens (see Appendix C.1).
We can extend Proposition 2.5 for a multi-bit watermarking
mapping rule as follows.
Proposition 4.2. A watermarking algorithm following a
multi-bit watermarking mapping rule Γt(Ω, V, M; Py) is
distortion-free if and only if for every prompt α and the past
generated tokens W[t−1] and for every message M ∈M,
P{yt ∈Aj,t(M)} = pM(vj | W[t−1], α) = Dt[vj]. (19)
Next, we propose a new multi-bit and distortion-free water-
marking algorithm called Distribution Interval Shift Coding
(DISC), which follows a multi-bit watermarking mapping
rule as depicted in Figure 4. Given a prompt α and the
past generated tokens W b
[i−1], the multi-bit watermarking
mapping rule Γi(Ω, V, M; Py) is specified as follows:
Ω= [0, 1],
yi ∼Uniform[0, 1],
Γi(Ω, M) = {A1,i(M), A2,i(M)},
Γi(A1,i(M)) = 0,
Γi(A2,i(M)) = 1.
(20)
For pi(1) + δM ≤1,
A1,i(M) = {0 ≤yi < δM} ∪{pi(1) + δM ≤yi ≤1},
A2,i(M) = {δM ≤yi < pi(1) + δM},
(21)
whereas for pi(1) + δM > 1,
A1,i(M) = {δM + pi(1) −1 ≤yi < δM},
A2,i(M) = {0 ≤yi < δM + pi(1) −1} ∪{δM ≤yi < 1},
(22)
where δM = Mδ, M ∈M = {0, 1, . . . , 2m −1}, and
δ = 2−m. As in Section 3, yi = Fsk(R, Si,h) ∈[0, 1] and
the DISC scheme can be shown to be distortion-free.
For a prompt αb and a message M, a sequence of uni-
form random numbers, Y [n+1:L] and a watermarked text
W b = (R, W b
[n+1:L]) will be generated, where the ini-
tial chunk of tokens, R is sampled from the language
model Mb, and the rest of the tokens are decided based
on the watermarking encoding rule in (20) and (22) with
yi = Fsk(R, Si,h). Therefore, we extend the encoding Al-
gorithm 1 in the Input and lines 13 and 15 to obtain the
DISC encoder given in Algorithm 3.
Algorithm 3 DISC encoder
Input: prompt α, secret key sk, message M ∈M
13:
Establish mapping rule Γt(Ω, V, M);
15:
wt ←1[yt ∈A2,t(M)];
Similar to Section 3.3, for a given text {W b}, the statistical
test ψ(W b, Y ) is used to test hypothesis H0 against H1.
This statistical test is performed as follows. First, an initial
chunk of W b with length m, is considered as R = W b
[m].
Then for that specific initial chunk R, Y (R) = Y[m+1:L]
is constructed. Then for δM ′ = M ′δ and M ′ ∈M as the
assumed message by the decoder,
C(W b, Y ; m, δM ′) =
L
X
i=m+1
s(wb
i, yi; δM ′),
(23)
is calculated. As in Section 3, any ngram “context + current
token” is considered only once. The score function is given
as follows (see Figure 9 in Appendix C.2):
s(wb
i, yi; δM ′) =









ln
1
yi−δM′+1
yi ∈[0, δM ′], wb
i = 1,
ln
1
yi−δM′
yi ∈(δM ′, 1], wb
i = 1,
ln
1
δM′−yi
yi ∈[0, δM ′), wb
i = 0,
ln
1
δM′−yi+1
yi ∈[δM ′, 1], wb
i = 0.
(24)
Similar to Section 3.3, after calculating C(W b, Y ; m, δM ′),
the estimated n and M in the decoder, i.e., n∗and M ∗,
defined as,
n∗, M ∗: = arg min
h ≤m≤L−1
M ′∈M

p-valuem,M ′(C(W b, Y ; m, δM ′))
	
.
=
min
h ≤m≤L−1
M ′∈M
Q(L −m, C(W b, Y ; m, δM ′)).
(25)
Then the global p-value(C(W b, Y )), defined as,
p-value(C(W b, Y ))
:= 1 −(1 −|M|p-valuen∗,M ∗(C(W b, Y ; n∗, M ∗)))L−h,
(26)
is calculated (see Appendix C.2).
If the global
p-value(C(W b, Y )) ≤FPR, the text is detected as water-
marked.
Therefore, we extend the detecting Algorithm 2 in the
Output and lines 4, 12-15, 17, 19, 20-21 to obtain the
DISC decoder given in Algorithm 4.
7

Multi-Bit Distortion-Free Watermarking for LLMs
Algorithm 4 DISC decoder
Output: true or false and message M if text is water-
marked
4:
C(W b, Y ; m, M ′) ←0;
12:
for
M ′ ∈M
do
13:
Calculate s(wb
i, yi; δM ′) {Eq. (24)}
14:
C(W b, Y ; m, M ′)
←
C(W b, Y ; m, M ′) +
s(wb
i, yi; δM ′);
15:
EndFor
17:
pm,M ′ ←Q(|Am|, C(W b, Y ; m, M ′));
19:
n∗, M ∗←arg minh≤m≤L−1
M ′∈M
pm,M ′
20:
If
1 −(1 −|M|pn∗,M ∗)L−h ≤FPR
then
21:
return true and M ′
Else return false Endif
Similar to Section 3.3, for a watermarked text Wb
W with
length L and initial chunk R, and the constructed Y(R),
generated as response to prompt αb, we can derive a lower
bound on the number of required watermarked tokens, Lmin,
such that the false positive rate and false negative rate are
bounded by FPR and FNR, respectively (see Appendix C.3).
For a watermarked text W b
W with length L and initial
chunk R = W b
[n], and the constructed Y (R), generated
as response to prompt αb, we can avoid the exhaustive
search over all M ′ ∈M in (25) to find M ∗. For m = n,
C(W b, Y ; m, ∆) follows the pattern given in Figure 12,
i.e., having maximum at ∆= 0 and two valleys on each side.
Because we know the pattern of C(W b, Y ; m, ∆), in order
to find M ∗, we can simply calculate C(W b, Y ; m, δM ′)
for a small set of equally spaced δM ′ ∈Ms, with |Ms| ≪
|M|. After calculating C(W b, Y ; m, δM ′) at δM ′ ∈Ms,
because of the patterns of C(W b, Y ; m, ∆) are known, we
can obtain a rough estimate of M ∗and then by a finer search
we can derive the exact M ∗. Therefore, for a text W b
W with
length L, the complexity of decoding Algorithm 4 can be
reduced to O(L2), and does not depend on the number of
possible embedded information bits.
5. Experiments
We assess the efficacy of DISC in embedding and extracting
the watermark by simulating the binary sequences. Specif-
ically, a real token is represented by 17 bits. For m-bits
watermark, there exist 2m distinct information options M
for watermarking, i.e., M ∈M = {0, 1, . . . , 2m −1}.
With M as the watermarking information to be conveyed,
δM = Mδ should be embedded during text generation. In
this context, we experiment with different values for m
ranging from 1 to 4.
For the text with L bits (i.e., L | 17 real tokens), we ran-
domly generate the probability of each bit being 1 and a
corresponding random value u for that bit. Subsequently,
Figure 5. BER when extracting bits with different length from the
text with various length.
the text is generated using the DISC encoder. In the process
of watermark decoding, we investigate various δM′ values
to pinpoint the one exhibiting the highest score within the
text. Figure 5 shows the BER at a logarithmic scale when
extracting watermarks of varying lengths across different
numbers of real tokens in the text. For each length of text,
the DISC algorithm was executed 10,000 times to compute
BER. Notably, the BER for four different m exhibits a sig-
nificant decrease initially. Specifically, the extraction of a
1-bit watermark achieves a 0 BER over a text of merely 6
tokens, while a 4-bit watermark attains 0 BER at 20 tokens.
6. Conclusion
We developed a new watermarking algorithm for Large Lan-
guage Models (LLMs) with a primary focus on achieving
distortion-free embedding of multiple information bits into
the watermark. Details on the mathematical justification
and analysis of the proposed watermarking algorithms are
provided in the appendices. The key contribution lies in
providing embedding power without compromising their
original functionality or quality. To the best of our knowl-
edge, this is the first work achieving multi-bit distortion-free
watermarking with efficient information decoding. This
advancement opens up avenues for many applications in
content authentication and communication security.
Future research can address the issue of further increasing
the embedding capacity of LLM watermarking algorithms.
In addition, watermarking algorithms that avoid the over-
head of binarization, e.g., (Aaronson, 2023), are preferable.
In ongoing work, we are modifying our proposed DISC
algorithms to work directly with the original token set of
the LLM. There is substantial scope for further exploration
and improvement of LLM watermarking methods.
8

Multi-Bit Distortion-Free Watermarking for LLMs
References
Aaronson, S. My AI Safety Lecture for UT Effective Altru-
ism., Nov. 2023. URL https://scottaaronson.
blog/?p=6823. Accessed May 5, 2023.
Abdelnabi, S. and Fritz, M. Adversarial watermarking trans-
former: Towards tracing text provenance with data hiding.
In 2021 IEEE Symposium on Security and Privacy (SP),
pp. 121–140. IEEE, 2021.
Bakhtin, A., Gross, S., Ott, M., Deng, Y., Ranzato, M.,
and Szlam, A. Real or fake? learning to discriminate
machine from human generated text.
arXiv preprint
arXiv:1906.03351, 2019.
Beresneva, D. Computer-generated text detection using ma-
chine learning: A systematic review. In Natural Language
Processing and Information Systems: 21st International
Conference on Applications of Natural Language to In-
formation Systems, NLDB 2016, Salford, UK, June 22-24,
2016, Proceedings 21, pp. 421–426. Springer, 2016.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-
lut, A., Brunskill, E., et al. On the opportunities and risks
of foundation models. arXiv preprint arXiv:2108.07258,
2021.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.
Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S.,
Fu, J., and Liu, Z. ChatEval: Towards better LLM-based
evaluators through multi-agent debate. arXiv preprint
arXiv:2308.07201, 2023.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.
Christ, M., Gunn, S., and Zamir, O. Undetectable Wa-
termarks for Language Models.
2023.
URL http:
//arxiv.org/abs/2306.09194.
Dong, X. L., Moon, S., Xu, Y. E., Malik, K., and Yu, Z.
Towards Next-Generation Intelligent Assistants Leverag-
ing LLM Techniques. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data
Mining, pp. 5792–5793, 2023.
Fagni, T., Falchi, F., Gambini, M., Martella, A., and Tesconi,
M. TweepFake: About Detecting Deepfake Tweets. Plos
one, 16(5):e0251415, 2021.
Fernandez, P., Chaffin, A., Tit, K., Chappelier, V., and Furon,
T. Three bricks to consolidate watermarks for large lan-
guage models. 2023.
Ge, Y., Hua, W., Ji, J., Tan, J., Xu, S., and Zhang, Y. Ope-
nagi: When llm meets domain experts. arXiv preprint
arXiv:2304.04370, 2023.
Goldstein, J. A., Sastry, G., Musser, M., DiResta, R.,
Gentzel, M., and Sedova, K.
Generative language
models and automated influence operations: Emerg-
ing threats and potential mitigations.
arXiv preprint
arXiv:2301.04246, 2023.
Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers,
I., and Goldstein, T.
A Watermark for Large Lan-
guage Models. 2023a. ISSN 26403498. URL http:
//arxiv.org/abs/2301.10226.
Kirchenbauer, J., Geiping, J., Wen, Y., Shu, M., Saifullah,
K., Kong, K., Fernando, K., Saha, A., Goldblum, M.,
and Goldstein, T. On the Reliability of Watermarks for
Large Language Models. 2023b. URL http://arxiv.
org/abs/2306.04634.
Kuditipudi, R., Thickstun, J., Hashimoto, T., and Liang, P.
Robust distortion-free watermarks for language models.
arXiv preprint arXiv:2307.15593, 2023.
Lapid, R., Langberg, R., and Sipper, M. Open sesame!
universal black box jailbreaking of large language models.
arXiv preprint arXiv:2309.01446, 2023.
Lavergne, T., Urvoy, T., and Yvon, F. Detecting fake content
with relative entropy scoring. Pan, 8(27-31):4, 2008.
Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
prompts for generation. arXiv preprint arXiv:2101.00190,
2021.
Liu, A., Pan, L., Hu, X., Meng, S., and Wen, L. A semantic
invariant robust watermark for large language models.
arXiv preprint arXiv:2310.06356, 2023.
Mialon, G., Dess`ı, R., Lomeli, M., Nalmpantis, C., Pa-
sunuru, R., Raileanu, R., Rozi`ere, B., Schick, T., Dwivedi-
Yu, J., Celikyilmaz, A., et al. Augmented language mod-
els: A survey. arXiv preprint arXiv:2302.07842, 2023.
Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., and
Finn, C. Detectgpt: Zero-shot machine-generated text
detection using probability curvature.
arXiv preprint
arXiv:2301.11305, 2023.
OpenAI.
Gpt-2:
1.5b
release.
Website,
2023.
https://openai.com/research/
gpt-2-1-5b-release/.
9

Multi-Bit Distortion-Free Watermarking for LLMs
Papoulis, A. and Pillai, S. Probability, Random Variables,
and Stochastic Processes. McGraw-Hill series in electri-
cal and computer engineering. McGraw-Hill, 2002. ISBN
9780071226615.
URL https://books.google.
com/books?id=k22UwAEACAAJ.
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and
Zhuang, Y. HuggingGPT: Solving AI tasks with Chat-
GPT and its friends in Hugging Face. arXiv preprint
arXiv:2303.17580, 2023.
Tamkin, A., Brundage, M., Clark, J., and Ganguli, D.
Understanding the capabilities, limitations, and soci-
etal impact of large language models. arXiv preprint
arXiv:2102.02503, 2021.
Tian,
E.
Gptzero update v1.
Website,
2023.
https://gptzero.substack.com/p/
gptzero-update-v1.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,
Azhar, F., et al. Llama: Open and efficient foundation lan-
guage models. arXiv preprint arXiv:2302.13971, 2023.
Vemprala, S., Bonatti, R., Bucker, A., and Kapoor, A. Chat-
gpt for robotics: Design principles and model abilities.
Microsoft Auton. Syst. Robot. Res, 2:20, 2023.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Weckesser, W., Bright, J., van der Walt, S. J., Brett, M.,
Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,
Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,
Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,
Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,
Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,
F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy
1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods, 17:261–272, 2020. doi:
10.1038/s41592-019-0686-2.
Wang, L., Yang, W., Chen, D., Zhou, H., Lin, Y., Meng, F.,
Zhou, J., and Sun, X. Towards Codable Text Watermark-
ing for Large Language Models. pp. 1–25, 2023. URL
https://arxiv.org/abs/2307.15992v1.
Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N.
Visual chatgpt: Talking, drawing and editing with visual
foundation models. arXiv preprint arXiv:2303.04671,
2023a.
Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S. Next-
GPT: Any-to-any multimodal LLM.
arXiv preprint
arXiv:2309.05519, 2023b.
Yang, K.-C. and Menczer, F. Anatomy of an AI-powered
malicious social botnet. arXiv preprint arXiv:2307.16336,
2023.
Ye, R., Zhang, C., Wang, R., Xu, S., and Zhang, Y.
Natural language is all a graph needs. arXiv preprint
arXiv:2308.07134, 2023.
Yoo, K., Ahn, W., Jang, J., and Kwak, N. Robust multi-bit
natural language watermarking through invariant features.
In Proceedings of the 61st Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: Long
Papers), pp. 2092–2115, 2023.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068, 2022.
10

Multi-Bit Distortion-Free Watermarking for LLMs
A. Preliminaries
A.1. Entropy of a language model
The set of all possible responses generated by M to a prompt α is denoted by W(M, α). Also, the set of all possible
responses generated by M to a prompt α with length greater than or equal to k, is denoted by W+
k (M, α). Hence, we have
W(M, α) = W+
1 (M, α),
W+
k (M, α) = {W[L] ∈W(M, α) : L ≥k}.
(27)
On the other hand, P{M(α)[L] = W[L]} is the probability of all the generated responses of a language model M to a prompt
α that starts with W[L].
With some abuse of notation, we use Dt in this paper to represent the probability distribution of a language model. The
assumption is that there is no ambiguity about the prompt α and the sequence of generated tokens so far, i.e., W[t−1]. When
we talk about altering a language model, we mean Dt →D′
t, where the prompt α and the sequence generated tokens so far,
i.e., W[t−1], are fixed. The entropy of the response of a language model M to a prompt α is defined as
H(α) := EM(α){−ln P{M(α)}}
(28)
Using (2) and (27), we can simplify H(α) as,
H(α) =
X
W[L]∈W(M,α)
−P{M(α) = W[L]} ln P{M(α) = W[L]}
=
X
W[L]∈W(M,α)
−P{M(α) = W[L]} ln
L
Y
t=1
pM(wt | W[t−1], α)
=
X
W[L]∈W(M,α)
L
X
t=1
−P{M(α) = W[L]} ln pM(wt | W[t−1], α) =
∞
X
L=1
X
W[L]∈VL
−P{M(α)[L] = W[L]} ln pM(wL | WL−1, α)
=
∞
X
L=1
X
W[L−1]∈VL−1
P{M(α)[L−1] = W[L−1]} ·
X
wL∈V
−pM(wL | W[L−1], α) ln pM(wL | W[L−1], α)
=
∞
X
L=1
X
W[L−1]∈VL−1
P{M(α)[L−1] = W[L−1]} · H(pM(wL | W[L−1], α)) =
∞
X
L=1
E{H(pM(wL | W [L−1], α))},
(29)
where H(pM(wL | W[L−1], α)) is defined as the conditional entropy of the L-th token, given already generated tokens
W[L−1], of response of a language model M to a prompt α. Hence, E{H(pM(wL | W [L−1], α))}, is the average conditional
entropy of the L-th token, conditioned on the past tokens, of response of a language model M to a prompt α . Therefore,
according to (29), entropy of response of a language model M to a prompt α, is the summation of the average conditional
entropy of the L-th token, conditioned on the past tokens, for L = 1, 2, . . ..
We define ζL(α) as the token-average of average conditional entropy, conditioned on the past tokens, for up to the L-th
token, of response of a language model M to a prompt α, for L = 1, 2, . . .. In other words,
ζL(α) = 1
L
L
X
i=1
E{H(pM(wL | W [L−1], α))}.
(30)
We define ζ(α) = limL→∞ζL(α), as the average conditional entropy, conditioned on the past tokens, per token of response
of a language model M to a prompt α. We assume for large values of L, ζL(α) ≈ζ(α).
A.2. Watermarking mapping rule
A watermarking mapping rule is shown in Figure 1. In our notation, Γ(·) and Γ(·, ·) denote different operations. When it
is used on a sample space, e.g. Γ(Ω), it denotes the partition formed on Ω. In other words, Γ(Ω) = {A1, A2, . . . , A|V|},
11

Multi-Bit Distortion-Free Watermarking for LLMs
whereas when it is used on a partition part, e.g., Γ(Aj), it denotes the mapped token, or Γ(Aj) = vj. Finally, when it is used
on a sample space and a token set, e.g., Γ(Ω, V), it means the whole process of partitioning the sample space and mapping
partition parts to the tokens.
Note that the watermarking mapping rule is not stationary and throughout the text, the partitioning of the sample space and
the mapping between partition parts to the different tokens changes. Hence, we denote the dependency of the mapping rule
on the t-th token by adding a subscript t as in Γt(·), Aj,t, and yt. However, we assume all yt’s are defined over the same
sample space, and they have the same distribution. A watermarking encoding following a watermarking mapping rule is
given in Algorithm 5.
Algorithm 5 Generating a watermarked text using a watermarking mapping rule Γ(Ω, V; Py)
Input: A prompt α
Output: Watermarked text W
1: t ←1;
2: while wt−1 ̸= done do
3:
Establish mapping rule Γt(Ω, V);
4:
Generate yt ∼Py
5:
for j ←1, . . . , |V| do
6:
if yt ∈Aj,t then
7:
wt ←vj;
8:
Break
9:
end if
10:
end for
11:
t ←t + 1;
12: end while
B. Zero-bit Distortion-Free Watermarking
B.1. Binarization of language models
Let E : V →{0, 1}log |V|, denote an encoding function, which converts each token into a binary string. For example,
E(w) = (wb
1, wb
2, . . . , wb
log |V|). Also, let E(w)n denote the n-th bit in the binary form of w. Then, given a prompt α
and the past generated tokens W[t−1], the distribution Dt for language model M, is converted into a series of probability
distributions Db
t = (Db
t,1, Db
t,2, . . . , Db
t,log |V|), where Db
t,k = (pt,k(0), pt,k(1)), for k = 1, . . . , log |V|, for the language
model Mb. Here,
pt,k(1) = P{wb
t,k =1 | W b
t,[k−1], W[t−1], α},
pt,k(0) = 1 −pt,k(1).
(31)
Note that, given a prompt α and the past generated tokens W[t−1], pt,k(0) can be simply calculated using Dt and the
encoding function E(·) as follows:
pt,k(1) =
X
v∈V
n
Dt[v] : E(v)[1:k] =(W b
t,[k−1], 1)
o
.
(32)
In this way, sampling log |V| times from the language model Mb is equivalent to sampling one token from language model
M.
Other coding schemes like Huffman Coding can represent each token with fewer binary tokens on average. However, they
cannot be applied here, as these encoders require the probability distribution of each token, and as it was mentioned before,
the underlying assumption in watermarking is that the decoder does not have access to the probability distributions.
Henceforth, we will use binary tokens to represent a text using the language model Mb. Hence, for convenience, we denote
pt,k(1) and wb
t,k as pi(1) and wb
i, respectively, where i = (t −1) log |V| + k. We assume the prompt α and the past
generated tokens W[t−1], are also represented by their E(·)-encoded counterparts, i.e., E(α) and E(W[t−1]), respectively.
Henceforth, we also adopt the superscript b, to denote the binary equivalent of a prompt, sequence of tokens, etc, for example,
E(α) = αb, E(W[t−1]) = W b
[t−1].
12

Multi-Bit Distortion-Free Watermarking for LLMs
As binarization of the language model M →Mb using the encoding operator E(·) sets up a one-to-one correspondence
between M and Mb, we have
H(α) = EMb(αb){−ln P{Mb(αb)}}.
(33)
Using a similar approach as (29), we can further simplify (33) as,
H(α) =
∞
X
i=1
E{H(pMb(wb
i | W b
[i−1], αb))} =
∞
X
i=1
E{Hb(pi(1))},
(34)
where Hb(pi(1)) is defined as the conditional entropy of the i-th binary token, given already generated binary tokens
W b
[i−1], of response of a language model Mb to a prompt αb and is defined as
Hb(x) := −x ln x −(1 −x) ln(1 −x).
(35)
Here, pi(1) is the probability of the i-th binary token being 1, given already generated tokens W b
[i−1], for response of a
language model Mb to a prompt αb, and is defined in (31) and (32). Additionally, E{Hb(pi(1))}, is the average conditional
entropy of the i-th binary token, conditioned on the past tokens, of response of a language model Mb to a prompt αb.
Similarly, pMb(W b
[0] | αb) = 1, and pMb(· | W b
[0], αb) = pMb(· | αb). We can express E{H(pM(wL | W [L−1], α))} as
summation of E{Hb(pl(1))} for l ∈{(L −1) log |V|, . . . , (L −1) log |V| + log |V| −1} as follows,
E{H(wL | W [L−1], α))} =
X
W[L−1]∈VL−1
P{M(α)[L−1] = W[L−1]} ·
X
wL∈V
−pM(wL | W[L−1], α) ln pM(wL | W[L−1], α)
=
X
W[L−1]∈VL−1
P{M(α)[L−1] = W[L−1]} ·
log |V|
X
j=1
X
W b
[i+1:i+j−1]∈Vbj−1
pMb(W b
[i+1:i+j−1] | W b
[L−1], αb)
·
X
wb
i+j∈Vb
−pMb(wb
i+j | W b
[i+1:i+j−1], W b
[i−1], αb) · ln pMb(wb
i+j | W b
[i+1:i+j−1], W b
[i−1], αb)
=
log |V|
X
j=1
E{Hb(pi+j(1))},
(36)
where i = (L −1) log |V|. Following the same steps as (30), we define ζb
L(αb) as the binary-token-average of average
conditional entropy, conditioned on the past tokens, for up to the L-th binary token, of response of a language model Mb to
a prompt αb, for L = 1, . . .. Hence,
ζb
L(αb) = 1
L
L
X
i=1
E{Hb(pi(1))}.
(37)
Similarly, we define ζb(αb) = limL→∞ζb
L(αb), as the average conditional entropy, conditioned on the past tokens, per binary
token of response of a language model Mb to a prompt αb. Here, we also assume for large values of L, ζb
L(αb) ≈ζb(αb).
According to (30), (36) and (37),
ζb(αb) =
1
log |V|ζ(α).
(38)
B.2. Watermarking without random initialization
B.2.1. PART 1
Assuming the watermarking process follows the described watermarking mapping rule and starts at the first token, then
a watermarked-text W b
[L] corresponds to a partition part of an L-dimensional hypercube [0, 1]L. For example, for a given
prompt αb and past generated tokens W b
[i−1], the event {wb
i = 1} corresponds to the event {yi ∈[0, pi(1))}. Hence, as this
watermarking rule is distortion-free, we have
E{Hb(pi(1))} = E{Hb(pi(1))},
(39)
13

Multi-Bit Distortion-Free Watermarking for LLMs
Algorithm 6 Zero-bit distortion-free watermarking encoder
Input: A prompt α and a secret key sk
Output: Watermarked text W b
1: t ←1;
2: while done ̸= wb
t−1 do
3:
pt(1) ←PMb(wb
i = 1 | wb
[t−1], αb);
4:
Establish mapping rule Γt(Ω, V);{Eq. (5)}
5:
yt ←Fsk(St,h);
6:
wt ←1[yt ∈A2,t];
7:
t ←t + 1;
8: end while
where pi(1) can be calculated using W b
[i−1] or Y[i−1]. Note that in the LHS of (39), the expectation is calculated over the
discrete distribution of W b
[i−1], whereas in the RHS, it is calculated over the continuous distribution of Y [i−1]. Henceforth
the expectations are done with respect to Y and hence the subscript Y are omitted. Hence,
ζb
L(αb) = 1
L
L
X
i=1
E{Hb(pi(1))}.
(40)
Therefore,
H(α) = EMb(αb){−ln P{Mb(αb)}} =
∞
X
i=1
E{Hb(pi(1))} = lim
L→∞Lζb
L(αb).
(41)
Following this procedure for a prompt α, a sequence of uniform random numbers, Y and a watermarked text W b will
be generated. The general Algorithm 5 for a watermarking encoding following a watermarking mapping rule can then be
updated to obtain Algorithm 6 for a Zero-bit distortion free watermarking encoding.
Suppose we have a watermarked text W b which was generated using the sequence of random numbers, Y , drawn from a
uniform distribution. Assuming the secret key sk was shared with the decoder and no changes were made to the watermarked
text, the decoder can reconstruct Y . Note that for a fixed secret key sk, the watermarking encoding in (6) always generates
the same response for a prompt α. This is an undesirable behaviour of a watermarking encoding and is addressed in the next
section. Furthermore, the probabilities P{C(W b
W, Y ) < θ} and P{C(W b
NW, Y ) ≥θ} are calculated over Y and because
watermarked text W b
W depends on Y , it is considered as random. However, the non-watermarked text, W b
NW, does not
depend on Y and therefore is not considered random. In other words, all the derivations for non-watermarked texts hold for
every fixed non-watermarked text. Although for a fixed αb and a fixed sk, Y will be deterministic, we can assume Y is
random due to uniform distribution over secret key sk or the use of true random function instead of a PRF.
B.2.2. PART 2
This score function is shown in Figure 6. Given a prompt αb, the expected value of the score function for the i-th token, if it
is non-watermarked, can be derived as follows:
E{s(wb
i, yi) | αb} = E

E

s(wb
i, yi) | Y [i−1], αb		
= E
Z 1
0
ln
 1
vi

dyi

= 1,
(42)
where vi = wb
i · yi + (1 −wb
i) · (1 −yi). On the other hand, given a prompt αb, the expected value of the score function for
the i-th token, if it is watermarked, can be derived as follows:
µi := E{s(wb
i, yi) | αb} = E

E

s(wb
i, yi) | Y [i−1], αb		
.
(43)
We can calculate E{s(wb
i, yi) | Y [i−1], αb} as follows,
E

s(wb
i, yi) | Y [i−1], αb	
=
Z pi(1)
0
ln
 1
yi

dyi +
Z 1
pi(1)
ln

1
1 −yi

dyi = 1 + Hb(pi(1)),
(44)
14

Multi-Bit Distortion-Free Watermarking for LLMs
Figure 6. Score function of the watermarking algorithm in (Christ et al., 2023).
where Hb(pi(1)) is defined in (35). Therefore, we can simplify (43) as
µi = 1 + E

Hb(pi(1))
	
.
(45)
The variance of the score value for the i-th token, if it is non-watermarked, given a prompt αb, is Var{s(wb
i, yi) | αb} = 1.
However, the variance of the score value for the i-th token, if it is watermarked, given a prompt αb, can be derived as follows,
σ2
i := Var{s(wb
i, yi) | αb} = E

E

s2(wb
i, yi) | Y [i−1], αb		
−µ2
i .
(46)
Similarly, we can calculate E{s2(wb
i, yi) | Y [i−1], αb} as,
E

s2(wb
i, yi) | Y [i−1], αb	
= G(pi(1)) + 2Hb(pi(1)) + 2,
(47)
where
G(x) := x ln2 x + (1 −x) ln2(1 −x),
(48)
Similarly, pi(1) can be calculated using W b
[i−1] or Y[i−1]. As H(p) ≥1, for all p ∈[0, 1], µ2
i > µi. Therefore, by using (48)
we can bound σ2
i as
σ2
i < max
pi(1)

G(pi(1)) + Hb(pi(1)) + 1
	
= 2.2.
(49)
The conditional expected value of the score function for a the i-th token, if it is watermarked, given a prompt αb and the past
generated tokens W b
[i−1], with respect to pi(1), is shown in Figure 7. As we can see in Figure 7, the score value for the
watermarked token is greater on average than the non-watermarked token. According to (45) and (47), if the distribution
of pi(1) is known, the expected value and variance of the score function for a the i-th token, if it is watermarked, can be
derived. Although we do not have any knowledge about the distribution of pi(1), if we assume pi(1) ∼Uniform[0, 1], we
can derive µi = 1.5 and σ2
i = 1.25.
Given a prompt αb and the past generated tokens W b
[i−1], without loss of generality, let us assume that pi(0) ≥pi(1). Then
for x ≥−ln pi(1), the conditional cumulative distribution function (cdf) of s(wb
i, yi) for the i-th token, if it is watermarked,
15

Multi-Bit Distortion-Free Watermarking for LLMs
Figure 7. Expectation and variance of the score value for a watermarked token.
given a prompt αb and the past generated tokens W b
[i−1], can be derived as follows:
F s(wb
i,yi)

x | W b
[i−1], αb
= P
n
s(wb
i, yi) ≤x | W b
[i−1], αbo
= P
n
s(wb
i, yi) ≤x | wb
i = 1, W b
[i−1], αbo
· P
n
wb
i = 1 | W b
[i−1], αbo
+ P
n
s(wb
i, yi) ≤x | wb
i = 0, W b
[i−1], αbo
· P
n
wb
i = 0 | W b
[i−1], αbo
= P

1
ln yi
≤x | yi ∈[0, pi(1)), W b
[i−1], αb

· P
n
yi ∈[0, pi(1)) | W b
[i−1], αbo
= P

1
ln(1−yi) ≤x | yi ∈[pi(1), 1], W b
[i−1], αb

· P
n
yi ∈[pi(1), 1] | W b
[i−1], αbo
= 1 −2e−x.
(50)
On the other hand, for −ln pi(1) > x ≥−ln pi(0), we have
F s(wb
i,yi)

x | W b
[i−1], αb
= P
n
s(wb
i, yi) ≤x | W b
[i−1], αbo
= P
n
s(wb
i, yi) ≤x | wb
i = 1, W b
[i−1], αbo
· P
n
wb
i = 1 | W b
[i−1], αbo
+ P
n
s(wb
i, yi) ≤x | wb
i = 0, W b
[i−1], αbo
· P
n
wb
i = 0 | W b
[i−1], αbo
= pi(0) −e−x.
(51)
Using (50) and (51), the conditional cumulative distribution function (cdf) of s(wb
i, yi) for the i-th token, if it is watermarked,
given a prompt αb and the past generated tokens W b
[i−1], is,
Fs(wb
i,yi)

x | W b
[i−1], αb
=

pi(0) −e−x,
−ln pi(1)> x≥−ln pi(0)
1 −2e−x,
x ≥−ln pi(1).
(52)
Note that according to (52), s(wb
i, yi) is a continuous random variable. As we can see, s(wb
i, yi) is not conditionally
distributed as the summation of an exponential random variable and a constant, as was mentioned in (Christ et al.,
2023)[Theorem 5]. On the other hand, given a prompt αb and the past generated tokens W b
[i−1], it can be easily shown
s(wb
i, yi) ∼Exp(1), where Exp(λ) denotes the exponential distribution with parameter λ, if it is non-watermarked,.
Therefore, using (43) and (46), for the watermarked text W b
W = W b
[L] with a fixed length L, the constructed Y [L], and a
prompt αb, we have,
ϑ := E
n
C(W b
W, Y ) | αbo
=
L
X
i=1
E{s(wb
i, yi) | αb} =L +
L
X
i=1
EY i−1

Hb(pi(1))
	
= L + Lζb
L(αb) ≈L + Lζb(αb),
(53)
if L is large enough. On the other hand, according to (49),
ς2 := Var{C(W b
W, Y ) | αb} = LVar{s(wb
i, yi) | αb} ≤2.2L.
(54)
16

Multi-Bit Distortion-Free Watermarking for LLMs
B.2.3. PART 3
The CLT for the summation of a sequence of independent but not identically distributed random variables is referred to as
the Berry-Esse´en Theorem (Papoulis & Pillai, 2002)[p. 283].
Theorem B.1 (Berry-Esse´en Theorem). Let {xi : i = 1, 2, . . .} be a sequence of independent random variables with
bounded E{xi} = µi and Var{xi} = σ2
i and
E{|xi −µi|3} < cσ2
i ,
∀i = 1, 2, . . . ,
(55)
where c > 0 is some constant. Then the cdf F¯x(x) of the normalized sum
¯x = 1
σ
n
X
i=1
(xi −µi),
(56)
where σ2 = Pn
i=1 σ2
i , converges to the cdf Φ(x) of a zero-mean, unit variance normal distribution, i.e., N(0, 1), with
Φ(x) =
R x
−∞
1
√
2πe−u2
2 du. This convergence of the cdf is denoted as ¯x
d−→N(0, 1).
According to (52), given a prompt αb and the past generated tokens W b
[i−1], s(wb
i, yi) for the i-th token, if it is watermarked,
is defined for positive x and has e−x terms. Therefore, for all n = 1, 2, . . .,
E

|s(wb
i, yi) −µi|n | W b
i−1, αb	
< c0,
E

|s(wb
i, yi) −µi|n | αb	
< c1,
for some c0, c1 > 0. Hence, the condition (55) in Theorem B.1, holds for some c > 0. Therefore, according to (53), for a
watermarked text W b
W and the constructed Y
C(W b
W, Y )
d−→N
 ϑ, ς2
,
(57)
where according to (54), ς ≤2.2L. We can also use the CLT approximation for a non-watermarked text W b
NW and the
constructed Y as,
C(W b
NW, Y )
d−→N (L, L) ,
(58)
However, as mentioned before, for non-watermarked token i, s(wb
i, yi) ∼Exp(1). Hence, using the assumption of
independence of s(wb
i, yi)’s, we can obtain a more accurate result as
C(W b
NW, Y ) ∼EL(1).
(59)
B.2.4. PART 4
For a given L and FPR, the threshold in (10), can be computed using gammainccinv in SciPy (Virtanen et al., 2020). The
detection algorithm for this watermarking scheme is given in Algorithm 7, where returning true means the text was detected
as watermarked, and returning false means the text was detected as non-watermarked.
For a given watermarked text W b
W with length L, and the constructed Y , using the normal approximation in (57), we can
derive the false negative rate for the threshold given in (10), as
false negative rate = P{C(W b
W, Y ) < θ} ≤Q
 ϑ −θ
1.5
√
L

,
(60)
where
Q(x) :=
1
√
2π
Z ∞
x
e−u2
2 du.
(61)
In order to derive a lower bound on the number of required watermarked tokens, Lmin, such that the false positive rate and
false negative rate are bounded by FPR and FNR, respectively, we can use the following numerical method. At first we
start with a estimate of Lmin as Lmin = L0, for a small value of L0. For this estimate Lmin = L0, we derive θ in (10) using
gammainccinv. Then using the derived θ, we derive an upper bound for false negative rate using (60). If the derived upper
17

Multi-Bit Distortion-Free Watermarking for LLMs
Algorithm 7 Zero-bit distortion-free watermarking detector
Input: A text W b
[L] ans a secret key sk
Output: true or false
1: A = ϕ;
2: C(W b
[L], Y ) ←0;
3: for i ←h + 1, . . . , L do
4:
if Si+1,h+1 /∈A then
5:
Add Si+1,h+1 to A;
6:
yi ←Fsk(Si,h);
7:
vi ←wb
i · yi + (1 −wb
i) · (1 −yi);
8:
s(wb
i, yi) ←ln 1
vi
9:
C(W b
[L], Y ) ←C(W b
[L], Y ) + s(wb
i, yi);
10:
end if
11: end for
12: θ ←Q−1(|A|, FPR); {Eq. (10)}
13: if C(W b
[L], Y ) ≥θ then
14:
return true;
15: else
16:
return false;
17: end if
bound is greater than FNR, we increase the estimate Lmin, i.e. Lmin ←Lmin + 1, and keep doing this process until for
Lmin = L∗, the upper bound in (60) is bounded by FNR, and we derive Lmin = log |V|
l
L∗
log |V|
m
. If L0 is chosen small
enough, the derived upper bound of the false negative for the corresponding θ, will be always greater than FNR. Using the
normal distribution approximation in (58) and the inequality
Q(x) ≤1
2e−x2
2 ,
(62)
we derive an estimate of θ given by
θN = L +
r
2L ln
1
2FPR = L + a1
√
L ≈θ.
(63)
B.3. Watermarking with initial random initialization
B.3.1. PART 1
By extending the definition in (28), the entropy of response of a language model Mb to a prompt αb that starts with R, is
derived as
H(αb, R) := EMb(αb,R)

−ln P

Mb(αb, R)
		
=
∞
X
i=1
E
n
H(pMb(wb
i | W b
[n+1:i−1], αb, r))
o
=
∞
X
i=n+1
E

Hb(pi(1))
	
,
(64)
where Hb(pi(1)) is the conditional entropy of the i-th binary token, given already generated tokens W b
[n+1:i−1], of response
of a language model Mb to a prompt αb that starts with R and can be derived using (35). Here pi(1) is the probability of the
i-th binary token being 1, given already generated tokens W b
[n+1:i], for response of a language model Mb to a prompt αb
that starts with R. Similarly, E{Hb(pi(1))} is the average conditional entropy of the i-th binary token, conditioned on the
past tokens, of response of a language model Mb to a prompt αb that starts with R.
Similarly, by extending (37), we define ζb
L(αb, R) as the binary-token-average of average conditional entropy, conditioned
on the past tokens, for up to the L-th binary token, of response of a language model Mb to a prompt αb that starts with R,
18

Multi-Bit Distortion-Free Watermarking for LLMs
for L = n + 1, n + 2, . . .. Hence,
ζb
L(αb, R) =
1
L −n
L
X
i=n+1
E

Hb(pi(1))
	
.
(65)
Similarly, we define ζb(αb, R) = limL→∞ζb
L(αb, R), as the average conditional entropy, conditioned on the past tokens,
per binary token of response of a language model Mb to a prompt αb that starts with R. Additionally, we assume for large
values of L, ζb
L(αb, R) ≈ζb(αb, R). Similarly, as the watermarking rule in here is distortion-free, we have
E

Hb(pi(1))
	
= E

Hb (pi(1))
	
,
(66)
where pi(1) can be calculated using Y[n+1:i−1] and R. Therefore, similar to (41), we have
H(αb, R) = EMb(αb,R)

−ln P

Mb(αb, R)
		
=
∞
X
i=n+1
E {H (pi(1))}= lim
L→∞(L −n)ζb
L(αb, R).
(67)
B.3.2. PART 2
The global p-value(C(W b, Y )), is defined as, the probability of observing a non-watermarked text as extreme as having
minimum p-value = pn∗, i.e.
p-value(C(W b, Y )) : = P
( L−1
[
m=h
n
C(W b, Y ; m) > p-value−1
m (pn∗)
o)
.
(68)
is calculated. If the global p-value(C(W b, Y )) ≤FPR, the text is detected as watermarked, where similar to the previous
section, FPR is the maximum tolerable false positive rate.
This leads to a critical region Dc = SL−1
m=h{C(W b, Y ; m) ≥θm}, where H0 is rejected, and a region of acceptance,
Dc
c = TL−1
m=h{C(W b, Y ; m) < θm}, where H0 is accepted. Note that, for a watermarked text W b
W, with length L and
initial chunk R = W b
[n], out of all possible choices of m, only for m = n, the generated Y (r) in the decoder will be equal
to Y[n+1:L] generated in the encoder. Therefore, for a watermarked text W b
W, for m ̸= n, there is no correlation between
Y (r) and W b
W.
Note that, similar to the previous section, for every m, the events {C(W b
W, Y ; m) < θm} and {C(W b
NW, Y ; m) ≥θm}
are defined over the sample space for Y (r), and hence, a non-watermarked text, W b
NW is independent of Y (r). On the
other hand, for the watermarked text W b
W with length L, with R = W b
[n], the event {C(W b
W, Y ; n) < θn} is defined
over the sample space Y (r) = Y [n+1:L], or equivalently over W b
[n+1:L]. This means, in the case R = W b
[n], the event
{C(W b
W, Y ; n) < θm} is defined for all the watermarked texts with length L that start with R = W b
[n].
As Y (r) is constructed based on the initial chunk of tokens, R = W b
[m], therefore, for different values of m, C(W b, Y ; m)
are independent from each other. Note that, in calculating P{C(W b, Y ; m) > θm | H0}, using a similar approach as
previous section, we can derive an exact and an approximate distribution for C(W b
NW, Y ; m) as,
C(W b
NW, Y ; m) ∼ErL−m(1),
C(W b
NW, Y ; m)
d−→N (L −m, L −m) ,
(69)
for m = h, . . . , L −1. As mentioned before, for the watermarked text W b
W, with length L, and with R = W b
[n], for m ̸= n,
constructed Y (r) is independent of W b
W. Therefore,
C(W b
W, Y ; m) ∼ErL−m(1),
C(W b
W, Y ; m)
d−→N (L −m, L −m) ,
(70)
for m ̸= n. Therefore,
pn∗=
min
h≤m≤L−1 Q(L −m, C(W b, Y ; m)).
(71)
19

Multi-Bit Distortion-Free Watermarking for LLMs
Hence, we can simplify (68) as,
p-value(C(W b, Y )) = 1 −
L−1
Y
m=h
P
n
C(W b, Y ; m) ≤Q−1(L −m, pn∗) | H0
o
= 1 −(1 −pn∗)L−h.
In other words, for a text W b
[L], if for pn∗we have,
pn∗≤β := 1 −(1 −FPR)
1
L−h ,
(72)
the text is detected as watermarked. Therefore, the set of threshold levels {θh, . . . , θL−1} are derived as,
θm = Q−1(L −m, α).
(73)
Similar to the previous section, the expected value of the score function for the i-th token, if it is watermarked, given a
prompt αb, with initial chunk R = W b
[n] is
µi := E

s(wb
i, yi) | αb, R
	
= E

E

s(wb
i, yi) | Y [n+1:i−1], αb, R
		
= 1 + E

Hb (pi(1))
	
.
(74)
Similarly, the variance of the score value for the i-th token, if it is watermarked, given a prompt αb, with initial chunk
R = W b
[n] is
σ2
i := Var{s(wb
i, yi) | αb, R} = E

E

s2(wb
i, yi) | Y [n+1:i−1], αb, R
		
−µ2
i
< max
pi(1)

G(pi(1)) + Hb(pi(1)) + 1
	
= 2.2.
(75)
Similar to the previous section, using (74) and (75), for the watermarked text W b
W with length L and initial chunk R = W b
[n],
and the constructed Y (r) = Y [n+1:L], generated as response to prompt αb, we have,
ϑn : = E
n
C(W b
W, Y ; n) | αbo
=
L
X
i=n+1
E{s(wb
i, yi) | αb, R} = L −n +
L
X
i=n+1
E

Hb (pi(1))
	
= L −n + (L −n)ζb
L(αb, R) ≈L −n + (L −n)ζb(αb, R),
(76)
if L is large enough. On the other hand,
ς2
n := Var
n
C(W b
W, Y ; n) | αbo
= (L −n)Var{s(wb
i, yi) | αb, R} ≤2.2(L −n)
(77)
Similar to the previous section, by using CLT we get,
C(W b
W, Y ; n)
d−→N
 ϑn, ς2
n

.
(78)
For a watermarked text W b
W with length L and initial chunk R = W b
[n], and the constructed Y (r), we can derive the false
negative rate, as
false negative rate = P
( L−1
\
m=h
n
C(W b, Y ; m) < θm | H1
o)
,
(79)
where the set of threshold levels {θh, . . . , θL−1} are given in (73). Using (70) and (78), we can simplify (79) further as,
false negative rate =
L−1
Y
m=h
m̸=n
(1 −Q(L −m, θm)) · Q
ϑn −θn
ςn

≤(1 −FPR)
L−h−1
L−h
· Q
 ϑn −θn
1.5
√
L −n

.
(80)
20

Multi-Bit Distortion-Free Watermarking for LLMs
Figure 8. Exact and approximate (dashed lines) Lmin for |V| = 50272, and n = 3h⌈log |V|⌉.
As in Section 3.2, using the normal distribution approximation we derive θN , as an approximate of θn as,
θN = L −n +
r
2(L −n) ln 1
2β ≈θn.
(81)
Similarly, using (81), (80), we derive the approximation for Lmin as,
f 2
1 ( FPR
Lmin , FNR)
ζb(αb)2
≈Lmin −n,
(82)
where f1(FPR, FNR) is defined in (13). Therefore, according to (82), the initial random chunk R has the equivalent effect
of reducing FPR by a factor L, on the required number of watermarked tokens. The exact and approximation value of
Lmin −n, i.e. number of watermarked tokens, derived by the numerical method mentioned here and by using (82), for
different values of ζ(αb) for vocabulary size |V| = 50272, , and |R| = 3h, are shown in Figure 8, where the approximation
values are depicted by dashed lines. In Figure 8, the bounds on false positive rate and false negative rate are considered
equal. Similar to Section 3.2, as the average conditional entropy per token for the generated text increases, lower number of
tokens are required to achieve the same false negative and positive rates. Comparing the results in Figures 3 and 8 shows,
having initial random chunk R, increases the required number of watermarked tokens by a factor about 1.4.
We can estimate ζb(αb, R) using already generated tokens in R = W b
[n] as,
ζb(αb, R) ≈1
n
n
X
i=1
H(pMb(wb
i | αb, R[1:i−1])) = 1
n
n
X
i=1
Hb(pi(1))
(83)
where the RHS is the binary-token-average of conditional entropy, conditioned on the past tokens, for tokens in initial chunk
R, of response of language model Mb for prompt αb.
C. Multi-bit Distortion-Free Watermarking
C.1. Part 1
Here, we also denote the dependence of the multi-bit watermarking mapping rule on the t-th token by adding a subscript t.
Then Algorithm 5 for a watermarking encoding following a watermarking mapping rule can be extended in the Input and
lines 3 and 6 to obtain Algorithm 8 for a watermarking encoding following a multi-bit watermarking mapping rule.
C.2. Part 2
Define
pn∗,M ∗:= p-valuen∗,M ∗(C(W b, Y ; n∗, δM ∗)).
(84)
21

Multi-Bit Distortion-Free Watermarking for LLMs
Algorithm 8 Generating a watermarked text using a multi-bit watermarking mapping rule Γ(Ω, V, M; PY )
Input: A prompt α and embedding message M ∈M
3:
Establish mapping rule Γt(Ω, V, M);
6:
If yt ∈Aj,t(M) then
Figure 9. Score function of the DISC algorithm.
Then, the global p-value(C(W b, Y )), is defined as
p-value(C(W b, Y )) := P

[
h≤m≤L−1
M ′∈M

C(W b, Y ; m, M ′) > p-value−1
m,M ′(pn∗,M ∗) | H0

.
(85)
If the global p-value(C(W b, Y )) ≤FPR, the text is detected as watermarked. Similar to Section 3.3, for a watermarked text
W b
W, with initial chunk R = W b
[n], there is no correlation between Y (r) and W b
W, for m ̸= n, where m is the length of the
considered initial chunk in the decoder.
As in Section 3.3, we can see, for a non-watermarked text W b
NW and for any initial chunk R = W b
[m], the constructed Y (r)
is independent of W b
NW. Similarly, for different values of m, C(W b, Y ; m, δM ′) are independent from each other.
Therefore, we can simplify (85) as
p-value(C(W b, Y )) : = 1 −
L−1
Y
m=h
\
M ′∈M
P
n
C(W b, Y ; m, δM ′) ≤p-value−1
m,M ′(pn∗,M ∗) | H0
o
= 1 −
L−1
Y
m=h
P
n
Cmax(W b, Y ; m) ≤p-value−1
m,M ′(pn∗,M ∗) | H0
o
,
(86)
where
Cmax(Wb, Y, m) := max
M ′∈M

C(Wb, Y; m, δM ′)
	
.
(87)
Note that, for a fixed m and different values of M ′ ∈M, C(W b, Y ; m, δM ′) are correlated with each other. In fact,
according to (24), for a fixed m and all different values of M ′ ∈M, s(wb
i, yi; δM ′) are functions of s(wb
i, yi; δ0). Hence,
for a fixed m, by having W b, Y and C(W b, Y ; m, δ0) all C(W b, Y ; m, δM ′) can be uniquely determined for all M ′ ∈M.
Therefore, we cannot simplify the probability of intersection event in (86) using product of probabilities.
The score function in (24), is just a shifted version of the score function in Section 3. Therefore, for a non-watermarked text
22

Multi-Bit Distortion-Free Watermarking for LLMs
(a) m = L −1, wb
L = 1, and θm = 3.
(b) m = L −2, (wb
L−1, wb
L) = (1, 1), and θm = 5.
Figure 10.

Cmax(W b
NW, Y; m) > θm | H0
	
for |M| = 4.
W b
NW with length L, C(W b
NW, Y ; m, δM ′) follows the same distribution as C(W b
NW, Y ; m) in Section 3.3. Hence,
C(W b
NW, Y ; m, δM ′) ∼EL−m(1),
C(W b
NW, Y ; m, δM ′)
d−→N (L −m, L −m) ,
(88)
for m = h, . . . , L −1, and M ′ ∈M. Similarly, for the watermarked text W b
W, with length L, and with R = W b
[n], for
m ̸= n and M ′ ∈M,
C(W b
W, Y ; m, δM ′) ∼EL−m(1),
C(W b
W, Y ; m, δM ′)
d−→N (L −m, L −m) .
(89)
We can simplify p-value(C(W b, Y )) in (86) using (88), as
P
n
Cmax(W b, Y ; m) > Q−1(L −m, pn∗,M ∗) | H0
o
≈|M|P

C(W b
NW, Y ; m, δM) > Q−1(L −m, pn∗,M ∗)
	
= |M|pn∗,M ∗,
(90)
for
Q−1(L −m,pn∗,M ∗) ≥θ(m)min := (L −m) ln
1
1 −δ|M|−1
= −(L −m) ln δ = (L −m)r ln 2.
(91)
The equality in (90) stems from the symmetry of C(W b
NW, Y ; m, δM ′) for M ′ ∈M, over the sample space Y ∈[0, 1]L−m.
This is shown in Figures 10(a) and 10(b) for m = L −1 and m = L −2, respectively, where the highlighted area shows the
area for which {Cmax(W b
NW, Y ; L −1) > θ}. Therefore,
p-value(C(W b, Y ))=

1 −(1 −|M|pn∗,M ∗)L−h,
pn∗,M ∗≤pmin,
1,
pn∗,M ∗> pmin,
(92)
where
pmin =
min
h≤m≤L−1 Q(L −m, θ(m)min) ≈Q(
√
L −h(r ln 2 −1)),
(93)
where we have used the following lemma.
Lemma C.1. For every x > 1 and m > n, we have
Q(m, mx) < Q(n, nx).
(94)
23

Multi-Bit Distortion-Free Watermarking for LLMs
0
u′
i
ui
1
1
δM
δM + pi(1)
δ′
M
δ′
M + pi(1)
ui = u′
i
wb
i = 0 wb
i = 1
wb
i = 0
Figure 11. ui and u′
i for DISC for δM + pi(1) ≥δ′
M ≥δM.
In other words, for a text W b
[L], if for pn∗,M ∗we have,
pn∗,M ∗≤βDISC := 1 −(1 −FPR)
1
L−h
|M|
,
(95)
the text is detected as watermarked. Therefore, similar to Section 3.3, the set of threshold levels {θh, . . . , θL−1} is derived
as follows:
θm = Q−1(L −m, αDISC), m = h, . . . , L −1,
(96)
where the text is detected as non-watermarked if TL−1
m=h{Cmax(W b, Y ; m) ≤θm}, and it is detected as watermarked if,
SL−1
m=h{Cmax(W b, Y ; m) > θm}.
C.3. Part 3
The expected value of the score function for the i-th token, if it is watermarked, given a prompt αb, with initial chunk
R = W b
[n], and the embedded message M in the encoder and the assumed message M ′ in the decoder, is
µi := E

s(wb
i, yi; δM ′) | R, αb	
= E

E

s(wb
i, yi; δM ′) | R, αb, Y [n+1:i−1]
		
.
(97)
As mentioned before, yi = Fsk(R, Si,h) is generated in the encoder and then based on yi and using (20) and (22), wb
i is
generated. In the decoder, on the other hand, yi is generated again, and using (24), the score function s(wb
i, yi; δM ′) is
calculated. Assuming the secret key is shared between encoder and decoder and the watermarked text is not modified, the
generated yi in the encoder and the decoder are equal. This process is shown in Figure 11 for δM + pi(1) ≥δM ′ ≥δM,
where yi generated in the encoder is denoted with ui and yi generated in the decoder is denoted with u′
i. Using Figure 11,
E{s(wb
i, yi; δM ′) | R, αb, Y [n+1:i−1]} is calculated as follows,
E

s(wb
i, yi; δM ′) | αb, R, Y [n+1:i−1]
	
=
Z δM
0
ln
1
δ′
M −u du +
Z δM′
δM
ln
1
u −δM ′ + 1 du
+
Z δM+pi(1)
δM′
ln
1
u −δM ′ du +
Z 1
δM+pi(1)
ln
1
δM ′ −u + 1 du = 1 −Hb(∆) + Hb(pi(1) −∆),
(98)
where Hb(x) is defined in (35) and ∆= δM ′ −δM.
It can be easily shown, for any continuous function g(x), E{g(s(wb
i, yi; δM ′)) | R, αb, Y [n+1:i−1]} for the i-th token, if it
is watermarked, given a prompt αb, with initial chunk R = W b
[n], and the embedded message M in the encoder and the
assumed message M ′ in the decoder, is δM-invariant, i.e.,
E
n
g(s(wb
i, yi; δM ′)) | R, αbo
=E

g(s(wb
i, yi; ∆)) | R, αb	
,
(99)
24

Multi-Bit Distortion-Free Watermarking for LLMs
Figure 12. µi and σ2
i for a watermarked token if pi(1) ∼Uniform[0, 1].
with ∆= δM ′ −δM. Using (99), we can calculate E{s(wb
i, yi; δM ′) | R, αb, Y [n+1:i−1]} for all the cases of δM and δ′
M as
E

s(wb
i, yi; δM ′) | αb, R, Y [n+1:i−1]
	
=







1−Hb(∆)+Hb(∆−pi(1)),
∆≥pi(1),
1−Hb(∆)+Hb(pi(1) −∆),
pi(1) ≥∆≥0,
1−Hb(−∆)+Hb(pi(1) −∆),
0 ≥∆≥−pi(0)
1−Hb(−∆)+Hb(−pi(0) −∆),
−pi(0) ≥∆.
(100)
Similarly, the variance of the score function for the i-th token, if it is watermarked, given a prompt αb, with initial
chunk R = W b
[n], and the embedded message M in the encoder and the assumed message M ′ in the decoder, for
δM + pi(1) ≥δM ′ ≥δM, is
σ2
i := Var{s(wb
i, yi; δM ′) | αb, R} = E

E

s2(wb
i, yi; δM ′) | αb, R, Y [n+1:i−1]
		
−µ2
i .
(101)
Similarly, using (99), we can calculate Eyi{s2(wb
i, yi; δM ′) | R, αb, Y [n+1:i−1]} for all the cases of δM and δ′
M as
E

s2(wb
i, yi; δM ′) | αb, R, Y [n+1:i−1]
	
=







2 −G(∆) −2Hb(∆) + G(∆−pi(1)) + 2Hb(∆−pi(1)),
∆≥pi(1),
2 −G(∆) −2Hb(∆) + G(pi(1) −∆) + 2Hb(pi(1) −∆),
pi(1) ≥∆≥0,
2 −G(−∆) −2Hb(−∆) + G(pi(1) −∆) + 2Hb(pi(1) −∆),
0 ≥∆≥−pi(0),
2 −G(−∆) −2Hb(−∆) + G(−pi(0) −∆) + 2Hb(−pi(0) −∆),
−pi(0) ≥∆.
(102)
E{s(wb
i, yi; δM ′) | R, αb, Y[n+1:i−1]} for a watermarked token, given a prompt αb, with initial chunk R = W b
[n], with
pi(1) = 0.7, and with embedded message M in the encoder and the assumed message M ′ in the decoder is shown in
Figure 13.
Similar to Section 3.2, if the distribution of pi(1) is known, the expected value and variance of the score function for a the
i-th token, if it is watermarked, can be derived. Similarly, if we assume pi(1) ∼Uniform[0, 1], we can derive µi and σ2
i , as
shown in Figure 12. Note that for ∆= 0, s(wb
i, yi; δM ′) is equal to s(wb
i, yi) in Section 3. Hence, for ∆= 0, σ2
i ≤2.2.
Therefore, for the watermarked text W b
W with length L and initial chunk R = W b
[n], and the constructed Y (r) = Y [n+1:L],
generated as response to prompt αb, we have,
ϑn : = E
n
C(W b
W, Y ; n, δM) | αbo
=
L
X
i=n+1
EY [n+1:i]{s(wb
i, yi−n; δM) | R, αb}
= L −n + (L −n)ζb
L([αb, R]) ≈L −n + (L −n)ζb([αb, R]),
(103)
25

Multi-Bit Distortion-Free Watermarking for LLMs
Figure 13. E{s(wb
i, yi; δM′) | R, αb, Y[n+1:i−1]} with pi(1) = 0.7.
if L is large enough. On the other hand,
ςn := Var
n
C(W b
W, Y ; n, δM) | αbo
:== (L −n)Var{s(wb
i, yi; δM) | αb, R} ≤2.2(L −n)
(104)
Similarly, by using CLT we get,
C(W b
W, Y ; n, δM)
d−→N(ϑn, ςn) .
(105)
Similar to Section 3.3, for a watermarked text W b
W with length L and initial chunk R = W b
[n], and the constructed Y (r),
with embedded message M in the encoder and the assumed message M ′ in the decoder, false negative rate is
false negative rate = P
( L−1
\
m=h
n
Cmax(W b, Y ; m) < θm | H1
o)
,
(106)
where the set of threshold levels {θh, . . . , θL−1} are given in (96). The events {Cmax(W b
W, Y ; L−1) > θm} for m = L−1
and m = L −2 are shown in Figures 14(a) and 14(b), respectively. As we can see, in Figure 14, for the case ∆= 0,
all the corners of the (L −m)-dimensional cube Y ∈[0, 1]L−m, and the strips along each edge of this cube satisfy
{Cmax(W b
W, Y ; L −1, δ0) > θm}. These areas for m = L −2 are shown with blue shaded area in Figure 14(b). For other
cases of ∆̸= 0, however, just a volume of Y equal to δM ′-shifted of one of the corners of the (L −m)-dimensional cube
and the corresponding edges strips satisfies {Cmax(W b
W, Y ; L −1, δ0) > θm(Z∗)}. For each case ∆= δM ′ ̸= 0, these
areas for m = L −2 are shown with shaded areas in green, red, or magenta in Figure 14(b). Therefore, using (105), we can
simplify (106), as
P
n
Cmax(W b, Y ; n) > θ | H1
o
≈

(|M| −1)L −n
2L−n + 1

· P
n
C(W b, Y ; n, δ0) > θn | H1
o
=

(|M| −1)L −n
2L−n + 1

Q
θn −ϑn
ςn

.
(107)
Therefore, using (89) and (107) we can simplify the false negative rate as
false negative rate =
L−1
Y
m=h
m̸=n
(1 −|M|Q(L −m, θm)) ·

1 −

(|M| −1)L −n
2L−n + 1

Q
θn −ϑn
ςn

≤(1 −FPR)
L−h−1
L−h
·

1 −

(|M| −1)L −n
2L−n + 1

Q
 θn −ϑn
1.5
√
L −n

,
(108)
26

Multi-Bit Distortion-Free Watermarking for LLMs
(a) m = L −1, pL(1) = 0.7, and θm(Z∗) = 3.
(b) m = L −2, (pL−1(i), pL(1)) = (0.7, 0.4), and θm = 5.
Figure 14.

Cmax(W b
NW, Y; m) > θm | H1
	
for |M| = 4 and δM = 0.
with θn given in (96). Similar Section 3.3, for a watermarked text W b
W with length L and initial chunk R = W b
[n], and the
constructed Y (R), generated as response to prompt αb, using (95) and (108), we can derive a lower bound on the number
of required watermarked tokens, Lmin, such that the false positive rate and false negative rate are bounded by FPR and FNR,
respectively.
As in Section B.3, using the normal distribution approximation we derive θN , as an approximate of θn as,
θN = L −n +
r
2(L −n) ln
1
2βDISC
≈θn.
(109)
Similarly, using (109), (108), we derive the approximation for Lmin as,
f 2
1 (
FPR
|M|Lmin , FNR)
ζb(αb)2
≈Lmin −n,
(110)
where f1(FPR, FNR) is defined in (13). Therefore, according to (110), the effect of having initial random chunk R and
the embedding message set M in DISC, on the required number of watermarked tokens is equivalent to reducing FPR
by a factor L|M|. The exact and approximation value of Lmin −n, i.e. number of watermarked tokens, derived by the
numerical method mentioned here and by using (110), for different values of ζ(αb) for vocabulary size V = 50272, ,
|R| = 3h⌈log |V|⌉, and |M| = 210, are shown in Figure 8, where the approximation values are depicted by dashed lines. In
Figure 8, the bounds on false positive rate and false negative rate are considered equal. Similar to Section 3.2, as the average
conditional entropy per token for the generated text increases, lower number of tokens are required to achieve the same
false negative and positive rates. Comparing the results in Figures 3 and 8 shows, having initial random chunk R and the
embedding message set M in DISC, increases the required number of watermarked tokens by a factor about 1.6.
27

Multi-Bit Distortion-Free Watermarking for LLMs
Figure 15. Exact and approximate (dashed lines) Lmin for |V| = 50272, n = 3h⌈log |V|⌉, |M| = 210.
28
