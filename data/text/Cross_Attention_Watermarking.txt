CROSS-ATTENTION WATERMARKING OF LARGE LANGUAGE MODELS
Folco Bertini Baldassini1
Huy H. Nguyen2
Ching-Chung Chang2
Isao Echizen2,3
1Sorbonne University, France
2National Institute of Informatics, Japan
3The University of Tokyo, Japan
ABSTRACT
A new approach to linguistic watermarking of language models is
presented in which information is imperceptibly inserted into the
output text while preserving its readability and original meaning. A
cross-attention mechanism is used to embed watermarks in the text
during inference. Two methods using cross-attention are presented
that minimize the effect of watermarking on the performance of a
pretrained model. Exploration of different training strategies for op-
timizing the watermarking and of the challenges and implications of
applying this approach in real-world scenarios clarified the tradeoff
between watermark robustness and text quality. Watermark selection
substantially affects the generated output for high entropy sentences.
This proactive watermarking approach has potential application in
future model development.
Index Terms— Large Language Models, Linguistic Water-
marking, Cross Attention, Steganography
1. INTRODUCTION
Linguistic watermarking refers to the insertion of particular unno-
ticeable information into a text document while preserving its read-
ability, intended meaning, and ability to withstand noise [1]. Interest
in such watermarking is growing due to the widespread emergence of
AI-generated text, which poses risks in various sectors, including on-
line influence campaigns, AI exploitation of authorship, spamming,
harassment, malware facilitation, and social engineering [2, 3]. Text
watermarking has various applications, including the tagging of AI-
generated text. It can be used to protect intellectual property, de-
tect leaks, and verify the source. In addition, it aids in complying
with regulations that mandate identification of AI-generated texts in
countries that are implementing such requirements [4]. Lastly, wa-
termarking can assist language model developers in ensuring that
model output is distinguishable to prevent model collapse during
training on generated text [5].
As the output of large language models (LLMs) approaches the
quality of human-generated text, it becomes more difficult to distin-
guish between them. As LLMs improve, increase in number, and
become easier to fine-tune, the existing post-hoc detectors for AI-
generated text are becoming more unreliable in correctly identify-
ing such content. The defenses often fail to generalize to real-world
scenarios [6] and often fail in adversarial settings [7]. Intention-
ally modifying models to embed watermarks in the output text is a
promising solution to this challenge. Unfortunately, the implemen-
tation of a useful watermark often involves substantial modifications
to the distribution of text generated by the template, potentially de-
grading its quality [8].
Code available at https://gitlab.com/folbaeni/linguistic-watermark
This work was partially supported by JSPS KAKENHI Grants JP18H04120,
JP20K23355, JP21H04907, and JP21K18023, and by JST CREST Grants
JPMJCR18A6 and JPMJCR20D3, Japan.
Our focus is on blind watermarking, which enables verification
based solely on bits extracted from the watermarked text [1]. They
can be used to identify information, copyrights, digital signatures,
or any other element that can establish the source or integrity of the
generated text. Watermarking ensures accurate content attribution,
assists in plagiarism detection, and prevents the malicious manip-
ulation of information. With this approach, nouns, verbs, adjec-
tives, and other grammatical elements are modified without chang-
ing the text’s intended meaning [1]. The exploitation of semantic
information in text content has shown promise in improving the ro-
bustness and generalization performance of deepfake text detection
systems [7].
However, adversaries have the goal of damaging a watermark
with minimal changes to the text. To counter this, a highly robust
watermark is needed that can withstand being heavily modified and
still remain effective. Such a watermark should discourage attackers
because it would make their efforts futile. Furthermore, the gener-
ated text should be of high quality to ensure that the model remains
viable. To summarize, a watermark should be able to seamlessly in-
tegrate and authenticate itself within the original text while making
minimal alterations to the original content. In addition, watermarks
should be imperceptible and indistinguishable from the original con-
tent even when subjected to repeated adaptive queries. Furthermore,
watermarks should be robust against simple attempts to remove them
and remain extremely unlikely to produce false positives.
Although numerous studies of watermarking using machine
learning have been reported, few have specifically addressed the
LLM domain and exploited the underlying LLM architecture to
produce watermarked output in real time during inference. We have
addressed this shortcoming. This work makes four key contribu-
tions:
• A watermarking layer has been designed that incorporates a cross-
attention mechanism into the language model. This approach ex-
ploits the LLM architecture to produce robust watermarks during
inference with a minimal increase in parameter count.
• An explainable framework for watermark verification has been de-
vised, and the challenges and implications of applying the pro-
posed watermarking approach in real-world scenarios have been
identified.
• Two methods using cross-attention are presented that minimize the
effect of watermarking on the performance of a pretrained model.
• Training strategies that enhance the performances and the robust-
ness of the proposed watermarking technique have been devel-
oped.
2. RELATED WORK
Natural language watermarking methods [9, 10] initially used syn-
tactic trees and large prime numbers as secret keys to insert water-
marks into natural language text. However, changes in language
models made these methods obsolete. An alternate approach [11]
uses word sense ambiguity to embed watermarks by replacing words
arXiv:2401.06829v1  [cs.CL]  12 Jan 2024

with labelled synonyms. More advanced techniques [12] include
a reversible watermarking method that combines arithmetic coding
and synonym substitution operations. Machine learning models have
also been applied to text watermarking, initially using long short-
term memory (LSTM) models [13] to select tokens carrying specific
bits and thereby ensuring that the generated watermarked text fol-
lowed the desired distribution. The original data can be recovered in
a deterministic manner by mapping the tokens back to their corre-
sponding bit blocks.
The advent of transformers led to the development of meth-
ods that use sequence incremental watermarking with an infill
model [14] or a neural paraphraser [15] whose candidates are se-
lected by semantic relevance and target bit. Siamese models like
Siamese-BERT (SBERT) [16] enable the determination of seman-
tic similarity among sentences.
There are methods that utilize a
pretrained Word2Vec model [17, 18] or an infill model like BERT
(Bidirectional Encoder Representations from Transformers) [19, 20]
for generating candidate replacement words.
SBERT is used to
evaluate the quality of each proposed sentence. The proposed sen-
tence with the greatest similarity to the original sentence becomes
the watermarked sentence. A pretrained BERT model is utilized for
watermark detection using a binary classifier.
Abdelnabi and Fritz [21] introduced a method that uses an
encoder-decoder transformer to rewrite the text and embed a binary
vector as a watermark. The model consists of an embedder that
uses an encoder-decoder transformer, a bidirectional extractor to
reconstruct the binary vector, and a bidirectional discriminator that
preserves language statistics. SBERT is used to incorporate semantic
loss, and a custom LSTM model is used for grammatical correction
loss. Unfortunately, these models necessitate identical embeddings,
which may be less discernible with contemporary tokenizers, such
as SentencePiece [22].
Kirchenbauer et al. [23] presented a comprehensive strategy for
watermarking text generated by LLMs. Their approach involves cat-
egorizing the tokens as either ’green’ or ’red’ on the basis of a key
with the aim of getting the model to produce more ’green’ words.
This is achieved by assigning more weight to the ’green’ words be-
fore applying the softmax operation. Statistical tests are used to de-
tect the presence of a watermark. This method utilizes the function-
ing of transformers and applies watermarks during inference at min-
imal cost. The watermark is directly inserted during text generation.
However, token selection for promotion or avoidance is randomized
rather than using a contextually informed approach. Moreover, this
approach provides only watermark detection functionalities; it is not
able to integrate a message into the produced text. These limita-
tions motivated us to devise novel structures that enable watermark-
ing within the model directly.
3. PROPOSED METHODS
The proposed methods use a two-part system comprising a water-
mark embedder for watermarked text generation and a watermark
extractor for watermark verification.
During text generation, the
embedder processes a textual prompt and binary vector watermark
to produce watermarked text as output.
It does this by using a
pretrained LLM enhanced with a watermark module and then fine-
tuned. The purpose of this enhancement is to preserve the quality of
the generated text while adding the watermark.
The extractor module analyzes the input sentence and produces
a binary vector representing the extracted watermark. If the input
text does not include a watermark, the resulting vector is a sequence
of random bits, making it impossible to verify. However, a modi-
fied watermarked document must still yield an extracted watermark
Fig. 1. (a) Cross-attention mechanism for embedding watermark;
linguistic input can be embeddings or self-attention. (b) Watermark
layers, each with cross-attention and feedforward block, are place
between pretrained decoder layers.
that partly corresponds to the authentication bits or displays visi-
ble indications of identifying information. The process of informa-
tion retrieval using symbiotic embedding and recovery yields fewer
false positives. This approach is reliable even when handling hybrid
human-machine texts. The retrieval procedure for the watermark
has adaptable capabilities even if only a part of the watermark is
preserved.
3.1. Models
Our study of cross-attention draws inspiration from work on mul-
timodal models that use it to incorporate images [24, 25]. Chen
et al. [25] included cross-attention in pretrained layers, similar to
the encoder-decoder attention mechanism suggested in the original
transformer [26]. Alayrac et al. [24] introduced a layer with gated
cross-attention and feedforward connections between pretrained lay-
ers. These approaches focus on integrating images into textual mod-
els, which necessitates the handling of a great amount of visual in-
formation. In our scenario, only a few pertinent bits per sentence
are conveyed, so a lighter and more parameter-efficient solution is
needed. We use a linear layer to integrate the watermark into the
model dimension through cross-attention (Figure 1(a)). Similar to
multimodal models, the watermark embedding serves as the key and
value, while the linguistic input forms the query.
The gated cross attention layer approach is based on the work
of Alayrac et al. [24]. The linear layer is structured as follows: w is
the embedded watermark and α and β are parameters initialized to
zero to maintain the model output and ensure stability.
out = LayerNorm
 y + FeedForward(y) ∗tanh(β)

y = LayerNorm
 x + Attention(q = w, kv = x) ∗tanh(α)

We place gated layers before the decoder layers (Figure 1(b)). We
avoid introducing unnecessary parameters by limiting their use to
only the last N layers since the watermark has fewer bits than an im-
age. During training, we freeze the entire base model and optimize
only the cross-attention components.
The decoder layer substitution
approach involves taking a
pretrained model, either decoder-only or encoder-decoder, and re-
placing the last decoder layer with one that includes self-attention,
cross-attention, and feedforward connections. This new layer keeps
the original model’s positional embeddings, biases, etc. In the ini-
tial phase of training, we freeze the entire base model and only un-
freeze the output projection layer at convergence, thereby modify-
ing the original model. This step is necessary to avoid a drop in
performance, which enables the model to further reduce watermark-
ing loss. The decoder layer substitution approach is a cost-effective

strategy during inference and even if it were made open source, it
would still require fine-tuning to eliminate the watermarking layer.
The watermark extractor is based on a bidirectional transformer
model, which leverages its ability to capture contextual information
from both preceding and succeeding tokens. Following transformer
output, a pooling mechanism aggregates the information, creating
a compact representation of the extracted features. These pooled
features are passed through a multi-layer perceptron, and a binary
vector is extracted.
The use of a pretrained model (in our case stabilityai/stablelm-
base-alpha-7b from HuggingFace) requires the placement of con-
straints on the hyperparameters of the new decoder layer such as the
hidden and feedforward dimensions and the bit precision. This also
applies to the extractor since it relies on the same embeddings. For
a model with 7 billion parameters, the gated cross attention layer
approach adds a total of 720 million (3 layers) parameters, whereas
the decoder layer substitution approach adds 330 million parameters,
and the extractor has 1 billion.
3.2. Unambiguous Text Identification
During watermark verification, the text is examined to establish
whether it has been watermarked, i.e., generated by our model. This
can be achieved through the implementation of two methods, mes-
sage embedding and key checking, which have the advantages of
content insertion and robustness, respectively.
Message embedding transforms the provided message into a bi-
nary vector, which acts as a watermark. The watermark is divided
into multiple vectors that match the bit capacity of a single sentence.
During the generation phase, these segmented tensors are used se-
quentially to encode individual sentences. Recurrent bits are incor-
porated to enhance detection capabilities. During the verification
stage, these bits are extracted from each sequence and then concate-
nated to reconstruct the original vector. If the text does not have a
watermark, the bits are random. If the text has a watermark, the re-
sulting vector reflects the original message. If the watermarked text
is modified to remove the watermark, or in the case of hybrid human-
machine composition, remnants of the original message fragments
remain, providing evidence of their origin from our model. The
extractor uses a sigmoid activation function to provide confidence
levels for each bit. This is crucial when dealing with adversaries
who attempt to remove the watermark while preserving the original
text. By identifying confident bits as pivots, we can use heuristics,
depending on how the message is translated into bits, to search for
uncertain bits. This enables message reconstruction even in the pres-
ence of corruption.
Alternatively, if the focus is on robustness, the watermark can
act as a key. For ease of detection, the same binary vector can be
used for each sentence. We would encode the same vector in each
sentence, so whenever we extract one, it indicates that the text is
watermarked. This can be achieved following the approach of Ab-
delnabi and Fritz [21]. The null hypothesis (H0) attributes observed
matching bits to chance. This means that the behavior of matching
bits (random variable X) adheres to a binomial distribution with n as
the bit count, k as the matching count, and 0.5 as the success proba-
bility. The p-value, quantifying the likelihood of k or more matches
under H0, is
Pr(X > k|H0) =
n
X
i=k
 
n
i
!
0.5n
A watermark is verified if its p-value is below threshold T, indicat-
ing that the occurrence of this specific sequence by mere chance is
highly improbable. We thereby obtain a degree of confidence for
Fig. 2. High level representation of the training scheme
each sentence. The results can be represented in a graphical inter-
face, with color highlighting used to represent the likelihood of each
sentence being watermarked. We can thus obtain valuable insights
beyond simple binary classification. This approach is advantageous
for hybrid texts and scenarios in which the text has been modified to
remove the watermark.
3.3. Enhancing Robustness and Training Methodology
The training pipeline consists of several key steps. Initially, the in-
put tokens, in our case obtained from the OpenWebText dataset [27],
along with a randomly generated watermark of eight bits per sen-
tence, are fed into the watermarker. The watermark is incorporated
into the process through a cross-attention mechanism. The resulting
output is then utilized to compute the cross-entropy loss, which is
measured against the tokens shifted by one position. Subsequently,
the Gumbel-Softmax method, as proposed by Jang et al. [28], is used
to calculate the one-hot encodings. These encodings are multiplied
by the weights of the embedding layers. Finally, the transformed
data are passed through the remaining stages of the extractor, and
the resulting output is evaluated using binary cross entropy against
the original watermark.
To improve the robustness of the embedder, we should simu-
late an attack on the generated text prior to its use by the extractor.
However, operating at the textual level becomes impractical as con-
version between tokens and text disrupts the computational graph.
Consequently, in order to improve the robustness of the embedder,
a simulated attack which is able to be backpropagated is required.
We propose adding Gaussian noise into the extractor after the em-
bedding layer, with the aim of imitating synonym substitution on
random tokens for a portion of the sentences.
˜Et
s =
(
Et
s
if s ≤|S| ∗psent
Et
s + Bern(pword) ∗N(0, σ2)
otherwise
In practice, we sum the embeddings with Bernoulli-masked noise:
˜E = E + N(S,T,E)(0, σ2) ⊙Bern(S,)(psent) ⊙Bern(S,T )(pword)
where S, T, E, and p represent the sentences, tokens, embeddings,
and probabilities of modification, respectively. This approach en-
sures backpropagation and facilitates improvement of the embed-
der’s robustness.
Furthermore, we train intependently the extractor with water-
marked text as input, which is then subjected to a paraphrasing
model (tuner007/pegasus paraphrase from Huggingface). Subse-
quently, the loss is computed using the original watermark applied
during the input phase. This enhances the robustness of our system
against machine learning-based automatic attacks.
Regarding attacks, we use a baseline involving the application of
synonym substitution on various tokens using Word2Vec [18] as well
as the random addition, deletion, and substitution of tokens, each
with distinct probabilities. It is important to note that these mod-
ifications degrade the quality of the text, rendering it suboptimal.
Undertaking a complete rewrite of the text would require substan-
tial effort and result in minimal modifications to the content, thereby

Text Cross Entropy
Watermark Cross Entropy
AWT
2.96
0.12
Ours
2.37
0.11
Table 1. Training losses on test set with AWT and our method.
Augmentation technique
Clean
Baseline
Paraphrase
None
0.986
0.943
0.672
Noise
0.991
0.953
0.688
Paraphrase
0.982
0.940
0.764
Paraphrase+Noise
0.973
0.947
0.774
Table 2. Watermark reconstruction bit precision with decoder layer
substitution approach if text is not modified or attacked using either
baseline or paraphrase approach.
making it challenging to establish the threshold at which the water-
mark should still persist. Another baseline is evaluating the model’s
robustness against a paraphraser simulating a common attack that
any potential adversary could easily carry out using ChatGPT at a
minimal cost.
4. FINDINGS
Models: Comparing our approach to others presents challenges as
they either focus on detecting watermark presence [23] or on rewrit-
ing text [21] while, our method aims to reconstruct the original wa-
termark and seamlessly integrate it during text generation. Therefore
the performance of the cross-attention is compared with that of the
adversarial watermarking transformer (AWT) model in Table 1. We
adapted the training pipeline [21] by incorporating the watermark
with cross-attention in the final layer instead of summing to encoder
memory, obtaining slightly better results.
For both proposed models, the overall text performance was far
from competitive. Nevertheless, we obtained reliable watermarking
reconstruction performance with legible output. The gated cross at-
tention layer approach affected the text quality less since we did
not modify the original model. However, this constraint poses chal-
lenges during training because failure to slightly affect the original
model output can lead to total collapse. We were able to achieve
70% bit accuracy while maintaining text quality. The decoder layer
substitution approach produced more promising outcomes; unfor-
tunately, it considerably alters the original model and requires con-
siderable fine-tuning before it can be put into practice. It attained a
considerably higher level of bit precision and capacity (Fig. 3); how-
ever, the level was inversely proportional to the quality of the text. It
initially demonstrated satisfactory text quality but subsequently ex-
perienced pronounced deterioration. In particular, with regards to
greedy sampling, this deterioration became evident after 64–128 to-
kens. Although a sampling strategy mitigates this phenomenon, the
model remains unreliable for long outputs in its current form. More
research is required to address this problem, particularly as models
become capable of processing more tokens.
Augmentation Techniques:
Both noise injection and para-
phrasing have been shown to increase bit accuracy (Table 2). Since
both apply to the extractor, they do not have a negative effect on text
quality. Noise injection is cost-effective, supporting its integration.
In contrast, integrating paraphrasing can substantially increase the
computational load during training. Despite the prevalence of para-
phraser attacks in contemporary scenarios, it is necessary to explore
the benefits of augmentation. Furthermore, the extractor could be
trained later over attack examples without modifying the model.
Challenges:
Our study highlights a challenging tradeoff be-
tween the capability to create robust watermarks and the quality
of the generated text. Achieving high watermark robustness typ-
Fig. 3. Watermark reconstruction bit precision with decoder layer
substitution approach under various attacks.
ically comes at the expense of text quality, as exemplified by the
straightforward approach of substituting words with specific syn-
onyms, which are easily detectable, leading to poor quality output
lacking contextual coherence. Our experiments confirmed that strik-
ing the right balance between these two objectives is crucial.
Sentence entropy: When different watermarks are used with
identical prompts, the output is highly variable but still syntactically
correct and coherent with the prompt. With low entropy, i.e., the
first few words strongly dictate the following words, the output re-
mains consistent regardless of the watermark input. Nevertheless,
we noticed more pronounced effects of the watermark on the topic
of the generated text when using open-ended prompts. For instance,
the prompt ”The man was accused of plowing into a group of. . . ”
with various watermarks results in outputs related to protesters, peo-
ple, capitalists, workers, or even innocent horses (deterministic sam-
pling). However, this apparently does not impede the model’s ability
to provide rational responses. Interestingly, the fixed nature of a
very low entropy sentence makes it unlikely to contain a watermark,
which is a notable discovery.
5. CONCLUSION
We have presented an innovative approach to linguistic watermark-
ing for language models.
Specific information is imperceptibly
embedded into text during inference while preserving readability
and intended meaning.
The two proposed methods using cross-
attention minimize the effect of watermarking on the performance
of the pretrained model. Despite a lower overall performance of the
LLMs, the results are promising. A particularly intriguing technique
is the substitution of the last layer. The integration of multimodal
methods with efficient strategies [29] could enhance the potential
of this watermarking technique. We have also introduced two tech-
niques—adding noise and paraphrasing—that improve the resilience
of the system, inexpensively and against paraphraser attacks, respec-
tively. We have also introduced two ways of authenticating water-
marks, message embedding and use of a fixed watermark vector,
each with its own strengths and limitations.
This study sheds light on the potential of watermarking tech-
niques to shape the themes of the generated content, especially with
open-ended prompts, raising questions for AI alignment. A key in-
sight is the existence of a tradeoff between watermark robustness
and text quality.
Achieving a high degree of watermark robust-
ness often means compromising the quality of the generated text,
while preserving quality can challenge the resilience of the water-
mark. Finding the optimal balance between these two objectives
remains a critical area for further investigation and improvement. In
essence, embedding watermarks directly into large language mod-
els has the potential to balance output quality and cost-effectiveness.
This proactive approach of integrating watermarks directly during
model development has the potential to simplify the watermarking
process and ensure widespread adoption.

6. REFERENCES
[1] Nurul Shamimi Kamaruddin, Amirrudin Kamsin, Lip Yee Por,
and Hameedur Rahman, “A review of text watermarking: The-
ory, methods, and applications,”
IEEE Access, vol. 6, pp.
8011–8028, 2018.
[2] Josh A Goldstein, Girish Sastry, Micah Musser, Renee
DiResta, Matthew Gentzel, and Katerina Sedova,
“Gener-
ative language models and automated influence operations:
Emerging threats and potential mitigations,”
arXiv preprint
arXiv:2301.04246, 2023.
[3] Evan Crothers, Nathalie Japkowicz, and Herna Viktor, “Ma-
chine generated text: A comprehensive survey of threat mod-
els and detection methods,” arXiv preprint arXiv:2210.07321,
2022.
[4] Natali Helberger and Nicholas Diakopoulos, “Chatgpt and the
ai act,” Internet Policy Review, vol. 12, no. 1, 2023.
[5] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal,
Nicolas Papernot, and Ross Anderson, “The curse of recur-
sion: Training on generated data makes models forget,” arXiv
preprint arxiv:2305.17493, 2023.
[6] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubrama-
nian, Wenxiao Wang, and Soheil Feizi, “Can ai-generated text
be reliably detected?,” arXiv preprint arXiv:2303.11156, 2023.
[7] Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Ab-
dullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin
Javed, and Bimal Viswanath, “Deepfake text detection: Lim-
itations and opportunities,” arXiv preprint arXiv:2210.09421,
2022.
[8] Miranda Christ, Sam Gunn, and Or Zamir, “Undetectable wa-
termarks for language models,” Cryptology ePrint Archive, Pa-
per 2023/763, 2023.
[9] Mikhail J Atallah, Victor Raskin, Michael Crogan, Christian
Hempelmann, Florian Kerschbaum, Dina Mohamed, and San-
ket Naik,
“Natural language watermarking: Design, analy-
sis, and a proof-of-concept implementation,” in International
Workshop on Information Hiding. Springer, 2001, pp. 185–
200.
[10] Mikhail J Atallah, Victor Raskin, Christian F Hempelmann,
Mercan Karahan, Radu Sion, Umut Topkara, and Katrina E
Triezenberg,
“Natural language watermarking and tamper-
proofing,” in International Workshop on Information Hiding.
Springer, 2002, pp. 196–212.
[11] Umut Topkara, Mercan Topkara, and Mikhail J Atallah, “The
hiding virtues of ambiguity: quantifiably resilient watermark-
ing of natural language text through synonym substitutions,” in
Proceedings of the 8th workshop on Multimedia and security,
2006, pp. 164–174.
[12] Lingyun Xiang, Yan Li, Wei Hao, Peng Yang, and Xiaobo
Shen, “Reversible natural language watermarking using syn-
onym substitution and arithmetic coding.,” Computers, Mate-
rials & Continua, vol. 55, no. 3, 2018.
[13] Tina Fang, Martin Jaggi, and Katerina Argyraki,
“Gen-
erating steganographic text with lstms,”
arXiv preprint
arXiv:1705.10742, 2017.
[14] Xi Yang, Jie Zhang, Kejiang Chen, Weiming Zhang, Zehua
Ma, Feng Wang, and Nenghai Yu, “Tracing text provenance
via context-aware lexical substitution,” in Proceedings of the
AAAI Conference on Artificial Intelligence, 2022, vol. 36, pp.
11613–11621.
[15] Jipeng Qiang, Shiyu Zhu, Yun Li, Yi Zhu, Yunhao Yuan,
and Xindong Wu,
“Natural language watermarking via
paraphraser-based lexical substitution,” Artificial Intelligence,
vol. 317, pp. 103859, 2023.
[16] Nils Reimers and Iryna Gurevych, “Sentence-bert: Sentence
embeddings using siamese bert-networks,”
arXiv preprint
arXiv:1908.10084, 2019.
[17] Travis Munyer and Xin Zhong, “Deeptextmark: Deep learning
based text watermarking for detection of large language model
generated text,” 2023.
[18] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean,
“Efficient estimation of word representations in vector space,”
arXiv preprint arXiv:1301.3781, 2013.
[19] KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak,
“Robust natural language watermarking through invariant fea-
tures,” arXiv preprint arXiv:2305.01904, 2023.
[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova,
“Bert:
Pre-training of deep bidirectional
transformers for language understanding,”
arXiv preprint
arXiv:1810.04805, 2018.
[21] Sahar Abdelnabi and Mario Fritz, “Adversarial watermarking
transformer: Towards tracing text provenance with data hid-
ing,” 2021.
[22] Taku Kudo and John Richardson, “Sentencepiece: A simple
and language independent subword tokenizer and detokenizer
for neural text processing,” arXiv preprint arXiv:1808.06226,
2018.
[23] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz,
Ian Miers, and Tom Goldstein, “A watermark for large lan-
guage models,” 2023.
[24] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al., “Flamingo: a
visual language model for few-shot learning,”
Advances in
Neural Information Processing Systems, vol. 35, pp. 23716–
23736, 2022.
[25] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed El-
hoseiny,
“Visualgpt: Data-efficient adaptation of pretrained
language models for image captioning,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, 2022, pp. 18030–18040.
[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin, “Attention is all you need,” Advances in neural
information processing systems, vol. 30, 2017.
[27] Gokaslan, Cohen, Pavlick, and Tellex, “Openwebtext corpus,”
2019.
[28] Eric Jang, Shixiang Gu, and Ben Poole,
“Categorical
reparameterization with gumbel-softmax,”
arXiv preprint
arXiv:1611.01144, 2016.
[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, “LoRA:
Low-rank adaptation of large language models,” in Interna-
tional Conference on Learning Representations, 2022.
