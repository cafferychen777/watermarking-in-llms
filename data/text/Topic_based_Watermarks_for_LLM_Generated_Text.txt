Topic-Based Watermarks for LLM-Generated Text
ALEXANDER NEMECEK, YUZHOU JIANG, and ERMAN AYDAY, Case Western Reserve University, USA
The indistinguishability of text generated by large language models (LLMs) from human-generated text poses significant challenges.
Watermarking algorithms are potential solutions by embedding detectable signatures within LLM-generated outputs. However, current
watermarking schemes lack robustness to a range of attacks such as text substitution or manipulation, undermining their reliability. This
paper proposes a novel topic-based watermarking algorithm for LLMs, designed to enhance the robustness of watermarking in LLMs.
Our approach leverages the topics extracted from input prompts or outputs of non-watermarked LLMs in the generation process of
watermarked text. We dynamically utilize token lists on identified topics and adjust token sampling weights accordingly. By using these
topic-specific token biases, we embed a topic-sensitive watermarking into the generated text. We outline the theoretical framework
of our topic-based watermarking algorithm and discuss its potential advantages in various scenarios. Additionally, we explore a
comprehensive range of attacks against watermarking algorithms, including discrete alterations, paraphrasing, and tokenizations.
We demonstrate that our proposed watermarking scheme classifies various watermarked text topics with 99.99% confidence and
outperforms existing algorithms in terms of z-score robustness and the feasibility of modeling text degradation by potential attackers,
while considering the trade-offs between the benefits and losses of watermarking LLM-generated text.
Additional Key Words and Phrases: Large Language Models (LLMs), Natural Language Processing (NLP), Watermarking
1
INTRODUCTION
The rapid expansion of artificial intelligence (AI) and natural language processing (NLP) has led to the development of
large language models (LLMs), such as OpenAI’s ChatGPT [15], which are capable of generating human-like text based
on extensive data training [17, 25]. While LLMs offer significant benefits, their growing usage raises concerns related to
potential misinformation, copyright infringement, and plagiarism [9, 18], resulting in the need for tools to differentiate
LLM- and human-generated content. Watermarking algorithms have potential to provide solutions to address these
concerns by embedding identifiable patterns in LLM-generated text [21]. Current watermarking schemes adjust the
generated output token (i.e., units of text used in NLP analysis, such as words, phrases, or characters as elements for
processing in NLP tasks) distributions by sampling tokens into “green” 𝐺and “red” 𝑅lists, enhancing the likelihood
of 𝐺tokens in the LLM output to create a detectable watermark [10, 26]. Detection is determined by comparing the
frequency of 𝑅tokens in a text sequence with human-generated text expected to contain significantly more 𝑅tokens
then LLM-generated text.
However, existing watermarking algorithms lack robustness, particularly against attack models involving text
insertion, manipulation, substitution, and deletion, which can disrupt watermark detection. To address these limitations,
we propose a topic-based watermarking algorithm that enhances robustness. By utilizing 𝐺and 𝑅lists based on specific
topics (e.g., sports, technology, music, etc.), our scheme allows for more enhanced targeted watermarking and detection.
This method assesses the effectiveness of various attack models on the watermark by analyzing changes in the text
topic(s).
In this paper, we introduce a novel topic-based watermarking scheme designed to address these limitations by pro-
viding robustness evaluations against common attacks. Our proposed scheme achieves 99.99% confidence in classifying
watermarked text across various topics. Comparative evaluations show that our method consistently achieves higher
z-scores than existing watermarking algorithms, particularly against baseline, discrete alterations, and tokenization
Authors’ address: Alexander Nemecek, ajn98@case.edu; Yuzhou Jiang, yxj466@case.edu; Erman Ayday, exa208@case.edu, Case Western Reserve
University, 10900 Euclid Ave., Cleveland, Ohio, USA, 44106-1715.
1
arXiv:2404.02138v3  [cs.CR]  19 Aug 2024

attacks, which allows for a higher z-score threshold. Additionally, our scheme accurately identifies paraphrased attacks
tailored to the correct topic, while existing algorithms often misclassify such texts as human-generated, demonstrating
our advancements in detecting subtle manipulations.
2
RELATED WORK
The development of prominent LLMs, built on transformer architectures with attention mechanisms [23], allows these
models to generate coherent text by predicting the next token in a sequence based on prior tokens [3]. The generation
process relies on probabilistic calculations from logits, which represent raw, unnormalized outputs from the LLMs final
layer [2].
Watermarking, a technique which involves embedding hidden markers or patterns within data, traditionally used
to assert copyright and verify authenticity in various fields [6–8], has been adapted to LLMs, where the watermark
embeds the hidden patterns within generated text to distinguish between human- and LLM-generated content. The
first watermarking schemes for LLMs, such as the probabilistic 𝐺and 𝑅lists proposed by Kirchenbauer et al. [10],
enable detection by comparing the presence of 𝑅tokens in a text sequence. However, these methods are challenged
with limitation concerning text manipulation attacks.
2.1
Extensions of Watermarking
To address the issues of early watermarking schemes, recent research has lead to enhanced robustness of these algorithms
against various attacks [1, 11–13, 20, 22, 26]. Kirchenbauer et al. [10], extends their initial watermarking approach by
incorporating the addition of one or more secret keys during token sampling. The addition of keys generate unique
tokens using a pseudorandom function, improving resistance to manipulation targeting 𝑅tokens. Furthermore, Liu
et al. [13] proposed a watermarking algorithm to enhance attack robustness against semantically invariant perturbations
(e.g., text modifications, text alterations), by embedding watermarks based on the semantics of the entire preceding text
instead of a fixed number of prior tokens.
2.2
Black-Box Detection
Prior research has also assumed black-box detection approaches, where the internal workings of the LLM are undisclosed
to the users. However, these methods have limitations such as data biases in LLM training which can impact performance
and limit generalizability across different contexts. Additionally, confidence calibration can affect the reliability and
trustworthiness of watermark detection, leading to issues such as false positive or negatives. The lack of adaptability in
these LLMs (black-boxed) limits their ability to improve over time and respond to advancing threat models [22].
Our proposed watermarking methodology focuses on addressing these challenges and limitations by enhancing
robustness, specifically by our approach to store 𝐺and 𝑅list pairs tailored to the specific topics of the input text
sequence. This adaptation ensures the watermark is contextually relevant and difficult to remove, preserving text
quality and increases security against attacks, offering a practical solution for watermarking LLM-generated text.
3
PROPOSED METHOD
This section provides an overview of the proposed watermarking scheme and other aspects of the model to be considered.
First, in Section 3.1, we discuss different relevant attack models. In Section 3.2, we introduce the basic highlights of our
proposed watermarking framework while comparing it to the work of Kirchenbauer et al. [10]. Finally, in Section 3.3,
we discuss our watermark detection scheme to differentiate between human- and LLM-generated output.
2

3.1
Threat Model
Embedded watermarks in an LLM’s output are subject to various types of attacks. All threat models consider the
distortion or removal of the watermark from a target text sequence (i.e., LLM-generated output) as the motivation of an
attacker to avoid detection. However, distorting the target text will degrade the overall quality of the text, hence the
overall topic as well.
Potential tools of an attacker include text insertion, substitution, and deletion in the generated output text sequence at
the character, word, sentence, and multi-model (combination of characters, words, or sentences) level [5, 10]. Insertion
is the addition or insertion of tokens into the text sequence. With the watermarked text encompassing significantly more
𝐺listed tokens, the addition of new tokens may violate the 𝑅list, increasing the count of 𝑅listed tokens with respect
to the 𝐺listed tokens. However, the attacker will need to insert a significant amount of 𝑅listed tokens to decrease
detection power of the watermark to classify the text as human-generated. The scenario of insertion attacks is realistic,
but the fact that an attacker would need to manually generate the same or more content than the LLM-generated
output is unrealistic, hence we expect a reasonable amount of insertion (i.e., manually generated tokens) as a part of
the insertion attack. Substitution of tokens involve swapping tokens in the generated output text. For example, an
attacker may substitute tokens in the text sequence with their respective synonym. The motivation of the attacker in
an instance of substitution is to remove 𝐺listed tokens and add 𝑅listed tokens to degrade or remove the watermark.
Deletion involves deleting tokens from a generated output sequence. If a significant amount of 𝐺listed tokens are
deleted, the amount of 𝑅tokens would increase. However, if only tokens are deleted from a text sequence, there exists a
potential trade-off between the text quality and detection accuracy. Similar to insertion, we also expect a reasonable
level of deletion in a rational attack scenario. Token insertion, substitution, deletion, and manipulation are used in
tandem to distort or remove the watermark of an LLM-generated output. An attacker can use a combination of these
aforementioned tools to generate its attack against the watermark. In the rest of this section, we discuss considered
threat models an attacker can perform against the LLM in our evaluation.
3.1.1
Baseline Attack. Baseline attack consists of the insertion, substitution, and deletion of the target text. The attacker
selects a single or a combination of techniques (insertion, substitution, and deletion) with the objective to diminish
detection accuracy. In doing so, this approach may lead to a compromise in the text quality of the manipulated target
sequence.
3.1.2
Paraphrasing Attack. A paraphrasing attack is a category of baseline substitution attacks, executed in two ways:
manually by an individual or via an LLM, expressing the same meaning of the target text, using different words.
• Manual Paraphrasing: Paraphrasing is conducted manually by the attacker without the aid of external tools.
This manual approach is considered for shorter sequences of text, as longer sequences would necessitate more
extensive intervention from the attacker. The proposed topic-based watermark is particularly effective against
manual paraphrasing, which would require significant modification to mislead the detection mechanism into
incorrectly classifying the LLM-generated text as human-generated. The robustness of the proposed watermark
is further enhanced by potential deterioration in text quality and the possible change in the overall topic(s) of
the paraphrased text.
• LLM Paraphrasing: The attacker generates a watermarked text sequence by feeding the LLM an input prompt.
The attacker does not manually alter any tokens in the output. Instead, the attacker subsequently supplies the
watermarked output into the identical LLM which originally produced it, along with a paraphrase command
3

(i.e., “Rephrase this text:”). The proposed watermarking scheme is robust against paraphrasing using the original
LLM due to the nature of topics that form the basis of the watermarked output. For instance, if the attacker’s
initial prompt leads to an output with the topic of “sports”, any paraphrased output would maintain the same
topic, ensuring the watermark can still be generated and detected.
3.1.3
Tokenization Attack. A tokenization attack is classified as a form of insertion where a single token is modified
into multiple subsequent tokens. For example, given the token “sports” within a 𝐺list of usable tokens. An attacker can
insert characters such as (‘_’) or (‘*’) within the token, causing it to split into multiple sub-tokens. The original 𝐺-listed
token “sports” could be manipulated into “s_p_o_r_t_s”, generating multiple new 𝑅-listed tokens in the text sequence.
The robustness of the proposed topic-based watermarking scheme is challenged by the degradation in text quality due
to the intentional insertion of noise (unwanted characters) within tokens in the 𝐺list.
3.1.4
Discrete Alterations. An attacker’s motivation behind this attack is to induce alterations in tokens, subsequently
introducing misspelling and grammar errors. These manipulations can be executed through insertion or deletion of
single or multiple characters, or entire stings of characters within the target text. We consider an attacker which
seeks to undermine the integrity of a watermarked text sequence without completely altering the text. By inserting or
deleting characters within certain tokens, the attacker can modify the text to introduce minor errors which impact the
detectability of the watermark. For example, altering the word “watermarked” to “wat3rmark3d” through character
substitution, introducing misspelling by complicating the process of watermark recognition. The proposed topic-based
watermark addresses current watermarking algorithm limitations through the implementation of the generalized topic
scheme. Even in the presence of minor spelling or grammatical errors, the overarching topic of the output remains
unaffected. This ensures that the watermark’s integrity is maintained, showcasing the robustness of the proposed
approach against textual alterations.
3.2
Proposed Watermarking Scheme
This section describes the proposed watermarking scheme which leverages topic extractions from either a non-
watermarked LLM output or directly from the user’s input prompt. Our method focuses on generating robust, topic-
based 𝐺and 𝑅lists to create the watermarked outputs. By employing these lists, the scheme ensures both the accuracy
of the extracted topic(s) and the watermark against malicious manipulation of the target text. The following outlines
the three main steps of the watermarking process including extraction of topics, 𝐺and 𝑅list generation, and the
watermarking of the output text itself.
3.2.1
Topic Extraction from Non-Watermarked Output. The proposed method for topic extraction employs a non-
watermarked LLM output. This process initiates with the user providing an input prompt to the LLM, which then
generates a non-watermarked output text sequence. This output serves as the basis for extracting topics relevant to the
user’s input, as described in Figure 1.
Furthermore, the non-watermarked output is fed back into the LLM along with a command to rephrase the text, such
as “rephrase this text.” Both the original non-watermarked output and the adjustments in token weights, derived from
the extracted topics, are used within the watermarked LLM to generate a watermark on the final output text sequence.
A secret key is employed at this stage to influence the creation of the 𝐺and 𝑅list pairs. These lists are generated using
on a pseudorandom function that incorporates a secret key, ensuring the watermarking scheme remains unpredictable
and more robust against unauthorized detection.
4

Message LLM...
Token Weight
Adjustment
Non-watermarked
LLM
Extracted Topics
Output Template
Watermarked 
LLM Output
Topic Prediction
Model
Watermarked LLM
Rephrase Command
Fig. 1. Overview of the topic-based watermarking scheme using non-watermarked LLM outputs. The input prompt is passed to the
non-watermarked LLM to extract the topic(s) of interest. Utilizing the topic(s), token weights are adjusted for the watermarked LLM
with the rephrased original non-watermarked output.
The utilization of the non-watermarked output over the input text sequence helps extract accurate topics when
(i) the input prompt is brief, consisting of only a few word to a sentence in length, or (ii) when the input contains
misspellings and grammatical errors. The non-watermarked LLM output provides a sufficient amount of quality text,
enabling the accurate identification of topics which align with the user’s input prompt. Additionally, the watermarking
scheme can extract multiple topics from a single input sequence. For instance, if the input asks about medical injuries
in various types of sports, the non-watermarked output might generalize these topics as “sports” and “medical injuries.”
However, to maintain model performance and topic extraction quality, given a large non-watermarked output (e.g.,
multiple paragraphs of text), the topic extraction process may iterate over all sentences to analyze for relevant topics
and skip those not strongly related to specified topics.
The extracted topic or combination of topics is then used to generate seeds and 𝐺and 𝑅list pairs, based on the
number of topics identified. The 𝐺and 𝑅list pairs are then used to adjust the token weights for the watermarked LLM,
in combination with the original non-watermarked LLM-generated output that was used for topic extraction.
Finally, the watermarked LLM utilizes the non-watermarked output text sequence with the rephrase command, to cor-
rectly sample from the generated 𝐺list for each topic. Similar to the watermarking algorithm proposed by Kirchenbauer
et al. [10], we sample tokens from the 𝐺list to generate an output sequence from the original input prompt.
3.2.2
Topic Extraction from Input Text Sequence. In a similar process, we consider an alternative method for topic
extraction, which involves extracting topics directly from the user’s input prompt. This approach is feasible when the
input prompt is free from spelling and grammatical errors and provides enough content to accurately identify and
extract multiple topics. In this scenario, the non-watermarked LLM is bypassed, and the input prompt serves as the
direct input for the watermarked LLM. The input prompt, along with the adjustments in token weights derived from
the extracted topics, is then utilized within the watermarked LLM to produce a watermarked output text sequence.
The method for generating 𝐺and 𝑅list pairs based on the extracted topics remains consistent across both the
non-watermarked and input prompt-based topic extraction approaches, with the primary difference being the source of
5

the topics. In our evaluation, we focus on the non-watermarked scenario, operating under the assumption that if a user
is capable of providing a sufficiently detailed input, they would not need to avoid the watermarking system.
3.3
Watermarking Detection
Existing watermarking algorithms that rely on the 𝐺and 𝑅list pair methodology focus on the occurrence of 𝐺and 𝑅
tokens within the target text (human- or LLM-generated output text). The detection method evaluates the frequency of
𝑅list violations within the target text. Human-generated text typically contains a higher frequency of 𝑅listed tokens,
resulting in fewer instances of 𝐺listed tokens. Conversely, watermarked LLM-generated text exhibits significantly more
𝐺listed tokens over 𝑅listed tokens. Kirchenbauer et al. [10] demonstrated the low probability of a human-generated
text sequence aligning with 𝐺listed tokens in the same way as a watermarked LLM-generated output.
Target Text
(watermarked LLM
output)
Topic Prediction
Model
Watermarked LLM
VS.
Human
Extracted Topics
(G, R)
(G, R)
(G, R)
Fig. 2. Detection mechanism of topic-based watermarking scheme. Token list pairs, generated from the extracted topics of the
non-watermarked LLM output, are compared to the output text sequence of the watermarked LLM to determine the classification of
the target text as human- or LLM-generated.
The proposed topic-based detection framework, illustrated in Figure 2, builds on the 𝐺and 𝑅token list pair approach
but introduces a novel mechanism classifying text as either human- or LLM-generated via topic extraction. This method
involves extracting and analyzing the topics present in the non-watermarked LLM output sequence. The detection
process creates specific 𝐺and 𝑅list pairs for each identified topic, enabling the detection mechanism to iterate through
all list pairs per topic for accurate classification. Upon receiving a non-watermarked LLM output sequence, the detection
framework identifies one or more topics present in the input text utilizing topic modeling techniques such as Latent
Dirichlet Allocation (LDA) or LLM prompting (e.g., instructing a model to “find 𝑛topics in this text”) [14]. For each
identified topic, the framework generates corresponding 𝐺and 𝑅lists and quantifies the occurrence of tokens from
these lists within the text. Based on the distribution of 𝐺and 𝑅tokens across multiple topics within the input text
sequence, hypothesis testing is employed to classify the target text. The specific statistical tests used depend on the
distribution characteristics of the token occurrences, aiming to evaluate the confidence level in determining whether
the target text is human- or LLM-generated.
Kirchenbauer et al. [10] utilized a “one portion z-test” for evaluating the 𝐺list of tokens, with the total number of
tokens in the text 𝑇, the count of 𝐺tokens in the text, and expected distribution of 𝐺tokens 𝛾. The overall z-score is
denoted as:
𝑧_𝑠𝑐𝑜𝑟𝑒=
𝐺−𝑇
𝛾
√︃
𝑇
𝛾∗(1 −𝛾)
(1)
6

This test will be evaluated in our proposed detection scheme. The topic-based approach allows for more generalized
analysis of the target text, which accommodates the multi-topic text sequences and provides a workflow to ensure the
accuracy and robustness of the watermarking scheme.
4
EXPERIMENTS
4.1
Experimental Setup
4.1.1
Evaluation. We employ the z-score, as defined by Equation 1, to evaluate the degradation rate of the watermark for
baseline attacks, discrete alterations, and tokenization attacks, assessing whether it falls below the threshold classified
from “watermarked” to “human-generated.” For paraphrasing attacks, the z-score is also utilized to determine the
effectiveness of the watermark after complete rephrasing. Our evaluations compare our topic-based watermarking
scheme against the original algorithm proposed by Kirchenbauer et al. [10]. In both cases, the total number of tokens in
the watermarked text 𝑇remains constant at 199 tokens. According to the experiments of the original watermarking
algorithm [10], the parameters are set with 𝛾= 0.25 and bias from 𝐺at 2.0. In our scheme, the bias remains fixed (2.0),
but 𝛾is adjusted based on the total number of tokens in the evaluated LLM vocabulary 𝑉and the number of pre-defined
topic mappings 𝑀. The adjustment of 𝛾varies to ensure equal token allocation per topic for the given 𝑀. 𝛾is calculated
as:
𝛾= 𝑉
𝑀
(2)
For our experiments, we have defined five topics, each assigned an equal number of tokens from the entire LLM
vocabulary 𝑉.
Table 1. Prompt Lengths for Each Topic
Topic
Prompt Length
Sports
229
Technology
212
Animals
241
Music
184
Medicine
188
4.1.2
Watermarking for Confidence. To evaluate the general efficacy of our watermarking scheme, we tested water-
marking capabilities across specific topics. We assessed prompts related to main detected topics and their respective
word lengths described in Table 1. We utilize the open-source facebook/opt-1.3b model [24], imposing a generation
constraint of 200 additional tokens beyond the main context. The same model is also employed to determine the overall
topic of the input prompt to ensure consistency in the generation and detection phases of our watermarking scheme.
We measured the performance of our scheme by confidence 𝐶, the difference between 1.0 and the specified p-value if
the z-score exceeds the established z-threshold of 4.0 described as:
𝐶=
n
1.0 −p-value,
if z-score > z-threshold
(3)
4.1.3
Watermarking for Attack Methods. In continuation on our evaluation of watermarking confidence, we assume that
the user provides sufficient text to identify a topic. Using “sports” as an example, we start with the initial prompt length
of 229 words and add a constraint of 200 additional tokens for generation. We assessed both a random watermarking
scenario, where the attacker is unaware of the watermark’s existence, and an inferred watermarking scenario, where
7

the attacker is aware that the LLM has a watermarking scheme. These evaluations cover the various attack methods of
our threat model (see Section 3.1):
• Baseline Attacks: We evaluated the impact of increasing modifications through insertion, deletion, substitution,
and combinations of these techniques, focusing on whether key words such as nouns, verbs, and adjectives are
modified. The number of modifications range from 0 to 50 (25% of the total output).
• Discrete Alterations: These modifications involve the addition of random lowercase letters (e.g., a-z), under
both random and inferred watermarking scenarios. To maintain consistency with the evaluation of baseline
attacks, discrete alterations are assessed with modifications ranging from 0 to 50 over the entire output.
• Tokenization Attacks: Under the same scenarios as baseline attacks and discrete alterations, we consider
tokenization examples such as “s_p_o_r_t_s” for tokenization.
• Paraphrasing Attacks: The effectiveness of generalized rephrasing commands, such as “rephrase this text” is
analyzed, along with the impact of correctly and incorrectly detected topic rephrasing commands (e.g.,“rephrase
this text tailored to the topic of ‘sports”’). The possible scenarios include:
* Generic rephrase command.
* A rephrase command tailoring to the correctly defined detected topic.
* A rephrase command tailoring to a relevant but different detected topic (e.g., inferring “health” instead of
“sports”).
* A rephrase command tailoring to an incorrectly detected topic (e.g., misidentifying “sports” as “technology”).
To determine the most robust watermarking method between the original and our topic-based approach, we set
the z-threshold to 2.5 to evaluate where each method succeeds or fails with confidence.
4.2
Experiment Results
4.2.1
Topic-Based Watermark Confidence. Table 2 details the z-scores and detection confidence from the watermarked
text given a specified topic, utilizing a z-score detection threshold of 4.0. Our topic-based watermarking scheme
significantly exceeds this threshold, achieving high confidence rates of approximately 99.99%. This higher z-score allows
for an elevated detection threshold, reducing the number of false positives (i.e., texts misclassified as human-generated
when they are watermarked).
Prompt Topic
Tokens Counted
# Tokens in G
Tokens in G(%)
z-score
p-value
Confidence
Sports
199
157
78.9%
11.30
9.18e-30
∼99.99%
Technology
199
146
73.4%
9.68
1.88e-22
∼99.99%
Animals
199
146
73.4%
9.68
1.88e-22
∼99.99%
Music
199
144
72.4%
9.39
3.07e-21
∼99.99%
Medicine
199
140
70.4%
8.81
6.35e-19
∼99.99%
Table 2. Confidence levels of our topic-based watermarking scheme for various detected topics with a z-score detection threshold of
4.0.
4.2.2
Baseline, Discrete Alterations, and Tokenization Attacks. Figure 3 compares our topic-based watermarking scheme
against the original algorithm proposed by Kirchenbauer et al. [10] across baseline attacks, discrete alterations, and
tokenization attacks. The evaluation of baseline attacks is illustrated in Figure 3(a) for our topic-based watermarking
scheme, and in 3(d) for the original scheme. Our proposed approach demonstrates an increase of 4.0 in z-scores compared
8

(a) Proposed Baseline
(b) Proposed Discrete Alterations
(c) Proposed Tokenization
(d) Original Baseline
(e) Original Discrete Alterations
(f) Original Tokenization
Fig. 3. Z-score plots showing the impact of varying percentages of text modifications (from 0% to 25%) on attack detection for baseline
attacks, discrete alterations, and tokenization attacks, comparing the topic-based (proposed) watermarking method with the original
scheme. The topic-based method consistently achieves significantly higher z-scores across all attack types, highlighting its robustness
compared to the original watermark.
to the original. A similar improvement is observed for discrete alterations, as shown in Figures 3(b) and 3(e) for the
topic-based and original schemes, respectively.
The tokenization attack evaluations, shown in Figures 3(c) and 3(f), also show an increase in z-score for our model.
However, the maximum z-score obtained is slightly lower than those for baseline attacks and discrete alterations.
While our approach maintains robustness against tokenization attacks as the percentage of modifications increase, the
comparatively lower z-score suggests that our scheme may be more susceptible to tokenization attacks than to baseline
and discrete alterations.
Overall, our scheme consistently achieves higher z-scores across all direct manipulation threat models, allowing
for the use of a higher z-score detection threshold to reduce the false positive rate in classification. As the percentage
of modifications to the watermarked text increases, the z-score naturally decreases. We restrict our evaluation to
modifications up to 25% because beyond this point, the cost of the attack in terms of text degradation becomes
prohibitive, where the more modifications that are made to coherent text, the less intelligible it becomes [10].
4.2.3
Paraphrasing Attacks. We further present a comparison in Table 3 of paraphrasing attacks using various prompts,
both generic and tailored to specific topics, at a z-score detection threshold of 2.5. Both watermarking schemes incorrectly
classify texts under the generic and inferred watermarking scenarios with incorrect topic rephrasing. However, it is
unlikely that an attacker would want to change the text topic this significantly, as this could degrade its relevance,
illustrated by a hypothetical scenario where a student must write a paper on “sports” and incorrectly prompts the
LLM to change the topic. This risk extends to the generic rephrasing process where the topic is more likely to shift.
9

Paraphrasing Type
𝐺/𝑇(%)
z-score
p-value
Confidence
Proposed
Original
Proposed
Original
Proposed
Original
Proposed
Original
Unmodified Text
78.9%
47.2%
11.30
7.24
9.18e-30
2.18e-13
∼99.99%
∼99.99%
Generic
50.0%
30.0%
1.98
1.1
0.0239
0.137
Human
Human
Infer Correct Topic
53.6%
34.5%
2.58
2.02
4.93e-03
0.0363
∼99.51%
Human
Infer Similar Topic
54.5%
38.6%
2.83
2.95
2.34e-03
0.0205
∼99.77%
∼97.95%
Infer Incorrect Topic
46.2%
29.0%
1.27
0.898
0.102
4.25e-03
Human
Human
Table 3. Detection results against paraphrasing attacks for the topic-based (proposed) and original watermarking schemes, with a
z-score detection threshold of 2.5. 𝐺/𝑇(%) denotes the ratio of 𝐺tokens given the number of tokens in the watermarked text 𝑇as a
percentage. “Human” denotes a misclassification as human-generated, with no confidence. Our topic-based scheme shows higher
z-scores and confidence, especially in detecting correct and similar topic rephrasing, proving more robust than the original scheme.
Our model robustly handles correct topic rephrasing and similar topic rephrasing, whereas the original watermarking
scheme only succeeds effectively with similar topic rephrasing.
5
LIMITATIONS AND FUTURE WORK
5.0.1
Trade-offs in List Pair Granularity. The proposed watermarking scheme is designed to facilitate the generation of
𝐺and 𝑅list pairs, dependent on the number of categories allocated for the topic-based watermarking. The anticipated
trade-off to consider is between the granularity of these list pairs and the quality of the LLM’s output. For instance, if
an input prompt is categorized under “sports” and is without a comprehensive list for that specific topic, the quality of
the LLM’s output may be compromised. The refinement of this trade-off, while beneficial for enhancing the specificity
of the text output, it also increases the model’s sensitivity to the accuracy of topic prediction. This sensitivity could
potentially lower the threshold for adversaries to exploit the LLM, allowing them to bypass the watermarking scheme or
produce false output sequences. The limitation lies in achieving an optimal balance that maintains the integrity of the
watermarking framework while mitigating the risk of manipulation, which could compromise the model’s effectiveness.
5.0.2
Vulnerabilities to Spoofing Attacks. It is important to acknowledge the potential threat posed by spoofing attacks,
which can significantly impact the privacy and security of watermarked LLMs. Spoofing attacks deceive either the LLM
or its users into believing that a given text output originates from a different source (i.e., a different LLM). Previous
research has highlighted the susceptibility of LLMs to such attacks. For instance, Sadasivan et al. [19], explored how
attackers could generate misleading or harmful outputs by deducing text signatures without access to the detection
mechanisms. Similarly, Pang et al. [16] examined how attackers could fabricate outputs that falsely appear watermarked,
potentially damaging the reputation the LLM or its developers.
5.0.3
Limitations with Paraphrasing and Generative Attacks. The robustness of our proposed topic-based watermarking
scheme against certain attacks remains a notable limitation [10, 16]. Specifically, the scheme is vulnerable to more
complex paraphrasing attacks, such as Multi-Model Collusion Attacks, where the attacker solicits outputs to the same
prompt from several LLMs, aiming to blend multiple responses into a single output. While the watermarking scheme
maintains robustness against paraphrasing attacks that use correct topics, the use of multiple LLMs for paraphrasing
increases its complexity. The current scheme only supports one model during both the generation and detection phases,
which limits its effectiveness.
Additionally, our scheme is susceptible to generative attacks, where the attacker prompts the model to alter its
output in a predictable but reversible way, corrupting the 𝐺and 𝑅lists by randomizing subsequent token associations.
10

An example of a generative attack is the emoji attack [4], where an LLM is prompted to generate emojis in place of
whitespaces. Once generation is complete, the emojis are replaced with whitespaces, and if fed into the watermarked
detection scheme, we anticipate an incorrect classification of human-generated text due to the randomization of tokens
associations for the 𝐺and 𝑅lists.
6
CONCLUSION
In this work, we propose a novel topic-based watermarking scheme that utilizes pairs of “green” and “red” lists generated
based on the topic of the input text prompt for an LLM. This method was specifically developed to address and overcome
the shortcomings of existing watermarking algorithms, notably their lack of robustness against known threat models.
Our results demonstrate that our scheme surpasses current watermarking algorithms by achieving higher z-score
detection thresholds, which significantly decreases the rate of false positives in differentiating between human-generated
and LLM-generated texts. Our model has proven robust against a variety of known attacks, making it a substantial
improvement over traditional methods, however future work will focus on further expanding the robustness of our
watermarking scheme. We aim to adapt and refine our methodology to counter more complex attacks and to enhance the
granularity of topics. With the strides generative AI has taken in recent years, by continuing to enhance watermarking
capabilities, we can ensure that LLMs can be utilized safely and reliably across different domains and applications.
REFERENCES
[1] Miranda Christ, Sam Gunn, and Or Zamir. 2023. Undetectable Watermarks for Language Models. arXiv preprint arXiv:2306.09194 (2023). https:
//doi.org/10.48550/arXiv.2306.09194
[2] Saeed Dehqan. 2020. Exploring Token Generation Strategies. https://www.packtpub.com/article-hub/exploring-token-generation-strategies
[3] Andrea Galassi, Marco Lippi, and Paolo Torroni. 2019. Attention, please! A Critical Review of Neural Attention Models in Natural Language
Processing. CoRR abs/1902.02181 (2019). arXiv:1902.02181 http://arxiv.org/abs/1902.02181
[4] Riley Goodside. 2023. There are adversarial attacks for that proposal as well — in particular, generating with emojis after words and then removing
them before submitting defeats it. https://x.com/goodside/status/1610682909647671306
[5] Shreya Goyal, Sumanth Doddapaneni, Mitesh M. Khapra, and Balaraman Ravindran. 2023. A Survey of Adversarial Defences and Robustness in
NLP. arXiv:2203.06414 [cs.CL] https://arxiv.org/abs/2203.06414
[6] Tianxi Ji, Erman Ayday, Emre Yilmaz, and Pan Li. 2022. Robust fingerprinting of genomic databases. Bioinformatics 38, Supplement 1 (06 2022),
i143–i152. https://doi.org/10.1093/bioinformatics/btac243
[7] Tianxi Ji, Erman Ayday, Emre Yilmaz, and Pan Li. 2023. Towards Robust Fingerprinting of Relational Databases by Mitigating Correlation Attacks.
IEEE Transactions on Dependable and Secure Computing 20, 4 (2023), 2939–2953. https://doi.org/10.1109/TDSC.2022.3191117
[8] Yuzhou Jiang, Emre Yilmaz, and Erman Ayday. 2022. Robust Fingerprint of Location Trajectories Under Differential Privacy. arXiv preprint
arXiv:2204.04792 (2022). https://doi.org/10.48550/arXiv.2204.04792
[9] Dinesh Kalla and Sivaraju Kuraku. 2023. Advantages, Disadvantages and Risks associated with ChatGPT and AI on Cybersecurity. (10 2023).
https://doi.org/10.6084/m9.jetir.JETIR2310612
[10] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2024. A Watermark for Large Language Models.
arXiv:2301.10226 [cs.LG] https://arxiv.org/abs/2301.10226
[11] Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, and Li Yaliang. 2024. Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning. arXiv
preprint arXiv:2402.14883 (2024). https://doi.org/10.48550/arXiv.2402.14883
[12] Aiwei Liu, Leyi Pan, Xuming Hu, Shu’ang Li, Lijie Wen, Irwin King, and Philip S. Yu. 2023. An Unforgeable Publicly Verifiable Watermark for Large
Language Models. arXiv preprint arXiv:2307.16230 (2023). https://doi.org/10.48550/arXiv.2307.16230
[13] Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. 2024. A Semantic Invariant Robust Watermark for Large Language Models. arXiv
preprint arXiv:2310.06356 (2024). https://doi.org/10.48550/arXiv.2310.06356
[14] Madhurima Nath. 2023. Topic modeling algorithms. Medium (2023). https://medium.com/@m.nath/topic-modeling-algorithms-b7f97cec6005#:~:
text=The%20most%20established%20go%2Dto,model%20which%20uses%20matrix%20factorization.
[15] OpenAI. 2024. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt/
[16] Qi Pang, Shengyuan Hu, Wenting Zheng, and Virginia Smith. 2024. Attacking LLM Watermarks by Exploiting Their Strengths. arXiv preprint
arXiv:2402.16187 (2024). https://doi.org/10.48550/arXiv.2402.16187
11

[17] Partha P. Ray. 2023. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope.
Internet of Things and Cyber-Physical Systems 3 (2023), 121–154. https://doi.org/10.1016/j.iotcps.2023.04.003
[18] Matthias C. Rillig, Marlene Ågerstrand, Mohan Bi, Kenneth A. Gould, and Uli Sauerland. 2023. Risks and Benefits of Large Language Models for the
Environment. Environmental Science & Technology 57, 9 (2023), 3464–3466. https://doi.org/10.1021/acs.est.3c01106
[19] Vinu S. Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Felzi. 2023. Can AI-Generated Text be Reliably Detected?
arXiv preprint arXiv:2303.11156 (2023). https://doi.org/10.48550/arXiv.2303.11156
[20] Victoria Smith, Ali S. Shamsabadi, Carolyn Ashurst, and Adrian Weller. 2023. Identifying and Mitigating Privacy Risks Stemming from Language
Models: A Survey. arXiv preprint arXiv:2310.01424 (2023). https://doi.org/10.48550/arXiv.2310.01424
[21] Siddarth Srinivasan. 2024. Detecting AI fingerprints: A guide to watermarking and beyond.
https://www.brookings.edu/articles/detecting-ai-
fingerprints-a-guide-to-watermarking-and-beyond/
[22] Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. 2023. The Science of Detecting LLM-Generated Texts. arXiv preprint arXiv:2303.07205 (2023).
https://doi.org/10.48550/arXiv.2303.07205
[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is
All You Need. arXiv preprint arXiv:1706.03762 (2017). https://doi.org/10.48550/arXiv.1706.03762
[24] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.
OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068 [cs.CL]
[25] Wayne X. Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023. A Survey of Large Language Models. arXiv preprint arXiv:2303.18223v13 (2023). https://doi.org/10.48550/arXiv.2303.18223
[26] Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang. 2023. Provable Robust Watermarking for AI-Generated Text. arXiv preprint
arXiv:2306.17439 (2023). https://doi.org/10.48550/arXiv.2306.17439
12
