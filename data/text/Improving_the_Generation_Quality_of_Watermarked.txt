Improving the Generation Quality of Watermarked Large Language
Models via Word Importance Scoring
Yuhang Li∗
UMich
liyuhang@umich.edu
Yihan Wang∗
UCLA
wangyihan617@gmail.com
Zhouxing Shi
UCLA
zshi@cs.ucla.edu
Cho-Jui Hsieh
UCLA
chohsieh@cs.ucla.edu
Abstract
The strong general capabilities of Large Lan-
guage Models (LLMs) bring potential ethical
risks if they are unrestrictedly accessible to ma-
licious users. Token-level watermarking inserts
watermarks in the generated texts by altering
the token probability distributions with a pri-
vate random number generator seeded by its
prefix tokens.
However, this watermarking
algorithm alters the logits during generation,
which can lead to a downgraded text quality if
it chooses to promote tokens that are less rele-
vant given the input. In this work, we propose
to improve the quality of texts generated by a
watermarked language model by Watermarking
with Importance Scoring (WIS). At each gen-
eration step, we estimate the importance of the
token to generate, and prevent it from being im-
pacted by watermarking if it is important for the
semantic correctness of the output. We further
propose three methods to predict importance
scoring, including a perturbation-based method
and two model-based methods. Empirical ex-
periments show that our method can generate
texts with better quality with comparable level
of detection rate.
1
Introduction
Recent progress in Large Language Models
(LLMs) has demonstrated their strong capabili-
ties of following human instructions and perform-
ing general tasks with prompts (OpenAI, 2023b;
Chowdhery et al., 2022; Touvron et al., 2023).
While the strong general capability is useful for
many real-world applications, it also leads to po-
tential ethical risks if it is unrestrictedly accessible
to malicious usage, such as online spamming, mis-
information, or academic plagiarism. Several meth-
ods have been proposed to detect texts generated
by certain LLMs (Mitchell et al., 2023; Solaiman
et al., 2019; OpenAI, 2023a; Kirchenbauer et al.,
2023; Hou et al., 2023), among which, token-level
∗Alphabetical order
watermarking (Kirchenbauer et al., 2023) can pro-
vide fast and relatively robust detection (Shi et al.,
2023).
However, watermarking intervenes the genera-
tion process with altered logits values, which can
lead to a trade-off between text quality and de-
tection success rate. To improve text generation
quality, Kirchenbauer et al. (2023) tends to only
impact tokens with high logits entropies, with the
intuition that tokens with low logits entropies have
few alternatives and watermarking these tokens
may have a more significant impact on text qual-
ity. However, due to the well-known hallucinations
in LLMs (Zhang et al., 2023b; Yao et al., 2023;
Rawte et al., 2023), the entropy of logits may not
correctly reflect the factual importance of tokens in
the generated output.
In this paper, we aim to improve the trade-off be-
tween text quality and detection rate of token-level
watermarking on LLMs, with a separate module
to estimate the importance of tokens in generation.
We propose to prioritize tokens with higher esti-
mated importance scores. We further propose two
types of inference-time importance scoring func-
tions: A perturbation-base scoring method, which
estimates the importance score by comparing the
semantic changes after removing the token, and
model-based scoring methods, which fine-tune a
language model (LM) to predict the importance
score with a dataset labeled by counting the ap-
pearances of each token in multiple paraphrases.
We show an example with improved generation
quality from WIS in Table 1. We summarize our
contributions below:
• We evaluate the trade-off between watermark
detection and text quality with several text
generation datasets.
• We propose a framework with a token impor-
tance scoring module to improve the trade-
off of LLM watermarking, and we propose
arXiv:2311.09668v1  [cs.CL]  16 Nov 2023

three importance scoring methods, including
a perturbation-based method and two model-
based methods.
• We demonstrate that the proposed framework
can improve the text quality with similar level
of watermark detection rate.
Prompt
# Tokens
z-score
Why is it important to measure
your heart rate while exercis-
ing? What are its practical and
advanced applications?
Baseline
Heart rate is the number of
times your heart contracts or
beats per minute. When you
are exercising your heart
beats faster to deliver more
blood and oxygen to your
working muscles.
When you are resting your
heart beats slower.
Heart rate can be measured
using a heart rate monitor or
by manually taking the pulse.
Heart rate can be measured
using a heart rate monitor or
by manually taking the pulse.
92
5.05
WIS-Perturbation
Measuring heart rate can help
determine the effectiveness of
your workout, as well as help
prevent injuries, such as heart
attacks. It is also used by
athletes to determine the
intensity and effectiveness of
their training.
There are many ways of
measuring heart rate, including
a stethoscope, EKG machine
and heart-rate monitors (worn
on the chest, wrists or fingers).
90
5.47
Table 1: A comparison of texts generated with the base-
line watermarking method and WIS-Perturbation. We
use γ = 0.25 for both methods. We set δ = 2.5 for
the baseline, and δ = 2.75 for WIS-Perturbation to
maintain a similar z-score. The text generated with
WIS-Perturbation provides a more complete answer.
2
Related Work
Watermarking for LLMs.
There has been a
long history of watermarking machine learning
models due to privacy or intellectual property
concerns. A series of works (Adi et al., 2018;
Darvish Rouhani et al., 2019) watermark models
during training to protect the ownership of the mod-
els. With the emergence of LLMs, watermarking is
used to watermark and identify texts generated by
a certain LLM. This is mostly done in the post-
training stage for efficiency. These watermark-
ing methods include post-inference watermarking,
which modifies the generated texts after the lan-
guage model produces the whole output (Zhang
et al., 2023a; Yoo et al., 2023), and inference-time
watermarking, which modifies the logits during
each generation step (Kirchenbauer et al., 2023).
Kirchenbauer et al. (2023) proposes a token-level
watermarking, which modifies the logits of the next
token conditioned on its prefix. Hou et al. (2023)
proposes a sentence-level watermarking, which
conditions the semantic embedding of the next sen-
tence on its prefix. In this work, we mainly focus on
the token-level watermarking (Kirchenbauer et al.,
2023).
Improving the text quality of watermarked
LLMs.
A few concurrent works are also aware
of the downgraded quality of texts generated by
watermarked LLMs. Fernandez et al. (2023) eval-
uates watermarked LLMs on QA tasks, but they
are limited to simple QA tasks with short answers.
As a follow-up work of Kirchenbauer et al. (2023),
Fu et al. (2023) proposes to expand the greenlist
based on the semantic information of tokens, but
part of their constructed greenlist is semantically
related to the input and shared by all the tokens in
the generation, which potentially allows attackers
to infer and evade the watermarking mechanism.
Hu et al. (2023) proposes a resampling algorithm
to generate texts with watermarking without sacri-
ficing text quality. However, it requires passing the
text through the watermarked LLM for reliable de-
tection, which is also inefficient especially consid-
ering the size and inference cost of current LLMs.
Takezawa et al. (2023) proposes an approach that
considers the minimal constraints necessary for
watermark detection in the generated text, which
however increases the time complexity by a factor
of the maximum length. Different from these con-
current works, our proposed framework requires
only a lightweight model for importance scoring

with a small overhead.
3
Background
3.1
Notations
We consider a generative language model M with
a vocabulary V . Given a sequence of prefix to-
ken s = [s1, ..., st−1], st = M(s) generates the
next token with the logits vector lt. Specifically,
the logits are normalized to define a probability
distribution over V , and the probability of token
vi(1 ≤i ≤|V |) in V being the t-th token in
the generated sequence is calculated as P(st =
vi|s1, ..., st−1) =
exp(lt[i])
P
1≤j≤|V | exp(lt[j]), where lt[i]
represents the logit corresponding to vi. The fi-
nal sequence s = [s1, ..., sT0, sT0+1, ..., sT0+T ] is
obtained by autoregressively generating T tokens,
following the initial prefix [s1, ..., sT0].
3.2
Token-level Watermarking and Detection
In this section, we review the token-level water-
marking proposed by Kirchenbauer et al. (2023).
Generating texts with watermark.
To generate
token st at step t with a watermarked LLM, the vo-
cabulary V is randomly partitioned into a greenlist
G and a redlist R with a random number generator
seeded by the hash value of its prefix [s1, ..., st−1].
The size of the greenlist is controlled by a hyperpa-
rameter γ, where |G| = γ|V |. The original logits lt
at t are then modified to l′
t according to the partition
of the greenlist and redlist:
∀i ∈[|V |], l′
t[i] =
(
lt[i] + δ
if vi ∈G,
lt[i]
otherwise,
(1)
where a hyperparameter δ is used to control the
strength of the watermark. The next token at t will
then be generated with the modified logits l′
t.
Detecting texts generated with watermarked
LMs.
As described in Eq. (1), texts generated
with watermarks tend to contain more tokens in the
greenlist, which can then be utilized to detect texts
generated with watermarked LMs. The detection
is done by testing the following null hypothesis:
H0: Token sequence s = [sT0+1, ..., sT0+T ] is
generated without watermark,
with a z-test:
z = (|s|G −γT)/
p
Tγ(1 −γ).
(2)
|s|G counts the number of greenlist tokens in the
sequence s. We reject H0 and detect s as a text
generated by a watermarked LLM if z is larger
than a certain threshold which guarantees the false
positive rate of the detection.
4
Proposed Method
Kirchenbauer et al. (2023) watermarks the gen-
erated text by increasing the logits of tokens in
greenlist, which can potentially harm the quality of
generated text if the candidate tokens in greenlist
cause worse text quality than the tokens with larger
original logits. We now describe our proposed
method, Watermarking with Importance Scoring
(WIS), which utilizes an external importance scor-
ing module to identify and keep important tokens
in the generation.
4.1
Framework Overview
To start with, we define an importance scoring func-
tion f(s = [s1, ..., st−1], st) : V ∗× V →[0, 1],
where s is the input concatenated with output that
is already generated at step t. V ∗denotes the set
of sequences with tokens from V . We say st is
predicted as an important token if f(s, st) ≥r0
given a pre-defined threshold r0. We then modify
the watermarking process described in Eq. (3) to
incorporate this external information about token
importance:
l′
t[i] =











lt[i] + δ
if vi ∈G
lt[i] + δ
if vi = M(s1, ..., st−1)and
f([s1, ..., st−1], vi) ≥r0
lt[i]
otherwise.
(3)
where the logits of token vi is increased by δ if
vi is in the greenlist, or if vi should be selected
without watermarking and it has an importance
score no less than r0. In the detection, we use the
same z-test as Kirchenbauer et al. (2023), which
is described in Eq. (2), as WIS does not affect the
z-test against texts generated without watermark.
To implement f, we define the importance of a
token as the degree to which it contributes to the
correctness of output text given an input s. We also
assume that for a properly pretrained LLM M, the
original output of M includes tokens that contain
important information given s. Therefore, we want
to identify and keep these tokens that would be
generated by M without watermarking. There are
several existing methods that aim to estimate the
importance of a token given a text context with at-
tention or gradients (Madsen et al., 2021; Belinkov

and Glass, 2019). However, these methods are com-
putationally expensive if we run the estimation at
each generation step. We will introduce two pro-
posed efficient implementations of f in the next
sections.
4.2
Perturbation-Based Importance Scoring
The perturbation-based importance scoring is based
on the idea that if st in s = [s1, ..., st] contains
important information, the semantic representation
of s would be changed significantly if we add a
perturbation to st, such as removing the token. We
consider a representation mapping ϕ : V ∗→Rd
which maps a text sequence to a d-dimensional
vector, and we can estimate the importance of st
given s with
f([s1, ..., st−1], st)
(4)
= cos
 ϕ([s1, ..., st−1]), ϕ([s1, ..., st]

,
where cos(·, ·) computes the cosine similarity be-
tween two vectors. We name the WIS with this
perturbation-based importance scoring as WIS-
Perturbation.
4.3
Model-Based Importance Scoring
We can also train a language model to implement f,
which accepts the sequence s = [s1, ..., st] as the
input and outputs the importance score of st given
s. In this case, we need to construct a sequence
dataset with importance labeling for each token.
Constructing a dataset with importance label-
ing.
We construct the dataset inspired by the in-
tuition that, if a token v is semantically impor-
tant given a text s, it will stay in the paraphrase
of s.
Therefore, for each text s(i) in a corpus
S = {s(1), s(2), ..., s(m)} with m texts, we para-
phrase s(i) for N times with an external paraphrase
model. For each token s(i)
t
in s(i) with T (i) tokens,
we count n(i)
t
as the number of appearances of s(i)
t
in these N paraphrases. We construct two datasets
with importance labels for each token in each text
given a corpus S:
Sclassification
(5)
= {(s(i), (I(n(i)
1
> 0), ..., I(n(i)
T (i) > 0)) : s(i) ∈S},
Sregression
= {(s(i), ((n(i)
1 /N), ..., (n(i)
T (i)/N)) : s(i) ∈S},
where Sclassification with binary labels is constructed
for training a classification model and Sregression is
constructed for training a regression model. We
train the importance scoring model to predict the
importance label for each token in the input se-
quence given its prefix tokens. We name the WIS
with a regression model or classification model as
the importance scoring module as WIS-Regression
and WIS-Classification, respectively.
5
Experiments
In this section, we empirically show that there is
a trade-off between generation quality and water-
mark detection rate. We further show that our pro-
posed WIS methods can produce texts with better
quality compared to the baseline token-level water-
marking method proposed by Kirchenbauer et al.
(2023), with a comparable level of detection rate.
5.1
Experimental Setup
Generative Model.
We employ the LLaMA-2-
13B model (Touvron et al., 2023) as the generation
model. In all of our following experiments, we set
the beam search width to 2 and set the maximum
sequence length T to 100.
Datasets.
We employ two datasets to assess the
performance of watermarking methods. The Fac-
tual Inconsistency Benchmark (FIB) (Tam et al.,
2022) contains a modified version of the test set
of XSum (Narayan et al., 2018) and CNN (Nal-
lapati et al., 2016) datasets. Each sample in the
FIB dataset includes a document and a manually re-
vised summary that is consistent with the facts. We
use 500 examples from the XSum split in FIB in
our experiments. ELI5 (Fan et al., 2019) is a long-
form question-answering dataset, consisting of a
collection of complex questions requiring detailed
explanations. We use 500 question-and-answer
pairs in ELI5 in the experiments.
Metrics.
We report the detection rate and
ROUGE-1 score (Lin, 2004) on each dataset with
different γ and δ settings. Following the settings
in Kirchenbauer et al. (2023), we employ a z-score
threshold of 4.0.
Importance
scoring
function
implementa-
tion.
Following
Section
4,
we
implement
three importance scoring functions, including
WIS-Perturbation,
WIS-Regression and WIS-
Classification. For WIS-Perturbation, we leverage
BERTScore (Zhang et al., 2019) to compute the
cosine similarity between embeddings with a

0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Detection Rate
0.12
0.13
0.14
0.15
0.16
ROUGE-1 Score
 = 0.25
Baseline
WIS-Pertubation
WIS-Regression
WIS-Classification
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Detection Rate
 = 0.5
Baseline
WIS-Pertubation
WIS-Regression
WIS-Classification
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Detection Rate
 = 0.75
Baseline
WIS-Pertubation
WIS-Regression
WIS-Classification
Figure 1: Comparison of different scoring functions on the FIB dataset with γ = {0.25, 0.5, 0.75}. We adjust δ
to demonstrate the relationship between the ROUGE-1 score and the detection rate. For γ = 0.25 and γ = 0.5, δ
is varied within the interval [1.5, 4]. For γ = 0.75, δ is varied within the range of [2.5, 8]. We choose the range
of δ to distribute the detection rate between 0.2 and 1.0. We use r0 = 0.02 for WIS-Perturbation and r0=0.9 for
WIS-Regression to decide important tokens.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Detection Rate
0.155
0.160
0.165
0.170
0.175
0.180
ROUGE-1 Score
 = 0.1
Baseline
WIS-Pertubation
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Detection Rate
 = 0.25
Baseline
WIS-Pertubation
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Detection Rate
 = 0.5
Baseline
WIS-Pertubation
Figure 2: Comparison between perturbation-based importance scoring and the baseline on the ELI5 dataset with
γ = {0.1, 0.25, 0.5}. For the baseline method, δ is varied within the interval [1, 2.75], and for WIS-Perturbation, δ
is varied within the range of [1.25, 3.75]. We choose the range of δ to distribute the detection rate between 0.2 and
1.0. We use r0 = 0.02 for WIS-Perturbation.
pretrained BERT-base model as the represen-
tational mapping ϕ.
For WIS-Regression and
WIS-Classification, we construct the datasets
as described in Section 4.3 using 180,000 ex-
amples from the XSum training dataset.
For
WIS-Regression,
we fine-tune a BERT-base
model (Devlin et al., 2019) with an additional
fully connected layer and a sigmoid activation
function.
For WIS-Classification, we fine-tune
another BERT-base model, augmented with an
extra fully connected layer for binary classification.
To enhance the efficiency during inference, we
incorporated a sliding window mechanism on the
input sequence s for all three importance scoring
functions:
f(s, st) = f([st−w, ..., st], st),
(6)
where the window size w is set to 16 for FIB and
10 for ELI5.
5.2
Trade-off between Detection Rate and
Generation Quality
We report the results from FIB dataset and ELI5
dataset in Figure 1 and Figure 2, respectively. We
group results from different γ and δ combinations
with different γ, as the calculation for z-score is
the same with the same γ.
We can observe a clear trend of trade-off be-
tween generation quality (reflected by ROUGE-
1 scores) and the detection rate on both datasets.
With the increment of γ, the reduction in ROUGE-
1 score brought by the increase in detection rate
decreases, indicative of a lesser impact caused by
watermarking on the generated text’s quality. This
effect is most pronounced at higher γ levels, where

the green list is increasingly likely to include tokens
with the highest original logits.
5.3
Improved Generation Quality with
Importance Scoring
Comparing across the FIB and ELI5 datasets, all
of our three methods, including WIS-Perturbation,
WIS-Regression and WIS-Classification, have
demonstrated superior performance over the base-
line. As depicted in Figure 1, WIS-Perturbation
outstrips the baseline in achieving higher ROUGE-
1 scores at comparable detection rates, showcasing
its efficacy notably at γ = 0.25. With an elevation
in γ, WIS-Regression and WIS-Classification be-
gin to dominate. For the ELI5 dataset, Figure 2
shows that WIS-Perturbation consistently provides
better text quality than the baseline at matching de-
tection rates. Nonetheless, the maximum detection
rate achievable by WIS-BERTScore is decreased as
γ is incremented, with a maximum detection rate
of 70.4% at γ = 0.5. This is due to the fact that
with the increment of γ, the number of necessary
green list tokens |s|G for the text to be detected in-
creases. However, our method retains tokens from
the redlist to improve the text quality, which limits
the detection rate under a large γ
6
Conclusion
This paper presents a new framework, Watermark-
ing with Importance Scoring (WIS), designed to
improve the trade-off between text quality and de-
tection efficacy in LLM watermarking. WIS in-
troduces an external importance scoring function,
which aims to preserve tokens critical for the accu-
racy of the generated text at inference time. The ex-
perimental results on several datasets demonstrate
that, with both perturbation-based and model-based
importance scoring methods, WIS significantly en-
hances text quality while maintaining a comparable
rate of watermark detection.
References
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny
Pinkas, and Joseph Keshet. 2018. Turning your weak-
ness into a strength: Watermarking deep neural net-
works by backdooring. In 27th USENIX Security
Symposium (USENIX Security 18), pages 1615–1631.
Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics, 7:49–72.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Bita Darvish Rouhani, Huili Chen, and Farinaz
Koushanfar. 2019. Deepsigns: An end-to-end wa-
termarking framework for ownership protection of
deep neural networks. In Proceedings of the Twenty-
Fourth International Conference on Architectural
Support for Programming Languages and Operat-
ing Systems, pages 485–497.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 3558–3567, Florence,
Italy. Association for Computational Linguistics.
Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien
Chappelier, and Teddy Furon. 2023. Three bricks to
consolidate watermarks for large language models.
arXiv preprint arXiv:2308.00113.
Yu Fu, Deyi Xiong, and Yue Dong. 2023. Watermarking
conditional text generation for ai detection: Unveiling
challenges and a semantic-aware watermark remedy.
arXiv preprint arXiv:2307.13808.
Abe Bohan Hou,
Jingyu Zhang,
Tianxing He,
Yichen Wang, Yung-Sung Chuang, Hongwei Wang,
Lingfeng Shen, Benjamin Van Durme, Daniel
Khashabi, and Yulia Tsvetkov. 2023. Semstamp: A
semantic watermark with paraphrastic robustness for
text generation. arXiv preprint arXiv:2310.03991.
Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu,
Hongyang Zhang, and Heng Huang. 2023. Unbiased
watermark for large language models. arXiv preprint
arXiv:2310.10669.
John Kirchenbauer,
Jonas Geiping,
Yuxin Wen,
Jonathan Katz, Ian Miers, and Tom Goldstein. 2023.
A watermark for large language models. In Inter-
national Conference on Machine Learning, pages
17061–17084. PMLR.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries.
In Text summarization
branches out, pages 74–81.
Andreas Madsen, Nicholas Meade, Vaibhav Adlakha,
and Siva Reddy. 2021. Evaluating the faithfulness

of importance measures in nlp by recursively mask-
ing allegedly important tokens and retraining. arXiv
preprint arXiv:2110.08412.
Eric Mitchell, Yoonho Lee, Alexander Khazatsky,
Christopher D Manning, and Chelsea Finn. 2023.
Detectgpt: Zero-shot machine-generated text detec-
tion using probability curvature.
arXiv preprint
arXiv:2301.11305.
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing
Xiang, et al. 2016. Abstractive text summarization
using sequence-to-sequence rnns and beyond. arXiv
preprint arXiv:1602.06023.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. ArXiv, abs/1808.08745.
OpenAI. 2023a. Ai text classifier.
OpenAI. 2023b.
Chatgpt.
https://openai.com/
blog/chatgpt/. Accessed on May 3, 2023.
Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A
survey of hallucination in large foundation models.
arXiv preprint arXiv:2309.05922.
Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen,
Kai-Wei Chang, and Cho-Jui Hsieh. 2023. Red team-
ing language model detectors with language models.
arXiv preprint arXiv:2305.19713.
Irene Solaiman, Miles Brundage, Jack Clark, Amanda
Askell, Ariel Herbert-Voss, Jeff Wu, Alec Rad-
ford, Gretchen Krueger, Jong Wook Kim, Sarah
Kreps, et al. 2019. Release strategies and the so-
cial impacts of language models.
arXiv preprint
arXiv:1908.09203.
Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa,
and Makoto Yamada. 2023. Necessary and sufficient
watermark for large language models. arXiv preprint
arXiv:2310.00833.
Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah
Kwan, Mohit Bansal, and Colin Raffel. 2022. Eval-
uating the factual consistency of large language
models through summarization.
arXiv preprint
arXiv:2211.08412.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
arXiv
preprint arXiv:2302.13971.
Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan
Ning, and Li Yuan. 2023. Llm lies: Hallucinations
are not bugs, but features as adversarial examples.
arXiv preprint arXiv:2310.01469.
KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun
Kwak. 2023. Robust multi-bit natural language wa-
termarking through invariant features. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 2092–2115.
Ruisi Zhang, Shehzeen Samarah Hussain, Paarth
Neekhara, and Farinaz Koushanfar. 2023a. Remark-
llm: A robust and efficient watermarking framework
for generative large language models. arXiv preprint
arXiv:2310.12362.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert.
arXiv preprint
arXiv:1904.09675.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, et al. 2023b. Siren’s song in the ai
ocean: A survey on hallucination in large language
models. arXiv preprint arXiv:2309.01219.
