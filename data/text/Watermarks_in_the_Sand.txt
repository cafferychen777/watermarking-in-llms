Watermarks in the Sand: Impossibility of Strong
Watermarking for Generative Models
Hanlin Zhang1, Benjamin L. Edelman∗1, Danilo Francati∗2,
Daniele Venturi3, Giuseppe Ateniese2, and Boaz Barak1
1Harvard University, Cambridge, MA, USA
2George Mason University, Fairfax, VA, USA
3Sapienza University of Rome, Rome, Italy
Abstract
Watermarking generative models consists of planting a statistical signal (watermark) in a model’s output so
that it can be later verified that the output was generated by the given model. A strong watermarking scheme
satisfies the property that a computationally bounded attacker cannot erase the watermark without causing
significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes.
We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This
holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms
share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark
attack; the attacker is not required to know the private key of the scheme or even which scheme is used.
Our attack is based on two assumptions: (1) The attacker has access to a “quality oracle” that can evaluate
whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a
“perturbation oracle” which can modify an output with a nontrivial probability of maintaining quality, and
which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions
can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model
itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier
to satisfy over time as models grow in capabilities and modalities.
We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking
schemes for large language models: Kirchenbauer et al. (2023a), Kuditipudi et al. (2023), and Zhao et al. (2023a),
as well as those for vision-language models Fernandez et al. (2023) and Mountain (2021). The same attack
successfully removes the watermarks planted by all schemes, with only minor quality degradation.1
1
Introduction
“The term “watermarking” means the act of embedding information, which is typically difficult to
remove, into outputs created by AI — including into outputs such as photos, videos, audio clips, or
text — for the purposes of verifying the authenticity of the output or the identity or characteristics
of its provenance, modifications, or conveyance.” 2
The advent of powerful generative models such as large language models (LLMs) (Brown et al.,
2020; OpenAI, 2023; Schulman et al., 2022) and text-to-image models (Betker et al., 2023; Radford
∗Equal contribution.
1Project website and demo at https://hanlin-zhang.com/impossibility-watermarks.
2Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence (of the President,
2023)
1
arXiv:2311.04378v4  [cs.LG]  23 Jul 2024

et al., 2021) has ushered in a new era where machines can be prompted to answer questions, draft
documents, generate images in various styles, write executable code, and more. As these models
become increasingly widely deployed and capable, there is growing concern that malicious actors
could misrepresent model outputs as human-generated content (Clark et al., 2021). To prevent
misuse at scale—e.g., misinformation (Hsu and Thompson, 2023), automated phishing (Hazell, 2023),
and academic cheating (Kasneci et al., 2023)—there has been a demand for algorithmic methods that
can distinguish between content produced by models and by humans (Westerlund, 2019). Some
solutions (Mitchell et al., 2023; Tian and Cui, 2023) aim to detect the output of a given model or
family of models without modifying the model’s generation process at all, but these tend to suffer
from high error rates (Kirchenbauer et al., 2023b). The idea of watermarking schemes for generative
models is to alter the inference procedure to plant identifiable statistical signals (watermarks) into
the model outputs (Aaronson, 2022; Christ et al., 2023; Fernandez et al., 2023; Kirchenbauer et al.,
2023a,b; Kuditipudi et al., 2023; Zhao et al., 2023a). Because these schemes can intervene in the
generation process, they are potentially more powerful than classical digital watermarking schemes
(Boland et al., 1995; O’Ruanaidh and Pun, 1997), which add imperceptible watermarks to individual
given pieces of content.
A watermarking scheme consists of a generation algorithm that is a modified version of the
model in which the signal is planted and a detection algorithm that can detect whether a piece of
output came from the watermarked model. Watermarking schemes can be partitioned onto at least
two different axes:
• Public versus private: A public watermarking scheme is one where the detection algorithm is
accessible to all parties. A private (or secret-key) watermarking scheme is one in which running
the detection algorithm requires some private information.
• Strong versus weak: A strong watermarking scheme is one where a (computationally bounded)
attacker cannot modify the output (e.g., rephrase the text, apply a filter to the image, etc.) to
remove the watermark without causing significant quality degradation. A weak watermarking
scheme only resists removal by a well-specified set of transformations. At a minimum, the
detection algorithm of a weak scheme must flag outputs that are simply “copied and pasted”.
More generally, it can ensure the detection of modified outputs as long as the modification is
close to the original according to some metric such as edit distance for text, ℓ1 norm for images.
Weak watermarks can still be useful for applications like preventing AI-generated content from
being used for training (Shumailov et al., 2023), making it more expensive or inconvenient to
generate misinformation or cheat on assignments, and tracking the provenance of the precise
text/image/etc, which the watermark was applied to. But they will not foil a determined
attacker.
In this paper, we focus on strong watermarking and hence drop the “strong” modifier from
this point on. (See Section 3 for the formal definition.) Our main result is negative: Under mild
assumptions (which we specify below), strong watermarking of generative models is impossible.
This holds even in the more challenging (for the attacker) secret-key watermarking setting, where the
adversary cannot access the watermarking algorithm.
Our assumptions already hold today in several settings, and we argue that they will become
only more likely as models grow in both capabilities and modalities. The impossibility result is
constructive: we design a generic attack methodology that can remove any watermark, given the
assumptions. Our attacker algorithm does not need access to the non-watermarked model or to
any model with similar capabilities; the attack can be instantiated with only black-box access to the
watermarked model, and white-box use of much weaker open-source models. To give a “proof of
2

concept”, we instantiate an implementation of the attack and use it to successfully remove LLM
watermarks planted by the schemes of Kirchenbauer et al. (2023a), Kuditipudi et al. (2023), and Zhao
et al. (2023a) while maintaining text quality as judged by GPT4 (OpenAI, 2023).
High-quality Outputs
Low-quality Outputs
Watermarked Outputs
1
...
...
14
W
What is fair use?
Query
24
24
14
1
Figure 1: An outline of our quality-preserving random walk attack schema (The differences with original watermarked
text are highlighted.). We consider the set of all possible outputs and within it the set of all high-quality outputs (with
respect to the original prompt). For any quality-preserving watermarking scheme with a low false-positive rate, the set of
watermarked outputs (green) will be a small subset of the high-quality output (orange). We then take a random walk on
the set of high-quality outputs to arrive at a non-watermarked output (red) by generating candidate neighbors through
the perturbation oracle and using the quality oracle to reject all low-quality candidates. We instantiate our attack for
text as follows: given a watermarked text, at each iteration, the malicious user can generate span substitutions using a
small masked LM, while making sure the response quality with respect to the user query does not decrease according to a
quality oracle such as GPT-3.5 or a reward model.
Our assumptions in a nutshell.
Consider a generative model M that takes as input a prompt
x ∈X to generate an output y ∈Y according to some probability distribution. Suppose that y
was watermarked in some way, and we consider a watermark-removing adversary A. Our starting
point is the following simple but powerful observation: A’s goal is not to find a non-watermarked y′
that is semantically equivalent to y; rather, it is sufficient for A to find a non-watermarked y′ that
has equivalent quality to y as a response to the prompt. For example, if x was a prompt to write an
essay on some topic, and y is a watermarked essay, then A does not need to find a rephrasing of
y: it is enough to find another essay that would get the same grade. Given the above, we believe
that the watermarking task should be phrased with respect to a prompt-dependent quality function
Q : X × Y →[0, 1] that on input a prompt x and response y returns a grade that captures the quality
of y as a response to the prompt x. Our assumptions are the following (see Section 4.1 for formal
statements):
• Quality oracle: the attacker has access to a “quality oracle” that enables it to efficiently compute
Q on input pairs (x, y) of its choice. The quality oracle only needs to be able to discern quality
up to the quality level of the outputs produced by M.
• Perturbation oracle: the attacker has access to a randomized “perturbation oracle“ P, such that
on an input response y ∈Y and its corresponding prompt x ∈X, P(x, y) is a random perturbed
response y′ (of x) which, roughly speaking, approximately satisfies the following conditions:
3

(1) the probability that Q(x, y′) ≥Q(x, y) is bounded away from zero, (2) the random walk that
the oracle P induces on the space of high-quality outputs has good mixing properties. (See
Section 4 and Appendix C for formal conditions.)
We claim that the two assumptions typically hold in practice. A proof of concept that a
sufficiently discerning quality oracle will exist is that the watermarked model itself can be used as a
quality oracle. We can simply prompt the model to rate the quality of y as a response to x. By the
heuristic that “verification is easier than generation”, if a model is powerful enough to generate
high-quality outputs, then it should also be powerful enough to check their quality. It is likely that
multi-modal generative models (Alayrac et al., 2022; Betker et al., 2023; Reed et al., 2022) will allow this
argument to extend beyond text to other modalities such as images and audio. There are already
existing quality metrics in some of these domains (Hessel et al., 2021; Heusel et al., 2017; Salimans
et al., 2016; Wu et al., 2023b; Xu et al., 2024). In general, as models become more capable, the quality
oracle assumption can be made stronger.
Regarding the perturbation oracle, note that the perturbation can be quite minor— for instance,
replacing a masked span of a few tokens. The new tokens can be sampled completely at random
(in which case the efficiency of the attack suffers with large vocabulary size) or, more efficiently, by
resampling the tokens using an open-source (non-watermarked) masked language model, which can
be significantly weaker than M. We use the latter approach in our experiments. More sophisticated
algorithms are possible: for instance, a second ‘harmonization’ phase could be added in which
tokens outside the span are resampled using M. Regardless of the implementation, the underlying
principle behind any perturbation oracle is that there is a cloud of high-quality responses that are
accessible starting from y through the accumulation of incremental quality-preserving modifications.
Watermarking schemes themselves typically rely on the connectivity of large portions of high-quality
output space. At one extreme, if the prompt x uniquely defines a single high-quality output (for
example, if the prompt asks for the canonically formatted answer to a mathematical problem for
which only one solution exists) then the response cannot be watermarked in the first place.
1.1
Theoretical Result
Our main theoretical result is the following:
Theorem 1 (Main result, informal). For every (public or secret-key) watermarking setting satisfying the
above assumptions, there is an efficient attacker that given a prompt x and (watermarked) output y with
probability close to one, uses the quality and perturbation oracles to obtain an output y′ such that (1) y′ is not
watermarked with high probability and (2) Q(x, y′) ≥Q(x, y).
Our formal definition of watermarking schemes is given in Section 3, and the formal statement
of Theorem 1 is given in Theorem 2 (in a simplified form), and fully in Theorem 6 of Appendix C.
The main idea behind Theorem 1 is simple: see Figure 1. The adversary uses rejection sampling to run
a random walk on the set of high-quality outputs of a prompt x. That is, given the initial response
y′ = y; for t = 0, 1, . . ., the adversary repeatedly samples yt
r←P(x, y′) from the perturbation oracle
and accepts y′ (i.e., sets y′ := yt) if Q(x, y′) ≥Q(x, y). See Algorithm 1 for pseudocode. The adversary
does not need to know any details of the watermarking scheme or the space of outputs beyond an
upper bound on the mixing time of the random walk.
We note that our definitions of watermarking schemes assume that there is a statistical signal
inserted randomly into the generative model output. This is different from so-called “AI detectors”
which can have deterministic detection algorithms. Injecting randomness is necessary to provide
bounds on the false-positive probability of detecting non-model generated outputs. Indeed, so-called
4

AI detectors suffer from multiple issues. Wu et al. (2023a) surveyed known systems and concluded
that existing detection methodologies do not reflect realistic settings, and their deployment may well
cause harm. Liang et al. (2023) showed that several existing detectors are biased against non-native
English writers. OpenAI’s FAQ for educators states that AI detectors do not work and suffer from
high rates of false positives (OpenAI, 2023-11-03).
Finally, we emphasize that, unlike in typical cryptographic settings, our adversary is weaker
computationally than the watermarked model it is attacking, and only has access to it as a black box.
This only makes the impossibility result stronger.
Table 1: Average results on three watermark schemes before and after our attack, as applied to
Llama2-7B model. The GPT-4 judge score is obtained as the average of pairwise comparisons
between the perturbed text and the original watermarked output. The score is +1 if GPT-4 strongly
prefers the perturbed text, −1 if it strongly prefers the original, and zero otherwise. The reported
value is the average score over hundreds of successfully attacked examples and random ordering of
the comparands in the prompt.
Framework
C4 Real News
GPT-4 Judge
z-score
p-value
KGW (Kirchenbauer et al., 2023a)
6.236 →1.628
0.002 →0.187
-0.0877
Unigram (Zhao et al., 2023a)
8.210 →1.456
4.563e-11 →0.208
-0.0812
EXP (Kuditipudi et al., 2023)
3.540 →0.745
< 1/5000 →0.3119
-0.0675
0
5
10
15
20
25
30
Steps
1
2
3
4
5
6
z-score
0
5
10
15
20
25
30
Steps
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
GPT-4 judge score
Figure 2: Detection and quality w.r.t. the number of perturbation steps using Llama2-7B with the
KGW scheme (Kirchenbauer et al., 2023a). Left: z-score (standard deviation deviation from the null
hypothesis of non-watermarked content). Right: GPT-4 Judge score. Results are aggregated across
12 examples and the order of comparands.
1.2
Experimental Results and Implementation of Our Attack
As proof of concept, we implement instantiations of quality and perturbation oracles for the text
modality, resulting in a practical attack against language model watermarking schemes. The attack
is successful in removing watermarks from the three watermark schemes on which we tested it
(Kirchenbauer et al., 2023a; Kuditipudi et al., 2023; Zhao et al., 2023a),3 see Table 1 and Figure 2.
3We selected these schemes because they have publicly available implementations.
5

Specifically, show that with enough quality-preserving perturbations, we can degrade water-
mark average detection performance on a C4 (Raffel et al., 2020) news completion task to a z-score
below 1.645 and a p-value greater than 0.05, a standard threshold that entails no more than 5%
false positive rates. In addition, although we use reward models and GPT-3.5 as our quality oracles
during our attack, to ensure that the attack is not overfitting to these imperfect proxies, we also
measure the quality of the perturbed output using GPT-4. (We stress that GPT-4 is only used to
measure the quality of the attack, and not in the attack itself.) We see that while the detection
probability steadily reduces, the quality score is generally stable (Figure 2). While the attack might
not be the most efficient approach for these particular schemes, it has the advantage of being generic
and not varying based on the implementation details of each scheme. See Section 5 for more details
on the implementation and experimental results.
1.3
A Thought Experiment
We expect that with time, generative AI models would both be able to handle more complex prompts,
as well as be able to generate outputs with more modalities. As intuition for both our attack and our
contention that increasing capabilities favor the watermark attacker, consider the following “mental
experiment”, inspired by Valiant and Vazirani (1985). We stress that this mental experiment is not
how our actual attack is implemented. Consider a powerful chat-like model that can generate content
based on highly complex prompts. Suppose that the prompt "Generate output y satisfying
the condition x" has N possible high-quality responses y1, . . . , yN and that some ϵ fraction of
these prompts are watermarked. Now consider the prompt "Generate output y satisfying
the condition x and such that h(y) < 2256/N" where h is some hash function (which can
be simple and non-cryptographic) mapping the output space into 256 bits (which we identify with the
numbers {1, . . . , 2256}). Since the probability that h(y) < 2256/N is 1/N, in expectation there should
be a unique i such that yi satisfies this condition. Thus, if the model is to provide a high-quality
response to the prompt, then it would have no freedom to watermark the output. An attacker could
use binary search to find the value of N, as well as use random “salt” values for the hash function, to
ensure a high probability of success. Once again, this is just a mental experiment. Current models are
unable to evaluate even simple hash functions and their performance deteriorates with additional
conditions. But we believe it does provide intuition as to how the balance in watermarking shifts
from defender to attacker as model capabilities increase.
Algorithm 1: Pseudocode for our attack
Input: prompt x, watermarked response y, quality oracle Q, perturbation oracle P, random
walk length T.
Output: response y′ without watermark.
y′ ←y ;
// initialize with the watermarked response
for t ←1 to T do
yt ←P(x, y′) ;
// apply perturbation
if Q(x, yt) ≥Q(x, y) then
y′ ←yt ;
// update if quality does not decrease
end
end
return y′ without watermark ;
// return the de-watermarked response
6

2
Related Work
Watermarks for generative models.
Digital watermarking—embedding imperceptible but algo-
rithmically detectable signals in data for the purpose of attributing provenance–has a long history
(see Cox et al. (2007) for a survey) across modalities such as images (Boland et al., 1995; Cox et al.,
2007; Hayes and Danezis, 2017; O’Ruanaidh and Pun, 1997; Zhu et al., 2018), text (Atallah et al.,
2001, 2002), and audio (Arnold, 2000; Boney et al., 1996). Watermarking of generative model outputs,
involving changing the generation process itself instead of simply adding watermarks to gener-
ated images post-hoc, is a more recent development which was first introduced in the context of
image generators (Yu et al., 2021) and then in the LLM setting (Aaronson, 2022; Kirchenbauer et al.,
2023a). Several proposed schemes in the vision setting modify the models themselves through
pre-training (Yu et al., 2021) or fine-tuning (Fei et al., 2022; Fernandez et al., 2023; Zeng et al., 2023;
Zhao et al., 2023c); the scheme of Wen et al. (2023) intervenes on a diffusion model’s sampling
process. Intervening on the sampling process (treating the neural network itself as a black box) is
the standard approach for LLM schemes (Christ et al., 2023; Kirchenbauer et al., 2023a,b; Kuditipudi
et al., 2023; Liu et al., 2023; Zhao et al., 2023a). The scheme in Kirchenbauer et al. (2023b) boosts the
probabilities of tokens from an adaptively chosen “green” subset of the vocabulary at each inference
step. Follow-up works (Kirchenbauer et al., 2023b; Liu et al., 2023; Zhao et al., 2023a) improve the
robustness of this scheme to bounded-edit distance attacks. The schemes of Christ et al. (2023) and
Kuditipudi et al. (2023) guarantee the desirable property that the marginal distribution of outputs
over the random choice of the secret key is the same as (or computationally indistinguishable from)
the un-watermarked distribution, ensuring no quality degradation on average. Fairoze et al. (2023)
embed a publicly-verifiable cryptographic signature into texts using rejection sampling.
Limitations of generative model watermarks.
There have also been various recent works that
attack watermarking schemes. In the vision domain, there are attacks (Lukas et al., 2023; Saberi et al.,
2023; Zhao et al., 2023b) that can erase watermarks for various watermarking schemes; relevant at-
tacks typically involve adding noise to either the images themselves or latent representations, and/or
performing some optimization procedure to remove the watermark. Some of these works prove that
their attacks will succeed under strict assumptions about watermarks. For instance, the impossibility
result of Zhao et al. (2023b) is focused on classical schemes that apply a bounded-perturbation
watermark post-hoc to individual images. Meanwhile, Saberi et al. (2023) propose a general attack—
diffusion purification— for image watermark schemes with a low “perturbation budget”, which is
the stringent requirement that even for a single key, the distribution of watermarked outputs is close
to the original distribution of outputs. They also demonstrate a “model substitution” attack against
more general image watermarking schemes. This attack finds adversarial perturbations with respect
to a proxy watermark detector. However, obtaining the proxy detector requires either white-box
access to the detection algorithm or a large number of watermarked and non-watermarked samples.
Sadasivan et al. (2023) and Saberi et al. (2023) prove limitations on post-hoc detectors when the
distributions of human and AI-generated text are close. In the LLM domain, Kirchenbauer et al.
(2023a,b); Kuditipudi et al. (2023) empirically study the viability of various attacks against the
schemes they propose. Notably, in paraphrasing attacks (which can be implemented using language
models, translation systems, or manually by humans) the attacker rewords the watermarked output
while preserving its meaning. The above works find that paraphrases often leave some substrings of
the text intact, or degrade text quality, and thus have mixed success on schemes that are robust to
bounded edit distance attacks. We emphasize that our attack does not preserve the semantics of the
output, and is therefore less limited than paraphrasing attacks. A watermarking impossibility result
has also been claimed in the LLM domain by Sato et al. (2023); however, their definition assumes
7

security against an unbounded-time adversary that can sample from the distribution of the original
unwatermarked generative model (see (Sato et al., 2023, Appendix B)).
Related settings.
Parallel to the line of work on watermarking generative models, there have been
various proposed schemes for post-hoc detection of generative model outputs (Chakraborty et al.,
2023; Gehrmann et al., 2019; Mitchell et al., 2023; Tian and Cui, 2023,?; Verma et al., 2023; Wang
et al., 2023; Wu et al., 2023a; Zellers et al., 2019). When there is no watermark in the data, detecting
whether it is generated by a given model is a harder problem, and there are fundamental limitations
to these schemes. Post-hoc detection needs to rely on empirical differences between the distributions
of generative model outputs and natural/human outputs. As generative models become better
at modeling their training distributions, this detection problem can become increasingly difficult;
and since adversaries can adapt their outputs to thwart these detectors, there cannot be error rate
guarantees in this setting. Empirically, paraphrasing attacks work well against post-hoc detection
algorithms for LLMs (Kirchenbauer et al., 2023b; Krishna et al., 2023). Distinct from watermarking
schemes that aim to make every output of a generative model detectable, there are also schemes that
aim to watermark the weights of the network for the purpose of intellectual property protection,
or enable a party to detect a signature in a given network given adaptive black-box access to the
network (Bansal et al., 2022; Rouhani et al., 2018; Uchida et al., 2017; Zhang et al., 2018). Finally, we
note that watermarking is a special case of steganography, which is the more general practice of
planting hidden information in data. This has also been studied in the context of generative models
(Fang et al., 2017; Tancik et al., 2020; Yang et al., 2018; Ziegler et al., 2019).
3
Secret-key Watermarking Schemes for Generative Models
In this section, we formalize the notion of a (strong) watermarking scheme for generative models.
First, we begin with the formalization of generative models. Then, we introduce the concept of
secret-key watermarking schemes. We refer the reader to Appendix B.1 for the notation used.
3.1
Generative Models
Generative models are randomized algorithms that produce a response y ∈Y (e.g., an image or text)
in response to a given input prompt x ∈X (e.g., a question or a description of an image).
Definition 1 (Generative models). A conditional generative model M : X →Y is a randomized efficient
algorithm that, given a prompt x ∈X, produces an output y ∈Y.4 We call X the prompt space, Y
the output space, and y
r←M(x) the M’s response to a prompt x ∈X.
To study watermarking formally, we associate with a generative model M a quality function Q
that assigns a score 0 ≤Q(x, y) ≤1 to a response y (generated by M) for a given prompt x. The
quality function serves as a measure of the “quality” of the generative model’s response. Moreover,
the quality Q must be “universal” in the sense that it can assign a score even to a response y
not generated by the corresponding M. For instance, y might be a response crafted by a human.
In essence, Q should be objective and not contingent upon the specific generative model under
consideration.
Definition 2 (Quality function). A quality function Q is a deterministic function Q : X × Y →[0, 1]
that assigns a score Q(x, y) ∈[0, 1] to the response y ∈Y for a given prompt x ∈X. We say a quality
4A conditional generative model M is considered efficient if its running time is polynomial in the length of the prompt
x ∈X.
8

function Q is associated with a conditional generative model M : X →Y if the response quality of M
is evaluated using Q. That is, Q(x, M(x)) is the score assigned to M’s response M(x) for a prompt
x ∈X.
3.2
Secret-Key Watermarking Schemes
In this section we formally define watermarking schemes. Because our focus is a negative result, we
focus on defining secret-key watermarking schemes. These are easier to construct, and hence ruling
them out makes our impossibility result stronger.
Definition 3 (Secret-key watermarking scheme). A secret-key watermarking scheme for a class of
generative models M = {Mi : X →Y} with a key space K consists of the following efficient
algorithms:5
Watermark(M): Given a generative model M ∈M, this randomized watermarking algorithm outputs
a secret key k ∈K and a watermarked generative model Mk : X →Y, dependent on k.
Detectk(x, y): Accepting a secret key k ∈K, a prompt x ∈X, and an output y ∈Y, this deterministic
detection algorithm returns a decision bit b ∈{0, 1}, where 1 indicates the presence of the
watermark, and 0 indicates its absence.
Definition 3 has several aspects that make it easier to realize such schemes: (i) we focus on secret-key
watermarking schemes, where the detection algorithm Detect uses the same secret key k that embeds
the watermark; (ii) we adopt prompt-conditional detection, meaning the Detect algorithm also receives
the prompt x; and (iii) we grant the watermarking algorithm Watermark non-black-box access to the
model. In other words, the watermarked version Mk of M could be derived by utilizing the internal
operations of M.6 These choices simplify the development of the watermarking scheme. Thus, our
chosen relaxations only reinforce our impossibility results presented in Section 4.
Properties of watermarking schemes.
Definition 3 captures only the syntactic conditions for
watermarking. Next, we define the properties of a watermarking scheme, which will be used later
to prove our impossibility result. Primarily, we aim to measure the “false negative” and “false
positive” rates. The former refers to the proportion of Mk’s responses that are incorrectly identified as
“un-watermarked”, i.e., Detectk(x, Mk(x)) = 0. The latter represents the probability of outputs y ∈Y
(not produced by Mk) that are still labeled as “watermarked”. The formal definitions are provided
below.7
Definition 4 (False negative and false positive ϵ-rates). Let ϵpos, ϵneg > 0 and Π = (Watermark, Detect)
be a secret-key watermarking scheme for a class of generative models M = {Mi : X →Y}.
False negative ϵneg-rate: Π has a false negative ϵneg-rate if, for every model M ∈{Mi : X →Y} and
every prompt x ∈X, Pr[Detectk(x, Mk(x)) = 0] ≤ϵneg. The probability is over the random
coins of Mk and the pair (k, Mk) output by Watermark(M).
False positive ϵpos-rate: Π has a false positive ϵpos-rate if, for every model M ∈{Mi : X →Y}, for
every prompt x ∈X, and for every output y ∈Y, Pr[Detectk(x, y) = 1] ≤ϵpos. The probability
is over the pair (k, Mk) output by Watermark(M).
5The watermarking scheme is considered efficient if both Watermark and Detect run in polynomial-time in the input
size.
6For instance, the watermarking scheme might exploit the internal parameters of M during the watermarking phase.
7As mentioned in Section 1, we consider the case of watermarking schemes with controlled false positive probability
taken over the choice of the injected randomness (i.e., key) and do not consider deterministic “AI detectors”.
9

The rates above might be influenced by x ∈X and y ∈Y, for instance, they may diminish with the
length of the output.
Broadly, a low false negative ϵneg-rate indicates that the watermarking scheme is functioning as
anticipated. This means there is a significant probability that the detection algorithm outputs 1
when y is derived from the watermarked model. Conversely, small false positive ϵpos-rate implies
there is a small likelihood for Detectk(x, y) = 1 when k is sampled independently of (x, y) ∈X.
Introducing a random key k ensures that we can bound the false positive rate without needing to
make assumptions on the unknown human-generated data distribution. Practical watermarking
schemes typically have a tunable hyperparameter (such as a z-score threshold or p-value) that
enables trading in the false positive and false negative rate. We remark that for many applications,
watermarking schemes require a very low false positive rate. For example, if professors routinely
use watermark detectors for testing for cheating in hundreds or thousands of problem-set or essay
submissions, then unless ϵpos will need to be very small, there would be a high chance of students
being falsely blamed.
Security of watermarking schemes.
Definition 4 is insufficient for ensuring security as it only
characterizes the basic properties that a watermarking scheme should possess. For instance, these
definitions might be met by watermarking an output by appending the sentence “This text was
generated by an AI model.” to the end of the LLM’s response.8 Regrettably, such an approach lacks
robustness since anyone can effortlessly remove the watermark by deleting the appended sentence.
This consideration leads us to the need to define security. Specifically, the watermark should be
robust against adversarial attempts at removal. The adversary’s objective is to strip the watermark
from y (i.e., evade detection) while maintaining the quality (as assessed by the quality function Q)
near the original response y level. This requirement is crucial; otherwise, any arbitrary response
could constitute a valid erasure attack. Given our aim is to establish an impossibility result, we will
set forth a basic condition that is necessary but not sufficient. The formal definition follows.
Definition 5 (Erasure attack against watermarking schemes). Let Π = (Watermark, Detect) be a
watermarking scheme for a class of generative models M = {Mi : X →Y} with associated quality
function Q : X × Y →[0, 1]. We say that an adversary A ϵ-breaks Π if for every M ∈M, for every
prompt x ∈X, we have
Pr
h
Detectk(x, y′) = 0 and Q(x, y′) ≥Q(x, y) : y
r←Mk(x), y′
r←A(x, y)
i
≥ϵ
where the probability is taken over (k, Mk) output by Watermark(M) and the random coins of A.
Note that in a secret-key watermarking scheme (where an attacker does not know k), the highest
probability we can anticipate for success in an erasure attack is 1−ϵpos, where ϵpos is the false positive
rate (see Definition 4).
Intuitively, a watermarking scheme Π = (Watermark, Detect) is considered robust and secure
(for a meaningful choice of ϵ) if there does not exist a computationally efficient adversary A that
satisfies the aforementioned Definition 5. The notion of “computationally efficient” depends on
the context, but at the very least the adversary should be able to: (1) make black-box queries to the
watermarked model itself, (2) perform efficient computations that do not require white-box access to
the model or the data that it was trained on. As we demonstrate in Section 4, there is a practical and
universal adversary that meets Definition 5’s criteria in several settings. However, if the adversary
is restricted in other ways (for example only allowed to modify the output up to ϵ distance in some
8Here, we are implicitly assuming that the appended sentence does not significantly degrade the quality of the response.
10

metric) then it may be possible to resist erasure attacks. This is the setting of “weak watermarking”
that we mention in Section 1. We do not investigate weak watermarking schemes in this paper.
On quality approximation/degradation.
According to Definition 5, an attack is considered suc-
cessful when the quality of the non-watermarked output y′ (computed by the adversary) is at least
that of the target watermarked output y
r←Mk(x), i.e., Q(x, y′) ≥Q(x, y). We highlight that we can
relax Definition 5 to consider adversaries that can remove the watermark at the price of some “small”
degradation in the quality. This can be accomplished by modifying Definition 5 and requiring that
Q(x, y′) ≥Q(x, y) −γ (instead of Q(x, y′) ≥Q(x, y)) where γ is the parameter defining the quality
degradation tolerated by the definition. For simplicity, in this work we focus on the simpler setting
in which there is no degradation, and the attacker has access to an exact quality oracle. In practice,
the quality oracle may not be perfect, and hence some approximation and degradation might be
necessary.
4
Impossibility Result
We now show that a secret-key watermarking scheme can be defeated (i.e., remove the watermark)
when the outputs of the underlying generative model can be perturbed. With the term “perturbed”
we mean that the output can be slightly modified without affecting the quality. Examples of
perturbable outputs are texts in which a word/token/span can be modified without changing the
meaning of the text, or images in which a single pixel/patch can be modified (e.g., slightly change
the RGB value of the target region) without affecting the picture. Thus, the impossibility result of
this theorem applies to LLMs or image models.
4.1
Assumptions and Discussion
Our impossibility result relies on the following two assumptions:
1. Adversaries can check their own work: The adversary has access to the quality oracle Q :
X × Y →[0, 1] (associated to the target generative model M : X →Y).
2. Random walk on the output space Y: At a high level, we assume that there is some graph
G = (V, E) in the output space V = Y of the target watermarked model Mk : X →Y. In
addition, the adversary has access to an oracle P (dubbed perturbation oracle) that, on input
y ∈Y (e.g., the watermarked output), returns a random neighbor y′ of y according to the graph
G. For example, in the case of language, this can be obtained by taking some subset of the
tokens and replacing them with random choices (either uniform or informed by large or smaller
language models) or semantically equivalent spans. The main assumption we make is that the
random walk has non-trivial mixing properties, and in particular that the graph is irreducible
and aperiodic (these two conditions guarantee that a long-enough random walk will converge
to its stationary distribution).
The definition of a perturbation oracle is given below.
Pertubation oracle.
We abstract the ability of the adversary to perform a random walk over the
output space of a (possibly watermarked) generative model M : X →Y by introducing the notion
of perturbation oracles. In a nutshell, a perturbation oracle P : X × Y →Y is a randomized oracle
that, on input x ∈X and y ∈Y, returns a high-quality y′ ∈Y (according to some distribution) with
probability at least ϵpert. The formal definition follows.
11

Definition 6 (Perturbation oracle). A perturbation oracle P is a randomized oracle that, on input
x ∈X and y ∈Y, returns an y′ ∈Y. We say a perturbation oracle P : X × Y →Y with quality
function Q : X × Y →[0, 1] is ϵpert-preserving if for every prompt x ∈X, for every y ∈Y, we have
Pr[Q(x, P(x, y)) ≥Q(x, y)] ≥ϵpert.
4.1.1
Mixing Condition
To define the mixing condition, we need to define an associated random walk for the perturbation
oracle with respect to some particular prompt. We then consider the mixing time of this random walk
(see Appendix B for a formal definition of mixing time). Fix a prompt x ∈X. The perturbation oracle
P(x, ·) can be represented using a weighted directed graph Gx = (Vx, Ex) where an edge (y0, y1) ∈Ex
if weighted with the probability of going from output y0 to output y1 using P(x, ·). Below, we
formally define two different graph (hierarchically ordered) representations corresponding to P. The
first, named thex-prompt graph representation, is essentially the graph Gx = (Vx, Ex) described above,
i.e., the graph representing the perturbation oracle P(x, ·) for a fixed prompt x ∈X. The second,
named q-quality x-prompt graph representation, is the subgraph G≥q
x
= (V≥q
x , E≥q
x ) of Gx = (Vx, Ex)
where we only consider vertices of quality at least q, i.e., Q(x, y) ≥q.
Definition 7 (Graph representation of perturbation oracles). Let P : X × Y →Y be a perturbation
oracle with associated quality function Q : X × Y →[0, 1].
x-prompt graph representation: Fix a prompt x ∈X, the x-prompt graph representation of P is a
weighted directed graph Gx = (Vx, Ex) such that Vx = Y and Ex ⊆Y × Y defined as follows:
Ex = {(y0, y1) ∈Y × Y : Pr[y1 = P(x, y0)] > 0} .
The weight function weight(y0, y1) of Gx = (Vx, Ex) is defined as follows:
weight(y0, y1) =
 Pr[y1 = P(x, y0)]
if (y0, y1) ∈Ex,
0
otherwise.
q-quality x-prompt graph representation: Fix q ∈[0, 1] and a prompt x ∈X, the q-quality x-prompt
graph representation of P is a graph G≥q
x
= (V≥q
x , E≥q
x ) such that V≥q
x
⊆Y and E≥q
x
⊆Y × Y
defined as follows:
E≥q
x
= {(y0, y1) ∈Y × Y : Q(x, y0) ≥q, Q(x, y1) ≥q, and Pr[y1 = P(x, y0)] > 0} , and
V≥q
x
= {y ∈Y : Q(x, y) ≥q} .
The weight function weight(y0, y1) of G≥q
x
= (V≥q
x , E≥q
x ) is defined as follows:
weight(y0, y1) =

Pr[y1 = P(x, y0)]
if (y0, y1) ∈E≥q
x ,
0
otherwise.
4.1.2
Discussion
The quality-oracle assumption is based on the intuition that it is easier, or at the very least not harder,
to verify outputs than to generate them. In practice, we have used both zero-shot prompting (see
Section 5) and reward models to compute the quality function. In the context of images, it is also
possible to use measures such as Inception score (Salimans et al., 2016), FID score (Heusel et al.,
2017), CLIP score (Hessel et al., 2021) or text-to-image reward models (Wu et al., 2023b; Xu et al.,
12

2024) etc. While at the moment zero-shot prompting works only for language models and not for
audio/visual data, as models grow in power, both in capabilities and modalities, computing quality
functions will only become easier.
The perturbation-oracle assumption is more subtle. First, note that the notion of “perturbation”
can be abstract and does not need to correspond to changing a discrete subset of the output (e.g., a
sequence of tokens or a patch of an image). The assumption can potentially fail for some prompts.
For example, if a prompt has the form “Solve for the following set of n linear equations in n
variables, and format the solution as ...” then there is zero entropy in the space of possible high-
quality solutions. In such a case there is no “perturbation oracle” that can be defined. However,
in such a case no watermarking is possible either. Indeed, watermarking schemes require some
sort of “perturbation oracle” as well. To ensure successful watermarking, there needs to be a large
set of potential high-quality solutions, and the scheme needs to be able to select a small subset of
it (with a measure bounded by ϵpos— the false positive rate) based on the key. Hence we believe
this assumption will be justified in cases where watermarking is feasible as well. Note that our
perturbation oracle is in one crucial case much weaker than what is needed for watermarking: we
do not require it to always preserve quality or even do so with high probability. It is enough that the
probability of preserving quality is bounded away from zero.
Another potential issue with the perturbation oracle is that there could be outputs that are
high-quality but would be very unlikely to be ever output from the model, whether watermarked
or not. Hence the graph might be disconnected simply because of these “unreachable outputs”.
However, these can be addressed by introducing weights on the vertices as well that correspond to
the probability of the output under the model. Such weighing makes no difference to our argument,
but we omit this for clarity of notation.
4.2
Proof Overview of the Impossibility Result
The idea behind the impossibility result is to allow the adversary to perform a random walk on
the graph while walking only over vertices with sufficiently high quality. Such a high-quality
random walk can be executed by leveraging the quality oracle Q and checking the quality of the
next perturbed vertex (at each step of the random walk). Somewhat more formally, let x be a prompt
and V≥q
x
be a subset of vertices (of the graph) of quality Q(x, y) ≥q. Assuming the vertices V≥q
x
are connected (i.e., there is a non-zero probability of reaching any vertex in V≥q
x ), then an adversary,
starting from a watermarked output y ∈Y with quality Q(x, y) ≥q, can perform a random walk
(using its corresponding oracle) to reach a y′ ∈V≥q
x
that is not watermarked. As we will see next,
the existence of such a non-watermarked y′ ∈V≥q
x
is guaranteed by the false positive rate of the
watermarking scheme (see Definition 4).
Impossibility result.
For simplicity, let us assume there exists a perturbation oracle P that is
1-preserving, i.e., for a fixed prompt x ∈X, the oracle always outputs a perturbed output y′ ∈Y
whose quality is at least that of the initial value y ∈Y. Now, consider an adversary A with oracle
access to P. The adversary’s objective is to perturb the watermarked output y0 t times (for some
large enough t) to produce intermediate perturbations y1, . . . , yt, such that the final perturbed output
yt will be independent of the watermarking secret key k. In this way, yt will likely be verified as
non-watermarked (i.e., Detectk(x, yt) = 0) with probability at least 1 −ϵpos where ϵpos is the false
positive rate of the watermarking scheme. This can be achieved by requiring the adversary to
compute yi
r←P(x, yi−1) for i ∈[t], where y0 corresponds to the initial watermarked output. This
adversarial strategy is effective in removing the watermark for the following two reasons:
13

• The interaction with the perturbation oracle can be represented as a t-step random walk over
the q0-quality x-prompt graph representation G≥q0
x
= (V≥q0
x
, E≥q0
x
) of P where q0 is the quality
of the starting (watermarked) output y0. Thus, y0, . . . , yt exactly represents the path taken by
the adversary during such a random walk (note that each y1, . . . , yt will have the same quality
q0 by definition of a 1-preserving perturbation oracle. See Definition 6).
• If G≥q0
x
is irreducible and aperiodic, a random walk over G≥q0
x
converges to its unique stationary
distribution ⃗π (see Appendix B.2).9 In addition, if t corresponds to (or is slightly larger than) the
ϵdist-mixing time of G≥q0
x
, then the distribution of the final perturbed output yt is ϵdist-close to the
stationary distribution ⃗π of G≥q0
x
. Furthermore, this distribution ⃗π is completely independent
of the secret key k of the watermarking scheme since the secret key k is sampled (by the
watermarking algorithm Watermark(M)) independently from P’s definition.
The above two facts, combined with the false positive ϵpos-rate of the watermarking scheme, directly
imply that yt will be classified as non-watermarked with probability at least (1 −ϵpos)(1 −ϵdist).
Moreover, we can set ϵdist to be arbitrarily small to achieve an adversarial advantage of (1 −ϵpos)(1 −
ϵdist) ≈(1 −ϵpos).10
The aforementioned approach relies on the fact that the perturbation oracle is 1-preserving;
thus, each oracle invocation will certainly correspond to a new step of the random walk over the
graph G≥q0
x
= (V≥q0
x
, E≥q0
x
). This is because, starting from the watermarked output y0 with quality q0,
each invocation will output (with probability 1) a perturbed output yi with at least the same quality
q0. To make the impossibility more generic, we extend the attack to deal with perturbation oracles
that may return low-quality perturbed values with non-zero probability, i.e., the perturbation oracle
is ϵpert-preserving for some ϵpert < 1. In such a setting, we are not guaranteed that each invocation
of P will correspond to a step of the random walk over G≥q0
x
. Indeed, with probability 1 −ϵpert, the
ϵpert-preserving perturbation oracle may return y′ ∈Y such that Q(x, y′) < q0, i.e., y′ ̸∈V≥q0
x
. To
overcome this difficulty, we give the adversary oracle access to the quality function Q. In such a way,
at each iteration i ∈[t], the adversary can check the quality of the perturbed value yi
r←P(x, yi−1)
and proceed as follows:
• If Q(x, yi) ≥q0, the adversary knows that the obtained perturbed value yi corresponds to a new
random walk step over G≥q0
x
. Hence, in the next iteration, it will perturb the recently obtained
output yi (as described previously for the case of a 1-preserving perturbation oracle).
• On the other hand, if Q(x, yi) < q0, the value yi does not correspond to a new random walk step.
In this case, the adversary needs to re-send yi−1 to P until it obtains a perturbed value yi such
that Q(x, yi) ≥q0.
Hence, by accessing the quality oracle Q, the adversary can continue walking over the high-quality
graph G≥q0
x
even if the perturbation oracle is not errorless (i.e., ϵpert < 1). Naturally, when ϵpert < 1,
the adversary needs to submit more queries to the perturbation oracle to overcome the low-quality
outputs of P. This is taken into account by our theorem which relates the number of queries needed
to the parameter ϵpert, i.e., the higher (resp. lower) ϵpert, the lower (resp. higher) the number of
queries.
9Irreducibility and aperiodicity are the fundamental mixing properties that P (and its corresponding graph) must
satisfy.
10Indeed, ϵdist approaches 0 exponentially as t increases (see Theorem 5).
14

Theorem 2 reports our impossibility result that bounds A’s advantage ϵ in erasing the watermark
(for every M ∈M in a particular class M = {Mi : X →Y}) when A has oracle access to a
perturbation oracle P. As discussed in this section, such a result relies on the fact that P’s graph
representation is irreducible and aperiodic, which are necessary conditions to converge to its unique
stationary distribution during the random walk. To make explicit the independence between the
adversary (including its perturbation oracle P) and the secret key k of the target watermarked model
Mk, we define P’s irreducibility and aperiodicity properties w.r.t. the median quality among those of
all possible responses (to a prompt x) that can be obtained by watermarking M ∈M. Observe that
the median quality does not depend on the secret key k used during the watermarking process but
instead depends on all possible choices of secret keys (i.e., the key space of Watermark). Formally, let
QM,x = {q1, q2, . . .} and qmin be defined as follows:
QM,x =
n
q : Pr
h
Q(x, Mk(x)) = q : (k, Mk)
r←Watermark(M)
i
> 0
o
,
(1)
qmin =
min
M∈M,x∈X{qM,x} where qM,x is the median quality of QM,x,
(2)
where the probability in Equation (1) is taken over the random coins of Watermark and Mk.11 Then,
the following Theorem 2 defines P’s properties w.r.t. qmin, i.e., the minimum among all median
qualities (see Equation (2)).
Theorem 2. Let Π = (Watermark, Detect) be a watermarking scheme for a class of generative models
M = {Mi : X →Y} with an associated quality function Q : X × Y →[0, 1]. Let P : X × Y →Y be a
perturbation oracle (defined over the same prompt space X and output space Y as the class M) with the same
associated quality function Q : X × Y →[0, 1] as Π.
Under the following conditions
1. The watermarking scheme Π has a false positive ϵpos-rate;
2. The perturbation oracle P is ϵpert-preserving;
3. For every non-watermarked model M ∈M, for every prompt x ∈X, for every quality q ∈[qmin, 1],
the q-quality x-prompt graph representation G≥q
x
of P is irreducible and aperiodic where qmin is the
minimum median defined in Equation (2). Also, let tM,x be the ϵdist-mixing time of G≥q
x
where ϵdist ≈0,
and let tmax be the largest mixing time among {tM,x}M∈M,x∈X ;
there exists an oracle-aided universal adversary AP(·,·),Q(·,·) that ϵ-breaks Π (Definition 5) by submitting at
most O(tmax/ϵpert) queries to P where ϵ ≈(1 −ϵpos)/2.
The following corollary is obtained from Theorem 2 assuming (i) the perturbation oracle is 1
2-
preserving, and (ii) the watermarking scheme has a false positive rate of 1
10.
Corollary 1. Let Π = (Watermark, Detect) be a watermarking scheme and P be a perturbation
oracle as defined in Theorem 2. If Π has a false positive 1
10-rate and P is 1
2-preserving, then there
exists an oracle-aided universal adversary AP(·,·),Q(·,·) that breaks Π with a success probability of
approximately 9
20, by submitting at most O(tmax) queries to P, where tmax is defined in Theorem 2.
On improving A’s advantage.
In Theorem 2, A’s advantage is approximately 1−ϵpos
2
whereas 1−ϵpos
is the maximum advantage that an adversary can achieve. The multiplicative 1
2 loss is due to the
11This is equivalent to saying that the quality values in QM,x = {q1, q2, . . .} are defined over all possible choices of secret
keys k (output by Watermark) and all possible random coins of the resulting watermarked model Mk.
15

definition of P w.r.t. to the minimum median qmin. Without loss of generality, it is possible to get an
adversarial advantage ϵ arbitrarily close to 1 −ϵpos by enforcing P’s properties over a larger range
of q values (Item 3 of Theorem 2). This can be accomplished by decreasing qmin (instead of using
the median quality) such that convergence will be guaranteed for almost all watermarked outputs y
(the ones with quality at least qmin).12 For this reason, in Appendix C, we include Theorem 6 (and its
corresponding proof) that is the extended version of the above theorem. In particular, Theorem 6
defines P’s properties w.r.t. v-th quality percentile (instead of the median) so that, by decreasing v,
we can get arbitrarily close to the best possible adversarial advantage 1 −ϵpos. Moreover, Theorem 6
explicates the concrete relation between the adversarial advantage ϵ, the number of queries, and the
ϵpert-preservation of P. We refer the reader to Appendix C for more details.
5
Experiments
5.1
Attack Implementation
As a proof of concept empirical test of our attack schema, we construct an implementation of it
and use it to attack several published watermarking schemes. While our general attack framework
applies across all modalities, our focus is language, where quality oracles are currently more
straightforward to implement. To implement our attack, we need to choose (1) the generative model
and (2) the watermarking scheme, as well as implement (3) the perturbation oracle, and (4) the
quality oracle.
We choose Llama2-7B (Touvron et al., 2023) as our generative model and attack three different
watermarking schemes (Kirchenbauer et al., 2023a; Kuditipudi et al., 2023; Zhao et al., 2023a). We
implement our perturbation oracle using T5-XL v1.1 (Raffel et al., 2020).13 This model occasionally
misses simple errors in the text such as repetitiveness, capitalization and punctuation mistakes, and
incoherent or irrelevant phrases, so we use a small Gramformer to do post-hoc grammatical error
correction or ask GPT-3.5 to perform a final check of the response quality with a focus on these sorts
of errors, whenever the output is approved by the reward model (see Figure 7 for the prompt).14
After trying out various potential reward model implementations, we settled on using an open-
source reward model (RoBERTa-v3 large (Liu et al., 2019) fine-tuned on OpenAssistant (Köpf et al.,
2023) preference data) as the primary quality oracle.15. For removing image watermarks, we use the
latest stable-diffusion-xl-base and its distilled version sdxl-turbo. We implement perturbation and
quality oracles as stable-diffusion-2-base (Rombach et al., 2022) and the reward model trained on
Human Preference Score v2 (Wu et al., 2023b),16 respectively. Experiments were all performed on
a combination of 40 GiB and 80 GiB A100s and our code is available at https://github.com/
hlzhang109/impossibility-watermark.
This model occasionally misses simple errors in the text such as capitalization and punctuation
mistakes and incoherent or irrelevant phrases, so we ask GPT-3.5 to perform a final check of the
response quality with a focus on these sorts of errors, whenever the output is approved by the
12For example, by setting qmin = 0, we obtain that the attack succeeds independently from the quality of the received
initial watermarked output y
r←Mk(x). This is because the quality of y is always non-negative and, for every quality
q ∈[0, 1], the q-quality x-prompt graph representation of P will be irreducible and aperiodic.
13This model is only pre-trained on C4 excluding any supervised training, to mask and infill one random span at each
iteration. This design choice might be important as we want to generate a non-watermarked example without substantially
shrinking the text length while the original T5 tends to infill texts with shorter lengths than the masked contents.
14https://github.com/PrithivirajDamodaran/Gramformer
15https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2
16https://huggingface.co/adams-story/HPSv2-hf
16

reward model (see Figure 7 for the prompt).17 See Appendix E.1 for more details on the experimental
setup.
All three of the watermarking schemes we attack were originally tested on the Real News subset
of the C4 dataset (Raffel et al., 2020), so we use this as our primary task as well. Specifically, we give
Llama-2-7B (Touvron et al., 2023) the task of generating a completion given the first 20 tokens of a
news article. Except where otherwise noted, we default to a generation length of 200 tokens.
We test the following three watermark frameworks and refer readers to Appendix E.2 for more
details:
• KGW (Kirchenbauer et al., 2023a) selects a randomized set of “green” tokens before a word is
generated, and then softly promotes the use of green tokens during sampling, which can be
detected efficiently.
• EXP (Kuditipudi et al., 2023) is a distortion-free watermark framework that preserves the
original LM’s text distribution (over the randomness of the secret key), at least up to some
maximum number of generated tokens.
• Unigram (Zhao et al., 2023a) is a watermarking scheme based on KGW designed to be provably
robust to perturbations with bounded edit distance from the original watermarked text.
• Stable Signature (Fernandez et al., 2023) refers to a method of embedding invisible watermarks
into images generated by Latent Diffusion Models (LDMs). This approach involves fine-tuning
the latent decoder part of the image generator, conditioning it on a binary signature.
• Invisible watermark (Mountain, 2021) is a default (classic) watermark to the Stable Diffusion
model series, which utilizes frequency space transformations to embed watermarks invisibly
into images, using Discrete Wavelet Transform and Discrete Cosine Transform.
5.2
Experimental Results
We include our results on C4 news completion tasks with three watermark frameworks in Table 1.
We show that with enough iterations, we can degrade watermark average detection performance
to a z-score below 1.645 and a p-value greater than 0.05, a standard threshold that entails no more
than 5% false positive rates. We also query GPT-4 to evaluate the comparative quality of the original
watermarked output and the output after our attack. To do this, we prompt GPT-4 to compare the
two outputs on a scale from 1 to 5 (corresponding to whether GPT4 “strongly” prefers one output,
“slightly“ prefers it, or judges them to “have similar quality”, see Figure 8). Because GPT-4 has a
bias towards preferring the first of the two outputs (Zheng et al., 2023), we query it twice, with both
orderings of the two outputs.
We take a closer look at the KGW results by plotting histograms of the detection and GPT-4
judgment statistics for around 200 examples (Figure 3). These demonstrate that our attack can sub-
stantially degrade detection performance while typically inducing only minor quality degradation.
Note that we choose the number of random walk steps for our attack to be high enough to push the
average p-value above 0.05; we expect that running for longer would result in even higher p-values.
Detection performance and text quality over the course of the random walk.
We also study how
the watermark z-score and GPT-4 judge quality score change over time during the attack. For the
17For each judgment, we calculate the scores of the original watermarked response and candidate response by passing
the output of the reward model through a softmax layer, since this is how the reward model was supervised. We count the
comparison as a tie if the difference in scores is less than ∆= 0.02.
17

0
2
4
6
8
z-score
0
20
40
60
Count
Attack
Watermarked
0.0
0.2
0.4
0.6
0.8
p-value
0
50
100
150
200
Count
Attack
Watermarked
(a) Detection performance
1
2
3
4
5
GPT-4 Judgment
0
20
40
60
Percentage (%)
(b) GPT-4 judgment
Figure 3: (a) Detection performance before (Watermarked) and after (Attack) our attack using
Llama2-7B with KGW (Kirchenbauer et al., 2023a). (b) Comparative evaluation on watermarked
texts against texts post-attack using GPT-4 as a judge. Scoring criteria: 1=post-attack response is
much better than watermarked one, 2=slightly better, 3=of similar quality, 4=slightly worse, 5=much
worse. Each example is included as two data points, one for each ordering of the two outputs in the
GPT-4 query.
sake of efficiency, we ran these tests for only 12 randomly selected prompts from the dataset. The
results are averaged over these 12 examples, and the GPT-4 judge results are averaged over the
two output orderings as well. In Figure 2, as the number of traversal steps increases, the z-score
steadily decreases while the average GPT-4 judgment score fluctuates around its initial value of 0.
In Appendix E.3 of the appendix, we showcase how the text, detection statistics, and GPT-4 judge
score change over the course of the random walk attack for a single example.
Controlling for perturbation oracle quality.
If the perturbation oracle (T5-XL v1.1) were strong
enough to produce texts on its own that are comparable in quality to the watermarked model M
(Llama-2-7B), then our attack would be trivial. As a baseline and to calibrate our GPT-4 judge, we
generate 40 samples by asking T5 for text completion iteratively: we feed the C4 news prefix for
text completion and concatenate the generated contents back as input until the length can match
the watermarked response. Then we ask GPT-4 to compare the watermarked responses against the
T5-generated counterparts using the quality judge prompt mentioned above. The results are stark:
the judge strongly prefers the watermarked Llama-2-7b output over the T5 output for every single
example. In other words, when the watermarked response is presented first, the judge always says it
is “much better” than the T5 response; and when the T5 response is presented first, the judge always
declares it “much worse”. This indicates that the quality oracle component of the attack was crucial
to its success.
The impact of response length on attack performance.
We plot the average z-score as a function
of T denoted as the token length of the generated text (Figure 4). Note that a length greater than
400 is much larger than the default settings KGW was tested on, and we observe that the z-score
keeps increasing for longer sequences, indicating detection performance degradation. On the other
hand, our attack plateaus, showing that we can remove the watermark for long texts which can be
essential for many practical tasks such as long-form reasoning (Nye et al., 2021), essay writing, etc.
5.3
Experimental Results on Vision-Language Models
As a proof of concept of the generality of our attack, we show that a version of the attack can
remove watermarks that have been used for image diffusion models. Specifically, we attack Invisible
18

400
450
500
550
600
650
700
750
800
Generated Text Length
0
2
4
6
z-score
Watermarked
Attack
400
500
600
700
800
Generated Text Length
10
4
10
3
10
2
10
1
p-value
Watermarked
Attack
Figure 4: Detection performance and w.r.t. the watermarked text length using Llama2-7B-Chat with
KGW (Kirchenbauer et al., 2023a). Results are aggregated across hundreds of examples.
(a) Invisible Watermark (Mountain, 2021)
(b) Stable Signature (Fernandez et al., 2023)
Figure 5: Qualitative examples of the watermarked images after (left) and before (right) our attack
for two watermarking schemes. Images are generated by prompting stable-diffusion-2-base with
the prompt “A long and winding beach, tropical, bright, simple, by Studio Ghibli and Greg Rutkowski,
artstation\n”. Detection and quality evaluation results: Invisible Watermark (p-value 3.5e-15 →
0.2354, CLIP score 34.82 →33.60, GPT-4 Judge 0), Stable Signature (p-value 1.3e-5 →0.468, CLIP
score 32.27 →31.58 , GPT-4 Judge 0).
Watermark (Mountain, 2021) applied to the diffusion model stable-diffusion-xl-base-1.0 (Podell et al.,
2023) and the Stable Signature (Fernandez et al., 2023) to sdxl-turbo (Sauer et al., 2023).
We first generate 200 images from randomly chosen prompts that are verified to be effective in
generating high-quality images and then filter those examples with p-values greater than 0.001. For
the remaining 141 valid examples (for Stable Signature, all are valid), we implement our perturbation
oracle using stable-diffusion-2-base (Rombach et al., 2022), which inpaints the image masked by
a random square mask of size 0.02 times the image size. Note that this is a weaker model than
the watermarked models, but it is not much weaker, so we see these results as preliminary. Our
quality oracle is implemented as a reward model trained on Human Preference Score v2 (Wu et al.,
2023b). We perturb the image for 100 valid random walk steps. Finally, the attacked images are
evaluated by calculating the CLIPScore (Hessel et al., 2021) and querying the multimodal API of
GPT-4 (OpenAI, 2023), gpt-4-turbo, to report the final quality comparison scores using the prompt
“[Prompt], Response A: [Image A], Response B: [Image B], Compare which of the two above figures is a better
response of higher-quality to the given prompt. Explain your reasoning step by step.”
19

Table 2: Average results on two image watermark schemes before and after attack, as applied to the
stable-diffusion-xl. We adopt the same evaluation protocol as on LMs (Table 1).
Framework
p-value
CLIP score
GPT-4 Judge
Stable Signature (Fernandez et al., 2023)
5.940e-6 →0.059
33.91 →33.40
-0.088
Invisible Watermark (Kirchenbauer et al., 2023a)
1.793e-5 →0.206
35.64 →35.51
- 0.138
We show in the Table 2 that our attack can successfully remove the watermarks with only slight
degradation in image quality using the same evaluation protocol as on LMs.
Figure 6: Un-watermarked example after our attack (red) and the corresponding original water-
marked text (green) for the query “Q: How come Tesla is worth so much money?” and an SAT essay
writing prompt. We do a word-by-word comparison and the differences between the two texts are
highlighted.
5.4
Qualitative Examples
In Figure 6 we display two concrete non-cherrypicked before-and-after examples of the effects of
our random walk attack on model outputs, so that the reader can get a sense of how quality is
affected by the process. We use one prompt from LFQA Fan et al. (2019) and one SAT essay prompt.
(For the SAT query, we use “In any field of inquiry, the beginner is more likely than the expert to make
important contributions. Write a response in which you discuss the extent to which you agree or disagree
with the statement and explain your reasoning for the position you take. In developing and supporting your
position, you should consider ways in which the statement might or might not hold true and explain how these
considerations shape your position.”). The comparison shows that the text after our attack can still be
coherent, fluent, and on-topic. Such high-quality non-watermarked examples are abundant in our
results. See Appendix D.1 for an example of the text at many intermediate rounds during the attack,
20

with corresponding detection results and quality judge evaluations. Moreover, Figure 5 showcases
two set of images before and after our attack. Some image details such as the background, and
shapes of objects get perturbed but the overall image can still fit the prompt provided. We again see
the p-values dramatically increase after our attack though with slight quality degradation.
6
Concluding Remarks
This work provides general impossibility results for strong watermarking schemes. Although our
specific attack is not very efficient, its primary advantage lies in its generality. Furthermore, while
our implementation targets language generation models, the concepts of quality and perturbation
oracles apply to all generative contexts, including audio-visual data. We posit, without formal proof,
that verification is simpler than generation; thus, a general enhancement in capabilities and flexibility
is likely to benefit the attacker (i.e., through improved quality and perturbation oracles) more than
the defender (i.e., through better planting and detection algorithms). Hence we are not optimistic
about the feasibility of strong watermarking schemes that would prevent a determined attacker
from removing the watermark while preserving quality. Nonetheless, weak watermarking schemes
do exist and can be useful, especially for safeguarding against unintentional leaks of AI-generated
data (e.g., into training sets) or in scenarios with minimal risk that assume a low-effort attacker.
Moreover, watermarking is just one tool in the arsenal of safety efforts for generative AI modeling.
Cryptography can also be used to establish provenance of data. Ultimately, in many instances of
disinformation, it is crucial to verify the data’s source rather than to confirm whether AI generated
it. Thus our goal is to set practical expectations regarding what watermarking schemes can achieve,
thereby contributing to the safer use of AI models.
Broader Impact
We believe that investigating the possibilities of watermarking schemes at this stage can help to
provide a better understanding of the inherent tradeoffs and give policymakers realistic expectations
of what watermarking can and cannot provide. While our techniques can be used to remove
watermarks from existing schemes, they are not the most efficient way to do so, with the benefit
being generality rather than efficiency. Moreover, our implementation is for text generation models,
while currently widely deployed watermarks are for image generation models. While it is possible
to adapt our ideas to attack deployed image generative models, we do not provide a recipe for
doing so in this paper. Thus, our work isn’t likely to be used by malicious actors. Rather, we see
exposing fundamental weaknesses in the watermarking paradigm as a contribution to the ongoing
discussion on how to mitigate the misuse of generative models. We hope our findings will be taken
into account by organizations building generative models and policymakers regulating them.
Acknowledgements
We thank John Thickstun and Tom Goldstein for helpful discussions. Kempner Institute computing
resources enabled this work. Hanlin Zhang is supported by an Eric and Susan Dunn Graduate
Fellowship. Boaz Barak acknowledges funding supported by the Kempner Institute, Simons In-
vestigator Fellowship, NSF grant DMS-2134157, DARPA grant W911NF2010021, and DOE grant
DE-SC0022199. Danilo Francati was supported by the Carlsberg Foundation under the Semper
Ardens Research Project CF18-112 (BCM). Ben Edelman acknowledges funding from the National
Science Foundation Graduate Research Fellowship Program under award DGE-214074. Daniele
Venturi was supported by project SERICS (PE00000014) under the MUR National Recovery and
Resilience Plan funded by the European Union - NextGenerationEU, and by Sapienza University
21

under the project SPECTRA. Giuseppe Ateniese was supported in part by grants from the Common-
wealth Cyber Initiative (CCI) and the Commonwealth Commercialization Fund (CCF), as well as
corporate gifts from Accenture, Lockheed Martin Integrated Systems, and Protocol Labs.
References
Scott Aaronson.
My ai safety lecture for ut effective altruism, 2022.
URL https://
scottaaronson.blog/?p=6823.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model
for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.
Michael Arnold.
Audio watermarking: Features, applications and algorithms.
In 2000 IEEE
International conference on multimedia and expo. ICME2000. Proceedings. Latest advances in the fast
changing world of multimedia (cat. no. 00TH8532), volume 2, pages 1013–1016. IEEE, 2000.
Mikhail J Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Florian Kerschbaum,
Dina Mohamed, and Sanket Naik. Natural language watermarking: Design, analysis, and a proof-
of-concept implementation. In Information Hiding: 4th International Workshop, IH 2001 Pittsburgh,
PA, USA, April 25–27, 2001 Proceedings 4, pages 185–200. Springer, 2001.
Mikhail J Atallah, Victor Raskin, Christian F Hempelmann, Mercan Karahan, Radu Sion, Umut
Topkara, and Katrina E Triezenberg. Natural language watermarking and tamperproofing. In
International workshop on information hiding, pages 196–212. Springer, 2002.
Arpit Bansal, Ping-yeh Chiang, Michael J Curry, Rajiv Jain, Curtis Wigington, Varun Manjunatha,
John P Dickerson, and Tom Goldstein. Certified neural network watermarks with randomized
smoothing. In International Conference on Machine Learning, pages 1450–1465. PMLR, 2022.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao,
and Aditya Ramesh. Improving image generation with better captions. 2023. URL https:
//cdn.openai.com/papers/dall-e-3.pdf.
F.M. Boland, J.J.K. O’Ruanaidh, and C. Dautzenberg. Watermarking digital images for copyright
protection. pages 326–330, 1995. doi: 10.1049/cp:19950674.
Laurence Boney, Ahmed H Tewfik, and Khaled N Hamdy. Digital watermarks for audio signals.
In Proceedings of the third IEEE international conference on multimedia computing and systems, pages
473–480. IEEE, 1996.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong
Huang. On the possibilities of ai-generated text detection. arXiv preprint arXiv:2304.04736, 2023.
Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. arXiv
preprint arXiv:2306.09194, 2023.
22

Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A Smith.
All that’s ‘human’is not gold: Evaluating human evaluation of generated text. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282–7296, 2021.
Ingemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. Digital watermarking
and steganography. Morgan kaufmann, 2007.
Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, and
Mingyuan Wang. Publicly detectable watermarking for language models. Cryptology ePrint
Archive, Paper 2023/1661, 2023.
URL https://eprint.iacr.org/2023/1661.
https:
//eprint.iacr.org/2023/1661.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:
Long form question answering. arXiv preprint arXiv:1907.09190, 2019.
Tina Fang, Martin Jaggi, and Katerina Argyraki. Generating steganographic text with lstms. arXiv
preprint arXiv:1705.10742, 2017.
Jianwei Fei, Zhihua Xia, Benedetta Tondi, and Mauro Barni. Supervised gan watermarking for
intellectual property protection. In 2022 IEEE International Workshop on Information Forensics and
Security (WIFS), pages 1–6. IEEE, 2022.
Pierre Fernandez, Guillaume Couairon, Hervé Jégou, Matthijs Douze, and Teddy Furon. The stable
signature: Rooting watermarks in latent diffusion models. arXiv preprint arXiv:2303.15435, 2023.
Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and
visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics: System Demonstrations, pages 111–116, 2019.
Jamie Hayes and George Danezis. Generating steganographic images via adversarial training.
Advances in neural information processing systems, 30, 2017.
Julian Hazell. Large language models can be used to effectively scale spear phishing campaigns.
arXiv preprint arXiv:2305.06972, 2023.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-
free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017.
Tiffany Hsu and Stuart A. Thompson.
Disinformation researchers raise alarms about
a.i. chatbots.
2023.
URL https://www.nytimes.com/2023/02/08/technology/
ai-chatbots-disinformation.html.
Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank
Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good?
on opportunities and challenges of large language models for education. Learning and individual
differences, 103:102274, 2023.
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A
watermark for large language models. arXiv preprint arXiv:2301.10226, 2023a.
23

John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun
Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks
for large language models. arXiv preprint arXiv:2306.04634, 2023b.
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant
conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327,
2023.
Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphras-
ing evades detectors of ai-generated text, but retrieval is an effective defense. arXiv preprint
arXiv:2303.13408, 2023.
Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free
watermarks for language models. arXiv preprint arXiv:2307.15593, 2023.
Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. Gpt detectors are biased
against non-native english writers. arXiv preprint arXiv:2304.02819, 2023.
Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. A semantic invariant robust watermark
for large language models, 2023.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Nils Lukas, Abdulrahman Diaa, Lucas Fenaux, and Florian Kerschbaum. Leveraging optimization
for adaptive attacks on image watermarks. arXiv preprint arXiv:2309.16952, 2023.
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. De-
tectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint
arXiv:2301.11305, 2023.
Shield Mountain.
Invisible watermark, 2021.
URL https://github.com/ShieldMnt/
invisible-watermark#supported-algorithms.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,
2021.
Executive Office of the President. Executive order 14110: Safe, secure, and trustworthy development
and use of artificial intelligence. Federal Register, 88:75191–75226, 2023.
OpenAI. Gpt-4 technical report, 2023.
OpenAI. How can educators respond to students presenting ai-generated content as their own? 2023-
11-03. https://help.openai.com/en/articles/8313351-how-can-educators-respond-to-students-
presenting-ai-generated-content-as-their-own.
Joseph JK O’Ruanaidh and Thierry Pun. Rotation, scale and translation invariant digital image
watermarking. In Proceedings of International Conference on Image Processing, volume 1, pages
536–539. IEEE, 1997.
24

Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,
and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis,
2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pages
8748–8763. PMLR, 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel
Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist
agent. Transactions on Machine Learning Research, 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, June 2022.
Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. Deepsigns: A generic watermarking
framework for ip protection of deep learning models. arXiv preprint arXiv:1804.00750, 2018.
Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao
Wang, and Soheil Feizi. Robustness of ai-image detectors: Fundamental limits and practical
attacks. arXiv preprint arXiv:2310.00076, 2023.
Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi.
Can ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156, 2023.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.
Ryoma Sato, Yuki Takezawa, Han Bao, Kenta Niwa, and Makoto Yamada. Embarrassingly simple
text watermarks. arXiv preprint arXiv:2310.08920, 2023.
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion
distillation. arXiv preprint arXiv:2311.17042, 2023.
John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Fe-
lipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language
models for dialogue. OpenAI blog, 2022.
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The
curse of recursion: Training on generated data makes models forget. arXiv preprint arxiv:2305.17493,
2023.
Matthew Tancik, Ben Mildenhall, and Ren Ng.
Stegastamp: Invisible hyperlinks in physical
photographs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
pages 2117–2126, 2020.
Edward Tian and Alexander Cui. Gptzero: Towards detection of ai-generated text using zero-shot
and supervised methods, 2023. URL https://gptzero.me.
25

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin’ichi Satoh. Embedding watermarks
into deep neural networks. In Proceedings of the 2017 ACM on international conference on multimedia
retrieval, pages 269–277, 2017.
Leslie G Valiant and Vijay V Vazirani. Np is as easy as detecting unique solutions. In Proceedings of
the seventeenth annual ACM symposium on Theory of computing, pages 458–463, 1985.
Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten
by large language models. arXiv preprint arXiv:2305.15047, 2023.
Hong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. Bot or human? detecting chatgpt imposters
with a single question. arXiv preprint arXiv:2305.06424, 2023.
Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fin-
gerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030,
2023.
Mika Westerlund. The emergence of deepfake technology: A review. Technology innovation manage-
ment review, 9(11), 2019.
Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F Wong, and Lidia S Chao. A sur-
vey on llm-gernerated text detection: Necessity, methods, and future directions. arXiv preprint
arXiv:2310.14724, 2023a.
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.
Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image
synthesis. arXiv preprint arXiv:2306.09341, 2023b.
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.
Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances
in Neural Information Processing Systems, 36, 2024.
Zhong-Liang Yang, Xiao-Qing Guo, Zi-Ming Chen, Yong-Feng Huang, and Yu-Jin Zhang. Rnn-stega:
Linguistic steganography based on recurrent neural networks. IEEE Transactions on Information
Forensics and Security, 14(5):1280–1295, 2018.
Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario Fritz. Artificial fingerprinting for
generative models: Rooting deepfake attribution in training data. In Proceedings of the IEEE/CVF
International conference on computer vision, pages 14448–14457, 2021.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending against neural fake news. Advances in neural information processing systems,
32, 2019.
Yu Zeng, Mo Zhou, Yuan Xue, and Vishal M Patel. Securing deep generative models with universal
adversarial signature. arXiv preprint arXiv:2305.16310, 2023.
Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and Ian
Molloy. Protecting intellectual property of deep neural networks with watermarking. In Proceedings
of the 2018 on Asia conference on computer and communications security, pages 159–172, 2018.
26

Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking
for ai-generated text. arXiv preprint arXiv:2306.17439, 2023a.
Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel,
Giovanni Vigna, Yu-Xiang Wang, and Lei Li. Invisible image watermarks are provably removable
using generative ai, 2023b.
Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe for
watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023c.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. arXiv preprint arXiv:2306.05685, 2023.
Jiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei. Hidden: Hiding data with deep networks.
In Proceedings of the European conference on computer vision (ECCV), pages 657–672, 2018.
Zachary M Ziegler, Yuntian Deng, and Alexander M Rush. Neural linguistic steganography. arXiv
preprint arXiv:1909.01496, 2019.
27

Appendix
Table of Contents
A
Questions & Answering
29
B
Mathematical Background and Random Walks
30
B.1
Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
B.2
Graphs, Random Walks, and Mixing Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
C
Extended Impossibility Theorem
31
C.1
Proof of Theorem 6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
D
Additional Experimental Results
35
D.1
Qualitative Results through Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
E
Experimental Details
36
E.1
Implementation Details of Attack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
E.2
Watermark Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
E.3
Prompt Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
28

A
Questions & Answering
Perturbation oracle only checks the quality of the output but does not guarantee content preser-
vation.
We believe that our theoretical definition of attack as quality preserving is the correct one for
both images and language models. For example, in the context of disinformation, if an attacker
generates an output with respect to a prompt X (e.g., “An image of a person doing X” or “A New
York Times story that X happened”) then they only care about the output’s fitness to the prompt. So,
if they are able to modify the output to erase the watermark while preserving quality (i.e. fitness to
prompt) this would be a successful attack, even if some semantic details are changed. Therefore, we
think that our metric of looking at quality (by a GPT4 judge, which crucially is not used in the attack
itself) is the correct way to evaluate success.
A common question is to explain the difference between ours and prior attacks, which attempted
to modify the output to be semantically equivalent while erasing the watermark.
One of the contributions of this paper is the formulation of the security of watermarking
with respect to a measure of quality which is fitness to the prompt, as opposed to a measure of
“similarity”. This formulation captures more closely what the attacker cares about: if they request an
output Y to a prompt X from a model, and want to perturb it to Y’ that is not watermarked, then
they don’t care whether Y’ is semantically identical to X but whether it has as good a fitness with the
prompt. This implies that we are supposed to evaluate text quality conditioned on the prompt as
opposed to metrics like perplexity that only evaluate the response.
Second, this formulation is necessary for the impossibility result to hold in generality. For
example, if the prompt is to write a story, then the model would have the freedom to choose
semantic details (e.g., names of characters, physical descriptions) that can be used to embed the
watermarking signal. It may be impossible to remove the watermark without modifying these
semantic details. Note that if the prompt did specify these details such as names of characters, then
neither the model would have the freedom to modify them to inject the watermark nor the attacker
could change them to erase it. Hence we believe fitness to the prompt (aka quality) is the right
measure for a watermark attack.
Since our attack has a different objective (quality preservation as opposed to semantic similarity),
it is also qualitatively different from prior works (Saberi et al., 2023; Sadasivan et al., 2023). We may
consider a similar space of perturbations, but our algorithm of whether to accept or reject a given
perturbation is fundamentally different.
What are the tradeoffs between type-I and type-II errors as the output space entropy changes?
We formalized only the minimal properties required to demonstrate our impossibility result
which does not include entropy, type-I and type-II errors. Setting aside the graph’s properties, our
impossibility result applies to watermarking schemes with a small false positive rate ϵpos (please refer
to Def. 4), which is fundamental for having a functional watermarking scheme. Indeed, small ϵpos
guarantees that, with (high) probability 1 −ϵpos, the scheme classifies as un-watermarked responses
computed independently from the secret key. This type of response corresponds to human-generated
content. This is a necessary property for having a functional watermarking scheme that permits
distinguishing between machine-generated and human-generated content (for more details see also
page 5, right after Def. 4).
To summarize, our impossibility result applies to any good enough watermarking scheme
independently from the output space entropy amount, type-I, and type-II errors in detecting the
29

watermark. This only makes our impossibility stronger and justifies the absence of entropy/type-
I/type-II formalizations.
B
Mathematical Background and Random Walks
B.1
Notation
We use the notation [n] = {1, 2, . . . , n}. Capital bold-face letters (such as X) are used to denote
random variables, small letters (such as x) to denote concrete values, calligraphic letters (such as X)
to denote sets, and serif letters (such as A) to denote algorithms. For a string x ∈{0, 1}, we let |x| be
its length; if X is a set, |X| represents the cardinality of X.
We denote with ⃗x ∈X n the column vector of length n with elements from X. Similarly, we
denote with ⃗X ∈X n×m the matrix with n rows, m columns, and elements from X. We write ⃗x⊤
(resp. ⃗X⊤) to denote the transpose vector of ⃗x (resp. the transpose matrix of ⃗X). Given a vector
⃗x ∈X n (resp. a matrix ⃗X ∈X n×m), we write ⃗x(i) ∈X (resp. ⃗X(i, j)) to denote the i-th element
of ⃗x (resp. the element located in the i-th row and j-th column of ⃗X). We say a vector ⃗x ∈Rn is
a (probability) distribution if P
i ⃗x(i) = 1; if ⃗x ∈Rn is a distribution, we use the notation ⃗x and x
interchangeably (where ⃗x(i) = x(i) for every i ∈[n]). When x is chosen uniformly from a set X, we
write x
r←X; if x is a distribution, we write x
r←x to denote the act of sampling x according to the
distribution x. That is, Pr
h
x = i : i
r←x
i
= x(i).
B.2
Graphs, Random Walks, and Mixing Time
Next, we focus on weighted directed graphs G = (V, E) composed of n vertices V = [n]. We denote by
weight(i, j) the weight of the edge (i, j) ∈E from i ∈V to j ∈V. We assume that weight(i, j) = 0 if
and only if (i, j) ̸∈E. Moreover, we define weight(i, ⋆) = P
j weight(i, j), i.e., weight(i, ⋆) represents
the sum of the edges’ weights with source vertex i.
Random Walks.
A t-step random walk over G is a probabilistic process such that, at each step, a
neighbor is selected from the neighbors of the current vertex, according to the weight distribution.
The transition matrix ⃗P ∈Rn×n of a random walk over a weighted directed graph G = (V, E) is
defined as ⃗P(i, j) = weight(i,j)
weight(i,⋆), i.e., ⃗P(i, j) represents the probability (based on the graph’s weights)
of reaching vertex j from vertex i. Let p0 ∈Rn be the starting distribution (of a random walk),
and let pt ∈Rn represent the distribution after t steps of the random walk. By definition, we have
pt+1(i) = P
j:(j,i)∈E pt(j) · ⃗P(j, i), where p(i) is the probability associated with i ∈V. Similarly, the
(global) distribution after a t-step random walk can be expressed as pt+1 = ⃗P ⊤· pt.
Below, we recall the definition of the stationary distribution of a random walk.
Definition 8 (Stationary distribution of random walks). Let G = (V, E) be a weighted directed graph
and ⃗P be the transition matrix of G. We say that ⃗π ∈Rn is a stationary distribution for ⃗P if ⃗P ⊤· ⃗π = ⃗π.
In other words, the above definition states that the probability distribution after a 1-step random
walk is the stationary distribution ⃗π if the starting distribution p0 is the stationary distribution itself.
By induction, this also implies that the distribution pt after a t-step random walk is equal to the
stationary distribution ⃗π if p0 = ⃗π, for every t ≥1.
The following theorem provides the stationary distribution for any weighted directed graph.
30

Theorem 3. Let G = (V, E) be a weighted directed graph and ⃗P be its transition matrix. The distribution
⃗π ∈Rn such that
∀i ∈V, ⃗π(i) =
weight(i, ⋆)
P
i∈V weight(i, ⋆) =
P
j∈V weight(i, j)
P
i∈V
P
j∈V weight(i, j) =
P
j∈V weight(i, j)
P
(i,j)∈E weight(i, j)
is a stationary distribution for ⃗P.
Interestingly, the stationary distribution is unique when the underlying directed graph G
is irreducible (i.e., the graph does not have leaf nodes). Moreover, if G is also aperiodic then,
independently of the initial starting distribution p0, a random walk converges to its stationary
distribution ⃗π as t →∞.
Theorem 4 (Convergence to the stationary distribution). If a weighted directed graph G = (V, E) is
irreducible and aperiodic, there exists a unique stationary distribution ⃗π. Moreover, for every p0 ∈Rn,
pt = (⃗P ⊤)t · p0 (i.e., the probability distribution after a t-step random walk with starting distribution p0)
converges to ⃗π as t →∞.
In this work, we are interested in setting a bound for t in the above theorem. In more detail, we
want to bound the minimum number of steps required by the random walk to get close enough to
its stationary distribution. This is known as the ϵdist-mixing time, and it can be bounded using the
second largest eigenvalue (in absolute value) of the transition matrix of the graph (note that the second
largest eigenvalue also has connections with the conductance of the graph). Below, we report the
formal definition of mixing time and its corresponding bound.
Definition 9 (ϵdist-mixing time). Let G = (V, E) be a weighted directed graph that is irreducible and
aperiodic (as in Theorem 4) and let ⃗P be its corresponding transition matrix. For any 0 < ϵdist ≤1,
the ϵdist-mixing time tmin(ϵdist) of ⃗P is the smallest number of steps t such that for every starting
distribution p0 ∈Rn, we have
pt −⃗π
 =
(⃗P ⊤)t · p0 −⃗π
 ≤ϵdist,
where ⃗π is the unique stationary distribution of ⃗P.
Theorem 5 (Bound on ϵdist-mixing time). Let G = (V, E) be a weighted directed graph that is irreducible
and aperiodic, and let ⃗P be its corresponding transition matrix. Also, let α1 ≥α2 ≥. . . ≥αn be the eigenval-
ues of ⃗P, and let ⃗π be the unique stationary distribution of ⃗P (Theorem 4). For g = max{|α2|, . . . , |αn|} and
πmin = min{⃗π(1), . . . ,⃗π(n)}, the ϵdist-mixing time of ⃗P is
tmin(ϵdist) ≤O

1
1 −g · log

1
πmin · ϵdist

.
C
Extended Impossibility Theorem
In this section, we include an extended version of the impossibility result which explicates the con-
crete relation between the adversarial advantage ϵ, the number of queries, and the ϵpert-preservation
of P. In addition, to make the result as generic as possible, we define P’s properties (i.e., irreducibility
and aperiodicity) w.r.t. any arbitrary v-th quality percentile (for v ∈[0, 100]) of the quality values
of all possible responses (of prompt x) that can be obtained by watermarking M ∈M (recall that
in Theorem 2 we used the median instead of the v-th quality percentile).
31

Formally, for v ∈[0, 100], let QM,x = {q1, q2, . . .} and qmin be defined as follows:
QM,x is defined as in Equation (1),
(3)
qmin =
min
M∈M,x∈X{qM,x} where qM,x is the v-th quality percentile of QM,x.
(4)
Below, we report the extended version of the impossibility result whose perturbation oracle is
defined w.r.t. qmin of Equation (4). The proof is given in Appendix C.1.
Theorem 6. Let Π = (Watermark, Detect) be a watermarking scheme for a class of generative models
M = {Mi : X →Y} with an associated quality function Q : X × Y →[0, 1]. Let P : X × Y →Y be a
perturbation oracle (defined over the same prompt space X and output space Y of the class M) with the same
associated quality function Q : X × Y →[0, 1] as Π.
Under the following conditions
1. The watermarking scheme Π has a false positive ϵpos-rate (Definition 4);
2. The perturbation oracle P is ϵpert-preserving (Definition 6);
3. For every non-watermarked model M ∈M, for every prompt x ∈X, for every quality q ∈[qmin, 1],
the q-quality x-prompt graph representation G≥q
x
of P is irreducible and aperiodic where qmin is the
minimum median defined in Equation (4) (for some arbitrary v ∈[0, 100]). Also, let ⃗πx,q be the unique
stationary distribution18 of the transition matrix ⃗Px,q of G≥q
x
(Definition 8) and, for ϵdist ∈[0, 1], let tx,q
be the ϵdist-mixing time of ⃗Px,q (Definition 9 and Theorem 5) defined as follows:
tx,q = ω
 
1
1 −max{|α(x,q)
2
|, . . . , |α(x,q)
n
|}
· log
 
1
π(x,q)
min · ϵdist
!!
where π(x,q)
min = min{⃗πx,q(1), . . . ,⃗πx,q(nx,q)}, nx,q = |V≥q
x |, and α(x,q)
1
≥α(x,q)
2
≥. . . ≥α(x,q)
n
are the
eigenvalues of the transition matrix ⃗Px,q;19
there exists an oracle-aided universal adversary AP(·,·),Q(·,·) that ϵ-breaks Π (Definition 5) by submitting at
most t queries to P where
ϵ =

1 −
v
100

(1 −ϵpos) (1 −ϵdist)
 
1 −
t−terr−1
X
k=0
t
k

(ϵpert)k (1 −ϵpert)t−k
!
, and
t =
max
x∈X,q∈[qmin,1]{tx,q} + terr.
Observe that the above theorem implies Theorem 2 since qmin is the minimum median quality (as
in Theorem 2) when v = 50.
For sufficiently small values of v, ϵdist, and sufficiently high values of ϵpert, the adversarial
advantage ϵ of Theorem 6 approaches 1 −ϵpos (i.e., the best possible advantage).20 In turn, if ϵpos is
also small (e.g., ϵpos ≤1
10), ϵ is close to 1.
18Recall that a random walk converges to its unique stationary distribution when the corresponding weighted directed
graph is irreducible and aperiodic (Theorem 4).
19Observe that tx,q is asymptotically larger than the ϵdist-mixing time of G≥q
x
as defined in Theorem 5.
20If ϵpert is not high enough, the same result can be achieved by increasing terr of Theorem 6. Note that by increasing terr,
we increase the overall number t of perturbation queries.
32

C.1
Proof of Theorem 6
Fix P and Q as defined in Theorem 6. Let M ∈M and let (k, Mk) (output by Watermark(M)) be the
original generative model and its watermarked version (with the associated secret-key k) that the
adversary is trying to break (recall that M and (k, Mk) are unknown to A). Consider the following
adversary AP(·,·),Q(·,·):
1. Receive a prompt x ∈X and a watermarked output y ∈Y (recall that y is computed as
y
r←Mk(x) as defined in the erasure attack experiment of Definition 5).
2. Initialize y0 = y and ctr = 0.
3. Send (x, y0) to Q and receive the answer q0.
4. For each i ∈[t], the adversary proceeds as follows (where t is as defined in Theorem 6):
(a) Send (x, yi−1) to P and receive the answer ˜y.
(b) Send (x, ˜y) to Q and receive the answer ˜q.
(c) If ˜q ≥q0, set yi = ˜y and increment the counter ctr (i.e., ctr = ctr + 1). Otherwise, if ˜q < q0,
set yi = yi−1.
5. Finally, output yt if ctr ≥t −terr (where t and terr are as defined in Theorem 6). Otherwise, if
ctr < t −terr, output ⊥(i.e., an error message).
To show that AP(·,·),Q(·,·) ϵ-breaks the watermarking scheme Π, we prove the following three lemmas.
Lemma 1. For every M ∈M, for every prompt x ∈X, we have
Pr[Q(x, y0) ≥qmin] ≥1 −
v
100,
where y0 = y is the watermarked output given as input to the adversary AP(·,·),Q(·,·).
Proof. The lemma follows by observing that qmin =
min
M∈M,x∈X{qM,x} where qM,x is the v-th quality
percentile of the list QM,x as defined in Equations (3) and (4). In other words, qmin is the minimum
among the v-th percentiles {qM,x} each calculated over all possible random coins of both Watermark
and the watermarked version of M. By definition, this implies that
Pr[Q(x, M(x)) ≥qmin] ≥1 −
v
100.
This concludes the proof of Lemma 1.
Lemma 2. For every M ∈M, for every prompt x ∈X, we have that
Pr
h
AP(·,·),Q(·,·)(x, y) ̸= ⊥
i
= 1 −
t−terr−1
X
k=0
t
k

(ϵpert)k (1 −ϵpert)t−k ,
where y0 = y is the watermarked output given as input to the adversary AP(·,·),Q(·,·).
Proof. Note that AP(·,·),Q(·,·)(x, y) outputs ⊥only if ctr < t −terr. Moreover, the counter ctr is not
incremented only when the perturbation oracle, on input (x, yi) (for some i ∈[t]), returns ˜y such that
Q(x, ˜y) < Q(x, yi). The latter occurs with probability at most 1 −ϵpert since the perturbation oracle P
is ϵpert-preserving (Definition 6).
33

Let X be the random variable describing the value of ctr at the end of the adversary’s computa-
tion. Then, we have that
Pr
h
AP(·,·),Q(·,·)(x, y) ̸= ⊥
i
= Pr[X ≥t −terr] = 1 −Pr[X < t −terr] = 1 −Pr[X ≤t −terr −1].
(5)
The probability Pr[X ≤t −terr −1] is characterized by a binomial distribution where the probability
of incrementing X (resp. not incrementing X) is ϵpert (resp. 1 −ϵpert). Formally,
Pr[X ≤t −terr −1] =
t−terr−1
X
k=0
t
k

(ϵpert)k (1 −ϵpert)t−k .
(6)
Lemma 2 follows by combining Equations (5) and (6).
Lemma 3. For every M ∈M, for every prompt x ∈X, conditioned on AP(·,·),Q(·,·)(x, y) ̸= ⊥and
Q(x, y) = q0 ≥qmin, we have that
AP(·,·),Q(·,·)(x, y) −⃗πx,q0
 ≤ϵdist
where ⃗πx,q0 is the unique stationary distribution of the transition matrix ⃗Px,q0 of G≥q0
x
.
Proof. Assume that AP(·,·),Q(·,·)(x, y) ̸= ⊥and Q(x, y) = q0 ≥qmin. It is easy to see that the computa-
tion of the adversary AP(·,·),Q(·,·)(x, y) is exactly a random walk over G≥q0
x
= (V≥q0
x
, E≥q0
x
), where q0 is
the quality of the watermarked output y = y0 given as input to the adversary. This is because at each
iteration i ∈[t], the adversary sets yi = ˜y (i.e., it moves from yi−1 to yi = ˜y according to G≥q0
x
) only
if ˜y has a quality of at least q0. This corresponds exactly to a random walk over the vertices with
quality at least q0, which is the definition of the q0-quality x-prompt graph representation G≥q0
x
of P.
In addition, the following conditions hold:
1. By leveraging Item 3 of Theorem 6, for every q ∈[qmin, 1], the q-quality x-prompt graph
representation G≥q
x
is irreducible and aperiodic. Thus, a random walk over the weighted
directed graph G≥q0
x
will eventually converge to its unique stationary distribution ⃗πx,q0 (recall
that q0 ≥qmin by assumption).
2. By assumption AP(·,·),Q(·,·)(x, y) ̸= ⊥. Thus, ctr ≥t −terr = maxx∈X,q∈[qmin,1]{tx,q} (as defined
in Item 3 of Theorem 6) which, in turn, implies ctr ≥tx,q0 since q0 ≥qmin. Note that ctr
corresponds to the number of steps performed by the adversary during the random walk over
the q0-quality x-prompt graph G≥q0
x
= (V≥q0
x
, E≥q0
x
).
By leveraging the above two conditions, we conclude that (i) a random walk over G≥q0
x
= (V≥q0
x
, E≥q0
x
)
converges to its unique stationary distribution ⃗πx,q0, and (ii) AP(·,·),Q(·,·)(x, y)’s random walk is
composed of at least ctr ≥tx,q0 steps where tx,q0 is asymptotically larger than the ϵdist-mixing time
of the transition matrix ⃗Px,q0 of G≥q0
x
(see Item 3 of Theorem 6). Thus, we conclude that the output
distribution of AP(·,·),Q(·,·)(x, y) is ϵdist-close to ⃗πx,q0, i.e.,
AP(·,·),Q(·,·)(x, y) −⃗πx,q0
 ≤ϵdist.
This concludes the proof.
By leveraging Lemmas 1 to 3, we have that for every M ∈M, for every prompt x ∈X, the
following conditions hold:
34

1. Let E be the event that Q(x, y) = q0 ≥qmin and AP(·,·),Q(·,·)(x, y) ̸= ⊥. Then, by leveraging Lem-
mas 1 and 2 we conclude that E occurs with probability at least

1 −
v
100
  
1 −
t−terr−1
X
k=0
t
k

(ϵpert)k (1 −ϵpert)t−k
!
,
where the probability is taken over (k, M) output by Watermark(M), the random coins of the
watermarked model Mk, and the perturbation oracle P.
2. Conditioned on E, the quality Q(x, yt) of yt ̸= ⊥(output by the adversary) is at least q0 ≥qmin.
This is because yt is the result of a random walk over the graph G≥q0
x
= (V≥q0
x
, E≥q0
x
) (of
the perturbation oracle P) composed of all vertices of quality at least q0 (see also the proof
of Lemma 3).
3. Conditioned on E, the output yt ̸= ⊥(produced by the adversary) is such that Detectk(x, yt) = 0
(i.e., yt is not watermarked) with probability at least (1 −ϵpos)(1 −ϵdist). This follows by
observing that, conditioned on E (i.e., Q(x, y) = q0 ≥qmin and AP(·,·),Q(·,·)(x, y) ̸= ⊥), the
output distribution of AP(·,·),Q(·,·)(x, y) is ϵdist-close to the unique stationary distribution ⃗πx,q0
(as defined in Lemma 3). In turn, since ⃗πx,q0 is independent of (k, Mk)
r←Watermark(M),21 we
have that Detectk(x, yt) = 0 with probability at least (1 −ϵpos) due to the false positive ϵpos-rate
(Definition 4) of the watermarking scheme Π.
By combining the above arguments, we conclude that, for every model M ∈M, for every prompt
x ∈X, the following probability holds:
Pr
h
Detectk(x, yt) = 0 and Q(x, yt) ≥Q(x, y) : y
r←Mk(x), yt
r←AP(·,·),Q(·,·)(x, y)
i
≥ϵ
where (k, Mk)
r←Watermark(M) and ϵ as defined in Theorem 6. This concludes the proof of Theo-
rem 6.
D
Additional Experimental Results
D.1
Qualitative Results through Steps
Given a C4 news article prefix ““Whoever gets him, they’ll be getting a good one,” David Montgomery said.
INDIANAPOLIS — Hakeem Butler has been surrounded by some of the best wide receivers on the planet this
week at the”, we showcase in Appendix E.3 the results of removing the watermark in the following
response: “Whoever gets him, they’ll be getting a good one,” David Montgomery said. INDIANAPOLIS —
Hakeem Butler has been surrounded by some of the best wide receivers on the planet this week at the NFL
Scouting Combine. They talk about technique, about routes, about film breakdowns. They discuss ideas for
how to improve their games. Butler is the outsider. He is the kid who always felt he belonged, but never quite
fit in. He is the kid who refused to sit on the bench, even if he didn’t know if he’d ever get in a game. He is the
kid who traveled from the middle of Iowa on a bus, all the way to Indianapolis this week, so he could show NFL
scouts he belonged. He is the kid, who when asked to sit down, he instead stood up. “I’m not a benchwarmer,”
Butler told The Athletic, “and it was like that all throughout college.” Butler is the guy who’s not afraid to
speak his mind.”
Note that for the initial watermarked response, the detection algorithm of KGW gives a z-score
of 7.340 and a p-value of 0.000. We report the results every 5 valid random walks. At each generation,
21This is because the output distribution of the perturbation oracle P (which, in turn, defines its corresponding graph) is
defined before the sampling of (k, Mk) according to Watermark(M).
35

we highlighted the different parts of the texts that are modified with the corresponding detection
(z-score, p-value) and GPT-4 quality judgment results.
We observe that the detection performance generally keeps decreasing while the new texts are
of high quality according to the quality oracle implemented as GPT-4. Especially, the oracle score
stays at 0 for the final several examples, showing that they are of similar quality to the watermarked
response.
E
Experimental Details
E.1
Implementation Details of Attack
We discuss our key design choices and implementations. In general, we found our attack effective
for all the settings considered and is not susceptible to the hyper-parameters and choices below.
Perturbation oracle. Recall that we generate watermarked texts with maximum generation
tokens of 200 or 512 and attack by replacing one span of the text at a time, thus we set the span
length to be 6 for all the attacks in the main table. For each infill, we do top-p sampling and p,
the minimum. and maximum infilled text length according to Table 5. Note that we generally use
the default hyper-parameters and don’t tune them too much. We incorporate backtracking into
the random walk as another error-reduction mechanism. If the perturbation oracle suggests many
candidates (above some “patience” threshold) without any of them passing the quality checks, then
we undo the most recent step of the walk.
Quality oracle.
Note that in general all kinds of watermarks are removable by omitting
contents but would degrade the quality. Therefore, it is important to ensure the quality at each step
of our iterative attack process. To realize this goal, we implement three alternative quality oracles,
trading off quality, cost, and efficiency (Table 3). In principle, attackers can tailor quality oracles
according to their needs, considering the trade-offs among quality, efficiency, and cost. For example,
i) Efficiency and Cost: malicious users can efficiently generate many high-quality texts using GPT-4
for automated phishing and strip the watermark using reward models as the quality oracle with
slight degradation in quality. ii) Quality: a student can wait a week to generate a solution with
GPT-4 to one assignment problem whose deadline is one week from now with hundreds of dollars
for paying for GPT-4 as a quality oracle.
Though our impossibility results are generic and the assumptions can be made stronger when
models become more capable, we instantiate our quality oracle reward models + GPT-3.5 for quality
checking for most experiments. The reasons are twofold: firstly, it’s much more efficient and less
expensive to use reward models for comparing responses to filter obviously bad texts; moreover, we
find that GPT-3.5 and GPT-4 have significant position biases (Zheng et al., 2023) that grade the first
response with higher quality when evaluating two responses to a given query (prompting details
are in Appendix E.3). We find that such position bias limitation is substantial in practice when the
response length is greater than 200 so it’d be hard to get non-watermarked examples in a reasonable
amount of queries even if powerful models can evaluate multiple nuance aspects of individual
responses. We detail the trade-offs in Table 3.
Design choices to ensure text quality. When masking a span, we split and mask the words
rather than tokens to avoid generating nonsensical words that degrade text quality. To alleviate the
impact of position bias on the quality oracle, we query it twice and categorize the results as win, tie,
or lose. Our goal is to get a new text we non-degrading quality, so we reject the new text if it loses in
both rounds at each step.
36

We filtered out low-quality watermarked examples (e.g. those with a great number of repetitions
(Zhao et al., 2023a)) since our perturbation oracle may persist in that repetition and the original text
would not pass quality oracle in the first place. This can be a reasonable intervention as repetition
violates our usable preassumption - we expect capable models like GPT-4 and future LMs not to
produce repetitiveness after watermarking their outputs.
Stopping conditions. Users are allowed to design the stopping condition according to their
needs and understanding of the detection: For the three watermarks we considered, we record
and score each intermediate example and early stop on the one with z-score less than 1.645, which
is practical whenever users roughly know the detection z-score threshold to make the stopping
iteration a tunable hyper-parameter; For high-stake settings where users don’t want to be detected
as using AI-generated texts when users don’t have any knowledge about the watermark scheme and
detector or when the watermark is not robust enough (for long texts etc such as EXP (Kuditipudi
et al., 2023), Unigram (Zhao et al., 2023a)), we stop when at least α of the words are replaced, where
α is set to be 70% for texts of length 500 or 512, and proportionally for other lengths, e.g. 80% for
length 600.
Table 3: The trade-offs of different quality oracle instantiation.
Model
Quality
(API) Costs
Efficiency
Reward Model
Reward Model + API Error Checking
GPT-3.5/4 API
E.2
Watermark Details
Denote |x|G as the number of green list tokens for a generated text with length T. We experiment
with three popular watermark schemes with their default hyper-parameters in general (Table 5).
• KGW (Kirchenbauer et al., 2023a) is about selecting a randomized set of “green” tokens before
a word is generated, and then softly promoting the use of green tokens during sampling, which
can be detected efficiently. We adopt a one proportion z-test, where z = 2 (|x|G −T/2) /
√
T. to
evaluate the null hypothesis H0:The text sequence is generated with no knowledge of the red list rule
and choose to reject the null hypothesis if z > 4.
• EXP Kuditipudi et al. (2023) is a distortion-free watermark framework that preserves the
original LM’s text distribution, at least up to some maximum number of generated tokens. For
detection, we compute a p-value with respect to a test statistic that measures the minimum cost
alignment between length k subsequences of the text and key, via a permutation test with 5000
resamples. If ϕ returns a small p-value then the text is likely watermarked.
• Unigram (Zhao et al., 2023a) is proposed as a watermark robust to edit property. We calculate
the number of green list tokens |x|G as well as the z-statistic z = (|x|G −γT) /
p
Tγ(1 −γ)
where γ = 0.5 means the fraction of the vocabulary included in the green list. The text is
predicted as AI-generated if z > 6. We set the strength parameter δ = 2, the larger δ is, the
lower the quality of the watermarked LM, but the easier it is to detect.
• Stable Signature (Fernandez et al., 2023) refers to a method of embedding invisible watermarks
into images generated by Latent Diffusion Models (LDMs). This approach involves fine-tuning
the latent decoder part of the image generator, conditioning it on a binary signature. The
modified decoder then generates images that inherently contain this watermark. A pre-trained
37

watermark extractor can later retrieve the hidden signature from any image produced by this
model, allowing for the identification of the image’s origins even after substantial modifications.
We utilize the existing VAE checkpoint from IMATAG and use its default hyper-parameters.
• Invisible watermark (Mountain, 2021) is a default (classic) watermark to the Stable Diffusion
model series, which utilizes frequency space transformations to embed watermarks invisibly
into images, using Discrete Wavelet Transform and Discrete Cosine Transform. These meth-
ods embed watermark bits into the significant frequency components of an image, ensuring
robustness against alterations like noise and compression while being sensitive to size and
aspect ratio changes. The process involves converting the image from BGR to YUV color space,
applying DWT to isolate frequency components, and then using DCT to embed the watermark,
making it imperceptible but extractable with appropriate algorithms.
Table 4: Default hyperparameters of our attack for LM watermarks.
KGW
EXP
Unigram
Attack steps
200
300
300
Secret key
15485863
42
0
z stopping threshold
1.645
Max watermarked length
{200, 512}
top-p of P
0.95
Span length
{4, 6, 8}
Num of spans l
1
Min infill length
{l, 1.5l}
Max infill length
{1.5l, 2l}
Table 5: Default hyperparameters of our attack for VLM watermarks.
Stable Signature
Invisible Watermark
Watermarked Model
Guidance scale
0
7.5
Num of inference steps
4
50
Secret key
0
Prompts
Gustavosta/Stable-Diffusion-Prompts
Watermark strength
Strong
-
Scheduler
KarrasDiffusionSchedulers
Attack
Attack steps
100
Square Mask ratio
0.02
Guidance scale
5
Num of inference steps
100
Scheduler
LMSDiscreteScheduler
38

E.3
Prompt Examples
Recall that it is a design choice to implement the quality oracle as a combination of a reward model
and GPT-3.5/GPT-4 to make sure the newly generated sample satisfies certain rubrics by rejecting
samples with errors. In our case, we use the prompt in Figure 7.
Prompt for avoiding basic errors that can degrade text quality
Below are two candidate responses to the query {query Q}:
Original response: {response A}
New response: {response B}
Text quality is affected by factors such as unnecessary repetitions, grammar, coherence,
relevance, and accuracy of the responses. Especially, having grammatical errors, repetitiveness,
capitalization errors or punctuation mistakes would greatly degrade the quality of a response.
Therefore, is the new modified response of equal or higher quality compared to the original
response? If so, answer Yes, otherwise answer No.
Figure 7: GPT-3.5 prompt for avoiding basic errors that can degrade text quality.
We showcase our hand-crafted prompts for our GPT-3.5 or GPT-4 quality oracle for final
evaluation (Figure 8). We also experiment with the system prompt in (Zheng et al., 2023), hoping to
reduce biases regarding position, length etc but find that it has very minor effects on the results.
Prompt for evaluating and comparing text quality with five choices.
Below are two candidate responses to the query {query Q}:
Response A: {response A}
Response B: {response B}
Compare which of the two responses above is a better response to the given query. Explain
your reasoning step by step.
(1) Response A is much better than response B
(2) Response A is slightly better than response B
(3) Responses A and B have similar quality
(4) Response B is slightly better than response A
(5) Response B is much better than response A
Figure 8: GPT-4 prompt for evaluating and comparing text quality with five choices
39

Figure 9: Intermediate text after attack (left, red) and its original watermarked text (right, green)
(a) The 5-th step, z-score = 5.35, p-value = 0.00, GPT-4 quality oracle score = 0.00.
(b) The 10-th step, z-score = 4.59, p-value = 0.00, GPT-4 quality oracle score = 0.00.
(c) The 15-th step, z-score = 2.78, p-value = 0.0027, GPT-4 quality oracle score = 0.00.
(d) The 20-th step, z-score = 1.66, p-value = 0.048, GPT-4 quality oracle score = 0.00.
40

(e) The 25-th step, z-score = 1.29, p-value = 0.0993, GPT-4 quality oracle score = 0.0
(f) The 30-th step, z-score = 0.654, p-value = 0.257, GPT-4 quality oracle score = 0.00.
(g) The 35-th step, z-score = 0.13, p-value = 0.448, GPT-4 quality oracle score = 0.00.
(h) The 40-th step, z-score = 0.52, p-value = 0.303, GPT-4 quality oracle score = 0.00.
41

(i) The 45-th step, z-score = -0.26, p-value = 0.602, GPT-4 quality oracle score = 0.00.
(j) The 50-th step, z-score = 0.52, p-value = 0.30, GPT-4 quality oracle score = 0.00.
42
