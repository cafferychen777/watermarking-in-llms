[
  {
    "title": "Is Watermarking LLM-Generated Code Robust?",
    "abstract": "We present the first study of the robustness of existing watermarking\ntechniques on Python code generated by large language models. Although existing\nworks showed that watermarking can be robust for natural language, we show that\nit is easy to remove these watermarks on code by semantic-preserving\ntransformations.",
    "authors": [
      "Tarun Suresh",
      "Shubham Ugare",
      "Gagandeep Singh",
      "Sasa Misailovic"
    ],
    "arxiv_id": "2403.17983",
    "url": "https://arxiv.org/abs/2403.17983"
  },
  {
    "title": "Towards Better Statistical Understanding of Watermarking LLMs",
    "abstract": "In this paper, we study the problem of watermarking large language models\n(LLMs). We consider the trade-off between model distortion and detection\nability and formulate it as a constrained optimization problem based on the\ngreen-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal\nsolution to the optimization problem enjoys a nice analytical property which\nprovides a better understanding and inspires the algorithm design for the\nwatermarking process. We develop an online dual gradient ascent watermarking\nalgorithm in light of this optimization formulation and prove its asymptotic\nPareto optimality between model distortion and detection ability. Such a result\nguarantees an averaged increased green list probability and henceforth\ndetection ability explicitly (in contrast to previous results). Moreover, we\nprovide a systematic discussion on the choice of the model distortion metrics\nfor the watermarking problem. We justify our choice of KL divergence and\npresent issues with the existing criteria of ``distortion-free'' and\nperplexity. Finally, we empirically evaluate our algorithms on extensive\ndatasets against benchmark algorithms.",
    "authors": [
      "Zhongze Cai",
      "Shang Liu",
      "Hanzhao Wang",
      "Huaiyang Zhong",
      "Xiaocheng Li"
    ],
    "arxiv_id": "2403.13027",
    "url": "https://arxiv.org/abs/2403.13027"
  },
  {
    "title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy",
    "abstract": "Text watermarking has emerged as a pivotal technique for identifying\nmachine-generated text. However, existing methods often rely on arbitrary\nvocabulary partitioning during decoding to embed watermarks, which compromises\nthe availability of suitable tokens and significantly degrades the quality of\nresponses. This study assesses the impact of watermarking on different\ncapabilities of large language models (LLMs) from a cognitive science lens. Our\nfinding highlights a significant disparity; knowledge recall and logical\nreasoning are more adversely affected than language generation. These results\nsuggest a more profound effect of watermarking on LLMs than previously\nunderstood. To address these challenges, we introduce Watermarking with Mutual\nExclusion (WatME), a novel approach leveraging linguistic prior knowledge of\ninherent lexical redundancy in LLM vocabularies to seamlessly integrate\nwatermarks. Specifically, WatME dynamically optimizes token usage during the\ndecoding process by applying a mutually exclusive rule to the identified\nlexical redundancies. This strategy effectively prevents the unavailability of\nappropriate tokens and preserves the expressive power of LLMs. We provide both\ntheoretical analysis and empirical evidence showing that WatME effectively\npreserves the diverse capabilities of LLMs while ensuring watermark\ndetectability.",
    "authors": [
      "Liang Chen",
      "Yatao Bian",
      "Yang Deng",
      "Deng Cai",
      "Shuaiyi Li",
      "Peilin Zhao",
      "Kam-fai Wong"
    ],
    "arxiv_id": "2311.09832",
    "url": "https://arxiv.org/abs/2311.09832"
  },
  {
    "title": "Topic-Based Watermarks for LLM-Generated Text",
    "abstract": "The indistinguishability of text generated by large language models (LLMs)\nfrom human-generated text poses significant challenges. Watermarking algorithms\nare potential solutions by embedding detectable signatures within LLM-generated\noutputs. However, current watermarking schemes lack robustness to a range of\nattacks such as text substitution or manipulation, undermining their\nreliability. This paper proposes a novel topic-based watermarking algorithm for\nLLMs, designed to enhance the robustness of watermarking in LLMs. Our approach\nleverages the topics extracted from input prompts or outputs of non-watermarked\nLLMs in the generation process of watermarked text. We dynamically utilize\ntoken lists on identified topics and adjust token sampling weights accordingly.\nBy using these topic-specific token biases, we embed a topic-sensitive\nwatermarking into the generated text. We outline the theoretical framework of\nour topic-based watermarking algorithm and discuss its potential advantages in\nvarious scenarios. Additionally, we explore a comprehensive range of attacks\nagainst watermarking algorithms, including discrete alterations, paraphrasing,\nand tokenizations. We demonstrate that our proposed watermarking scheme\nclassifies various watermarked text topics with 99.99% confidence and\noutperforms existing algorithms in terms of z-score robustness and the\nfeasibility of modeling text degradation by potential attackers, while\nconsidering the trade-offs between the benefits and losses of watermarking\nLLM-generated text.",
    "authors": [
      "Alexander Nemecek",
      "Yuzhou Jiang",
      "Erman Ayday"
    ],
    "arxiv_id": "2404.02138",
    "url": "https://arxiv.org/abs/2404.02138"
  },
  {
    "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
    "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
    "authors": [
      "Xiang Li",
      "Feng Ruan",
      "Huiyuan Wang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "arxiv_id": "2404.01245",
    "url": "https://arxiv.org/abs/2404.01245"
  },
  {
    "title": "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models",
    "abstract": "Watermarking generative-AI systems, such as LLMs, has gained considerable\ninterest, driven by their enhanced capabilities across a wide range of tasks.\nAlthough current approaches have demonstrated that small, context-dependent\nshifts in the word distributions can be used to apply and detect watermarks,\nthere has been little work in analyzing the impact that these perturbations\nhave on the quality of generated texts. Balancing high detectability with\nminimal performance degradation is crucial in terms of selecting the\nappropriate watermarking setting; therefore this paper proposes a simple\nanalysis framework where comparative assessment, a flexible NLG evaluation\nframework, is used to assess the quality degradation caused by a particular\nwatermark setting. We demonstrate that our framework provides easy\nvisualization of the quality-detection trade-off of watermark settings,\nenabling a simple solution to find an LLM watermark operating point that\nprovides a well-balanced performance. This approach is applied to two different\nsummarization systems and a translation system, enabling cross-model analysis\nfor a task, and cross-task analysis.",
    "authors": [
      "Piotr Molenda",
      "Adian Liusie",
      "Mark J. F. Gales"
    ],
    "arxiv_id": "2403.19548",
    "url": "https://arxiv.org/abs/2403.19548"
  },
  {
    "title": "Duwak: Dual Watermarks in Large Language Models",
    "abstract": "As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.",
    "authors": [
      "Chaoyi Zhu",
      "Jeroen Galjaard",
      "Pin-Yu Chen",
      "Lydia Y. Chen"
    ],
    "arxiv_id": "2403.13000",
    "url": "https://arxiv.org/abs/2403.13000"
  },
  {
    "title": "Lost in Overlap: Exploring Watermark Collision in LLMs",
    "abstract": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications.",
    "authors": [
      "Yiyang Luo",
      "Ke Lin",
      "Chao Gu"
    ],
    "arxiv_id": "2403.10020",
    "url": "https://arxiv.org/abs/2403.10020"
  },
  {
    "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
    "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.",
    "authors": [
      "Eva Giboulot",
      "Teddy Furon"
    ],
    "arxiv_id": "2403.04808",
    "url": "https://arxiv.org/abs/2403.04808"
  },
  {
    "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection",
    "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which\noffers feature extraction capabilities for addressing various downstream tasks\nin Natural Language Processing (NLP). Prior studies have shown that EaaS can be\nprone to model extraction attacks; nevertheless, this concern could be\nmitigated by adding backdoor watermarks to the text embeddings and subsequently\nverifying the attack models post-publication. Through the analysis of the\nrecent watermarking strategy for EaaS, EmbMarker, we design a novel CSE\n(Clustering, Selection, Elimination) attack that removes the backdoor watermark\nwhile maintaining the high utility of embeddings, indicating that the previous\nwatermarking approach can be breached. In response to this new threat, we\npropose a new protocol to make the removal of watermarks more challenging by\nincorporating multiple possible watermark directions. Our defense approach,\nWARDEN, notably increases the stealthiness of watermarks and has been\nempirically shown to be effective against CSE attack.",
    "authors": [
      "Anudeex Shetty",
      "Yue Teng",
      "Ke He",
      "Qiongkai Xu"
    ],
    "arxiv_id": "2403.01472",
    "url": "https://arxiv.org/abs/2403.01472"
  },
  {
    "title": "EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models",
    "abstract": "This paper introduces EmMark,a novel watermarking framework for protecting\nthe intellectual property (IP) of embedded large language models deployed on\nresource-constrained edge devices. To address the IP theft risks posed by\nmalicious end-users, EmMark enables proprietors to authenticate ownership by\nquerying the watermarked model weights and matching the inserted signatures.\nEmMark's novelty lies in its strategic watermark weight parameters selection,\nnsuring robustness and maintaining model quality. Extensive proof-of-concept\nevaluations of models from OPT and LLaMA-2 families demonstrate EmMark's\nfidelity, achieving 100% success in watermark extraction with model performance\npreservation. EmMark also showcased its resilience against watermark removal\nand forging attacks.",
    "authors": [
      "Ruisi Zhang",
      "Farinaz Koushanfar"
    ],
    "arxiv_id": "2402.17938",
    "url": "https://arxiv.org/abs/2402.17938"
  },
  {
    "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
    "abstract": "Large language models generate high-quality responses with potential\nmisinformation, underscoring the need for regulation by distinguishing\nAI-generated and human-written texts. Watermarking is pivotal in this context,\nwhich involves embedding hidden markers in texts during the LLM inference\nphase, which is imperceptible to humans. Achieving both the detectability of\ninserted watermarks and the semantic quality of generated texts is challenging.\nWhile current watermarking algorithms have made promising progress in this\ndirection, there remains significant scope for improvement. To address these\nchallenges, we introduce a novel multi-objective optimization (MOO) approach\nfor watermarking that utilizes lightweight networks to generate token-specific\nwatermarking logits and splitting ratios. By leveraging MOO to optimize for\nboth detection and semantic objective functions, our method simultaneously\nachieves detectability and semantic integrity. Experimental results show that\nour method outperforms current watermarking techniques in enhancing the\ndetectability of texts generated by LLMs while maintaining their semantic\ncoherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
    "authors": [
      "Mingjia Huo",
      "Sai Ashish Somayajula",
      "Youwei Liang",
      "Ruisi Zhang",
      "Farinaz Koushanfar",
      "Pengtao Xie"
    ],
    "arxiv_id": "2402.18059",
    "url": "https://arxiv.org/abs/2402.18059"
  },
  {
    "title": "No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices",
    "abstract": "Advances in generative models have made it possible for AI-generated text,\ncode, and images to mirror human-generated content in many applications.\nWatermarking, a technique that aims to embed information in the output of a\nmodel to verify its source, is useful for mitigating the misuse of such\nAI-generated content. However, we show that common design choices in LLM\nwatermarking schemes make the resulting systems surprisingly susceptible to\nattack -- leading to fundamental trade-offs in robustness, utility, and\nusability. To navigate these trade-offs, we rigorously study a set of simple\nyet effective attacks on common watermarking systems, and propose guidelines\nand defenses for LLM watermarking in practice.",
    "authors": [
      "Qi Pang",
      "Shengyuan Hu",
      "Wenting Zheng",
      "Virginia Smith"
    ],
    "arxiv_id": "2402.16187",
    "url": "https://arxiv.org/abs/2402.16187"
  },
  {
    "title": "Multi-Bit Distortion-Free Watermarking for Large Language Models",
    "abstract": "Methods for watermarking large language models have been proposed that\ndistinguish AI-generated text from human-generated text by slightly altering\nthe model output distribution, but they also distort the quality of the text,\nexposing the watermark to adversarial detection. More recently, distortion-free\nwatermarking methods were proposed that require a secret key to detect the\nwatermark. The prior methods generally embed zero-bit watermarks that do not\nprovide additional information beyond tagging a text as being AI-generated. We\nextend an existing zero-bit distortion-free watermarking method by embedding\nmultiple bits of meta-information as part of the watermark. We also develop a\ncomputationally efficient decoder that extracts the embedded information from\nthe watermark with low bit error rate.",
    "authors": [
      "Massieh Kordi Boroujeny",
      "Ya Jiang",
      "Kai Zeng",
      "Brian Mark"
    ],
    "arxiv_id": "2402.16578",
    "url": "https://arxiv.org/abs/2402.16578"
  },
  {
    "title": "Watermarking Makes Language Models Radioactive",
    "abstract": "This paper investigates the radioactivity of LLM-generated texts, i.e.\nwhether it is possible to detect that such input was used as training data.\nConventional methods like membership inference can carry out this detection\nwith some level of accuracy. We show that watermarked training data leaves\ntraces easier to detect and much more reliable than membership inference. We\nlink the contamination level to the watermark robustness, its proportion in the\ntraining set, and the fine-tuning process. We notably demonstrate that training\non watermarked synthetic instructions can be detected with high confidence\n(p-value < 1e-5) even when as little as 5% of training text is watermarked.\nThus, LLM watermarking, originally designed for detecting machine-generated\ntext, gives the ability to easily identify if the outputs of a watermarked LLM\nwere used to fine-tune another LLM.",
    "authors": [
      "Tom Sander",
      "Pierre Fernandez",
      "Alain Durmus",
      "Matthijs Douze",
      "Teddy Furon"
    ],
    "arxiv_id": "2402.14904",
    "url": "https://arxiv.org/abs/2402.14904"
  },
  {
    "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models",
    "abstract": "Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of cross-lingual consistency in text watermarking, which assesses the\nability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks, decreasing the AUCs to a random-guessing level\nwithout performance loss. Furthermore, we analyze two key factors that\ncontribute to the cross-lingual consistency in text watermarking and propose\nX-SIR as a defense method against CWRA. Code: https://github.com/zwhe99/X-SIR.",
    "authors": [
      "Zhiwei He",
      "Binglin Zhou",
      "Hongkun Hao",
      "Aiwei Liu",
      "Xing Wang",
      "Zhaopeng Tu",
      "Zhuosheng Zhang",
      "Rui Wang"
    ],
    "arxiv_id": "2402.14007",
    "url": "https://arxiv.org/abs/2402.14007"
  },
  {
    "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
    "abstract": "Large language models (LLMs) excellently generate human-like text, but also\nraise concerns about misuse in fake news and academic dishonesty.\nDecoding-based watermark, particularly the GumbelMax-trick-based watermark(GM\nwatermark), is a standout solution for safeguarding machine-generated texts due\nto its notable detectability. However, GM watermark encounters a major\nchallenge with generation diversity, always yielding identical outputs for the\nsame prompt, negatively impacting generation diversity and user experience. To\novercome this limitation, we propose a new type of GM watermark, the\nLogits-Addition watermark, and its three variants, specifically designed to\nenhance diversity. Among these, the GumbelSoft watermark (a softmax variant of\nthe Logits-Addition watermark) demonstrates superior performance in high\ndiversity settings, with its AUROC score outperforming those of the two\nalternative variants by 0.1 to 0.3 and surpassing other decoding-based\nwatermarking methods by a minimum of 0.1.",
    "authors": [
      "Jiayi Fu",
      "Xuandong Zhao",
      "Ruihan Yang",
      "Yuansen Zhang",
      "Jiangjie Chen",
      "Yanghua Xiao"
    ],
    "arxiv_id": "2402.12948",
    "url": "https://arxiv.org/abs/2402.12948"
  },
  {
    "title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text",
    "abstract": "Recent watermarked generation algorithms inject detectable signatures during\nlanguage generation to facilitate post-hoc detection. While token-level\nwatermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023)\napplies watermark on the semantic representation of sentences and demonstrates\npromising robustness. SemStamp employs locality-sensitive hashing (LSH) to\npartition the semantic space with arbitrary hyperplanes, which results in a\nsuboptimal tradeoff between robustness and speed. We propose k-SemStamp, a\nsimple yet effective enhancement of SemStamp, utilizing k-means clustering as\nan alternative of LSH to partition the embedding space with awareness of\ninherent semantic structure. Experimental results indicate that k-SemStamp\nsaliently improves its robustness and sampling efficiency while preserving the\ngeneration quality, advancing a more effective tool for machine-generated text\ndetection.",
    "authors": [
      "Abe Bohan Hou",
      "Jingyu Zhang",
      "Yichen Wang",
      "Daniel Khashabi",
      "Tianxing He"
    ],
    "arxiv_id": "2402.11399",
    "url": "https://arxiv.org/abs/2402.11399"
  },
  {
    "title": "Proving membership in LLM pretraining data via data watermarks",
    "abstract": "Detecting whether copyright holders' works were used in LLM pretraining is\npoised to be an important problem. This work proposes using data watermarks to\nenable principled detection with only black-box model access, provided that the\nrightholder contributed multiple training documents and watermarked them before\npublic release. By applying a randomly sampled data watermark, detection can be\nframed as hypothesis testing, which provides guarantees on the false detection\nrate. We study two watermarks: one that inserts random sequences, and another\nthat randomly substitutes characters with Unicode lookalikes. We first show how\nthree aspects of watermark design -- watermark length, number of duplications,\nand interference -- affect the power of the hypothesis test. Next, we study how\na watermark's detection strength changes under model and dataset scaling: while\nincreasing the dataset size decreases the strength of the watermark, watermarks\nremain strong if the model size also increases. Finally, we view SHA hashes as\nnatural watermarks and show that we can robustly detect hashes from\nBLOOM-176B's training data, as long as they occurred at least 90 times.\nTogether, our results point towards a promising future for data watermarks in\nreal world use.",
    "authors": [
      "Johnny Tian-Zheng Wei",
      "Ryan Yixiang Wang",
      "Robin Jia"
    ],
    "arxiv_id": "2402.10892",
    "url": "https://arxiv.org/abs/2402.10892"
  },
  {
    "title": "Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs",
    "abstract": "In this paper, we propose a new decoding method called Permute-and-Flip (PF)\ndecoder. It enjoys robustness properties similar to the standard sampling\ndecoder, but is provably up to 2x better in its quality-robustness tradeoff\nthan sampling and never worse than any other decoder. We also design a\ncryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but\nnaturally tailored for PF decoder. The watermarking scheme does not change the\ndistribution to sample, while allowing arbitrarily low false positive rate and\nhigh recall whenever the generated text has high entropy. Our experiments show\nthat the PF decoder (and its watermarked counterpart) significantly\noutperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms\nof perplexity, while retaining the same robustness (and detectability), hence\nmaking it a promising new approach for LLM decoding. The code is available at\nhttps://github.com/XuandongZhao/pf-decoding",
    "authors": [
      "Xuandong Zhao",
      "Lei Li",
      "Yu-Xiang Wang"
    ],
    "arxiv_id": "2402.05864",
    "url": "https://arxiv.org/abs/2402.05864"
  },
  {
    "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities of\ngenerating texts resembling human language. However, they can be misused by\ncriminals to create deceptive content, such as fake news and phishing emails,\nwhich raises ethical concerns. Watermarking is a key technique to address these\nconcerns, which embeds a message (e.g., a bit string) into a text generated by\nan LLM. By embedding the user ID (represented as a bit string) into generated\ntexts, we can trace generated texts to the user, known as content source\ntracing. The major limitation of existing watermarking techniques is that they\nachieve sub-optimal performance for content source tracing in real-world\nscenarios. The reason is that they cannot accurately or efficiently extract a\nlong message from a generated text. We aim to address the limitations.\n  In this work, we introduce a new watermarking method for LLM-generated text\ngrounded in pseudo-random segment assignment. We also propose multiple\ntechniques to further enhance the robustness of our watermarking algorithm. We\nconduct extensive experiments to evaluate our method. Our experimental results\nshow that our method substantially outperforms existing baselines in both\naccuracy and robustness on benchmark datasets. For instance, when embedding a\nmessage of length 20 into a 200-token generated text, our method achieves a\nmatch rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only\nachieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits\nwithin an edit distance of 17 on average for each paragraph under the same\nsetting.",
    "authors": [
      "Wenjie Qu",
      "Wengrui Zheng",
      "Tianyang Tao",
      "Dong Yin",
      "Yanze Jiang",
      "Zhihua Tian",
      "Wei Zou",
      "Jinyuan Jia",
      "Jiaheng Zhang"
    ],
    "arxiv_id": "2401.16820",
    "url": "https://arxiv.org/abs/2401.16820"
  },
  {
    "title": "Instructional Fingerprinting of Large Language Models",
    "abstract": "The exorbitant cost of training Large language models (LLMs) from scratch\nmakes it essential to fingerprint the models to protect intellectual property\nvia ownership authentication and to ensure downstream users and developers\ncomply with their license terms (e.g. restricting commercial use). In this\nstudy, we present a pilot study on LLM fingerprinting as a form of very\nlightweight instruction tuning. Model publisher specifies a confidential\nprivate key and implants it as an instruction backdoor that causes the LLM to\ngenerate specific text when the key is present. Results on 11 popularly-used\nLLMs showed that this approach is lightweight and does not affect the normal\nbehavior of the model. It also prevents publisher overclaim, maintains\nrobustness against fingerprint guessing and parameter-efficient training, and\nsupports multi-stage fingerprinting akin to MIT License. Code is available in\nhttps://cnut1648.github.io/Model-Fingerprint/.",
    "authors": [
      "Jiashu Xu",
      "Fei Wang",
      "Mingyu Derek Ma",
      "Pang Wei Koh",
      "Chaowei Xiao",
      "Muhao Chen"
    ],
    "arxiv_id": "2401.12255",
    "url": "https://arxiv.org/abs/2401.12255"
  },
  {
    "title": "Adaptive Text Watermark for Large Language Models",
    "abstract": "The advancement of Large Language Models (LLMs) has led to increasing\nconcerns about the misuse of AI-generated text, and watermarking for\nLLM-generated text has emerged as a potential solution. However, it is\nchallenging to generate high-quality watermarked text while maintaining strong\nsecurity, robustness, and the ability to detect watermarks without prior\nknowledge of the prompt or model. This paper proposes an adaptive watermarking\nstrategy to address this problem. To improve the text quality and maintain\nrobustness, we adaptively add watermarking to token distributions with high\nentropy measured using an auxiliary model and keep the low entropy token\ndistributions untouched. For the sake of security and to further minimize the\nwatermark's impact on text quality, instead of using a fixed green/red list\ngenerated from a random secret key, which can be vulnerable to decryption and\nforgery, we adaptively scale up the output logits in proportion based on the\nsemantic embedding of previously generated text using a well designed semantic\nmapping model. Our experiments involving various LLMs demonstrate that our\napproach achieves comparable robustness performance to existing watermark\nmethods. Additionally, the text generated by our method has perplexity\ncomparable to that of \\emph{un-watermarked} LLMs while maintaining security\neven under various attacks.",
    "authors": [
      "Yepeng Liu",
      "Yuheng Bu"
    ],
    "arxiv_id": "2401.13927",
    "url": "https://arxiv.org/abs/2401.13927"
  },
  {
    "title": "Excuse me, sir? Your language model is leaking (information)",
    "abstract": "We introduce a cryptographic method to hide an arbitrary secret payload in\nthe response of a Large Language Model (LLM). A secret key is required to\nextract the payload from the model's response, and without the key it is\nprovably impossible to distinguish between the responses of the original LLM\nand the LLM that hides a payload. In particular, the quality of generated text\nis not affected by the payload. Our approach extends a recent result of Christ,\nGunn and Zamir (2023) who introduced an undetectable watermarking scheme for\nLLMs.",
    "authors": [
      "Or Zamir"
    ],
    "arxiv_id": "2401.10360",
    "url": "https://arxiv.org/abs/2401.10360"
  },
  {
    "title": "Cross-Attention Watermarking of Large Language Models",
    "abstract": "A new approach to linguistic watermarking of language models is presented in\nwhich information is imperceptibly inserted into the output text while\npreserving its readability and original meaning. A cross-attention mechanism is\nused to embed watermarks in the text during inference. Two methods using\ncross-attention are presented that minimize the effect of watermarking on the\nperformance of a pretrained model. Exploration of different training strategies\nfor optimizing the watermarking and of the challenges and implications of\napplying this approach in real-world scenarios clarified the tradeoff between\nwatermark robustness and text quality. Watermark selection substantially\naffects the generated output for high entropy sentences. This proactive\nwatermarking approach has potential application in future model development.",
    "authors": [
      "Folco Bertini Baldassini",
      "Huy H. Nguyen",
      "Ching-Chung Chang",
      "Isao Echizen"
    ],
    "arxiv_id": "2401.06829",
    "url": "https://arxiv.org/abs/2401.06829"
  },
  {
    "title": "Optimizing watermarks for large language models",
    "abstract": "With the rise of large language models (LLMs) and concerns about potential\nmisuse, watermarks for generative LLMs have recently attracted much attention.\nAn important aspect of such watermarks is the trade-off between their\nidentifiability and their impact on the quality of the generated text. This\npaper introduces a systematic approach to this trade-off in terms of a\nmulti-objective optimization problem. For a large class of robust, efficient\nwatermarks, the associated Pareto optimal solutions are identified and shown to\noutperform the currently default watermark.",
    "authors": [
      "Bram Wouters"
    ],
    "arxiv_id": "2312.17295",
    "url": "https://arxiv.org/abs/2312.17295"
  },
  {
    "title": "Towards Optimal Statistical Watermarking",
    "abstract": "We study statistical watermarking by formulating it as a hypothesis testing\nproblem, a general framework which subsumes all previous statistical\nwatermarking methods. Key to our formulation is a coupling of the output tokens\nand the rejection region, realized by pseudo-random generators in practice,\nthat allows non-trivial trade-offs between the Type I error and Type II error.\nWe characterize the Uniformly Most Powerful (UMP) watermark in the general\nhypothesis testing setting and the minimax Type II error in the model-agnostic\nsetting. In the common scenario where the output is a sequence of $n$ tokens,\nwe establish nearly matching upper and lower bounds on the number of i.i.d.\ntokens required to guarantee small Type I and Type II errors. Our rate of\n$\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$\nhighlights potentials for improvement from the rate of $h^{-2}$ in the previous\nworks. Moreover, we formulate the robust watermarking problem where the user is\nallowed to perform a class of perturbations on the generated texts, and\ncharacterize the optimal Type II error of robust UMP tests via a linear\nprogramming problem. To the best of our knowledge, this is the first systematic\nstatistical treatment on the watermarking problem with near-optimal rates in\nthe i.i.d. setting, which might be of interest for future works.",
    "authors": [
      "Baihe Huang",
      "Hanlin Zhu",
      "Banghua Zhu",
      "Kannan Ramchandran",
      "Michael I. Jordan",
      "Jason D. Lee",
      "Jiantao Jiao"
    ],
    "arxiv_id": "2312.07930",
    "url": "https://arxiv.org/abs/2312.07930"
  },
  {
    "title": "A Survey of Text Watermarking in the Era of Large Language Models",
    "abstract": "Text watermarking algorithms are crucial for protecting the copyright of\ntextual content. Historically, their capabilities and application scenarios\nwere limited. However, recent advancements in large language models (LLMs) have\nrevolutionized these techniques. LLMs not only enhance text watermarking\nalgorithms with their advanced abilities but also create a need for employing\nthese algorithms to protect their own copyrights or prevent potential misuse.\nThis paper conducts a comprehensive survey of the current state of text\nwatermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their detectability, impact on text\nor LLM quality, robustness under target or untargeted attacks; (3) potential\napplication scenarios for text watermarking technology; (4) current challenges\nand future directions for text watermarking. This survey aims to provide\nresearchers with a thorough understanding of text watermarking technology in\nthe era of LLM, thereby promoting its further advancement.",
    "authors": [
      "Aiwei Liu",
      "Leyi Pan",
      "Yijian Lu",
      "Jingjing Li",
      "Xuming Hu",
      "Xi Zhang",
      "Lijie Wen",
      "Irwin King",
      "Hui Xiong",
      "Philip S. Yu"
    ],
    "arxiv_id": "2312.07913",
    "url": "https://arxiv.org/abs/2312.07913"
  },
  {
    "title": "On the Learnability of Watermarks for Language Models",
    "abstract": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.",
    "authors": [
      "Chenchen Gu",
      "Xiang Lisa Li",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "arxiv_id": "2312.04469",
    "url": "https://arxiv.org/abs/2312.04469"
  },
  {
    "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
    "abstract": "With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.",
    "authors": [
      "Karanpartap Singh",
      "James Zou"
    ],
    "arxiv_id": "2312.02382",
    "url": "https://arxiv.org/abs/2312.02382"
  },
  {
    "title": "Mark My Words: Analyzing and Evaluating Language Model Watermarks",
    "abstract": "The capabilities of large language models have grown significantly in recent\nyears and so too have concerns about their misuse. It is important to be able\nto distinguish machine-generated text from human-authored content. Prior works\nhave proposed numerous schemes to watermark text, which would benefit from a\nsystematic evaluation framework. This work focuses on LLM output watermarking\ntechniques - as opposed to image or model watermarks - and proposes Mark My\nWords, a comprehensive benchmark for them under different natural language\ntasks. We focus on three main metrics: quality, size (i.e., the number of\ntokens needed to detect a watermark), and tamper resistance (i.e., the ability\nto detect a watermark after perturbing marked text). Current watermarking\ntechniques are nearly practical enough for real-world use: Kirchenbauer et al.\n[33]'s scheme can watermark models like Llama 2 7B-chat or Mistral-7B-Instruct\nwith no perceivable loss in quality on natural language tasks, the watermark\ncan be detected with fewer than 100 tokens, and their scheme offers good tamper\nresistance to simple perturbations. However, they struggle to efficiently\nwatermark code generations. We publicly release our benchmark\n(https://github.com/wagner-group/MarkMyWords).",
    "authors": [
      "Julien Piet",
      "Chawin Sitawarin",
      "Vivian Fang",
      "Norman Mu",
      "David Wagner"
    ],
    "arxiv_id": "2312.00273",
    "url": "https://arxiv.org/abs/2312.00273"
  },
  {
    "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
    "abstract": "Potential harms of Large Language Models such as mass misinformation and\nplagiarism can be partially mitigated if there exists a reliable way to detect\nmachine generated text. In this paper, we propose a new watermarking method to\ndetect machine-generated texts. Our method embeds a unique pattern within the\ngenerated text, ensuring that while the content remains coherent and natural to\nhuman readers, it carries distinct markers that can be identified\nalgorithmically. Specifically, we intervene with the token sampling process in\na way which enables us to trace back our token choices during the detection\nphase. We show how watermarking affects textual quality and compare our\nproposed method with a state-of-the-art watermarking method in terms of\nrobustness and detectability. Through extensive experiments, we demonstrate the\neffectiveness of our watermarking scheme in distinguishing between watermarked\nand non-watermarked text, achieving high detection rates while maintaining\ntextual quality.",
    "authors": [
      "Kaan Efe Keleş",
      "Ömer Kaan Gürbüz",
      "Mucahid Kutlu"
    ],
    "arxiv_id": "2311.18054",
    "url": "https://arxiv.org/abs/2311.18054"
  },
  {
    "title": "Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring",
    "abstract": "The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.",
    "authors": [
      "Yuhang Li",
      "Yihan Wang",
      "Zhouxing Shi",
      "Cho-Jui Hsieh"
    ],
    "arxiv_id": "2311.09668",
    "url": "https://arxiv.org/abs/2311.09668"
  },
  {
    "title": "Downstream Trade-offs of a Family of Text Watermarks",
    "abstract": "Watermarking involves implanting an imperceptible signal into generated text\nthat can later be detected via statistical tests. A prominent family of\nwatermarking strategies for LLMs embeds this signal by upsampling a\n(pseudorandomly-chosen) subset of tokens at every generation step. However,\nsuch signals alter the model's output distribution and can have unintended\neffects on its downstream performance. In this work, we evaluate the\nperformance of LLMs watermarked using three different strategies over a diverse\nsuite of tasks including those cast as k-class classification (CLS), multiple\nchoice question answering (MCQ), short-form generation (e.g., open-ended\nquestion answering) and long-form generation (e.g., translation) tasks. We find\nthat watermarks (under realistic hyperparameters) can cause significant drops\nin LLMs' effective utility across all tasks. We observe drops of 10 to 20% in\nCLS tasks in the average case, which shoot up to 100% in the worst case. We\nnotice degradations of about 7% in MCQ tasks, 10-15% in short-form generation,\nand 5-15% in long-form generation tasks. Our findings highlight the trade-offs\nthat users should be cognizant of when using watermarked models.",
    "authors": [
      "Anirudh Ajith",
      "Sameer Singh",
      "Danish Pruthi"
    ],
    "arxiv_id": "2311.09816",
    "url": "https://arxiv.org/abs/2311.09816"
  },
  {
    "title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
    "abstract": "To mitigate the potential misuse of large language models (LLMs), recent\nresearch has developed watermarking algorithms, which restrict the generation\nprocess to leave an invisible trace for watermark detection. Due to the\ntwo-stage nature of the task, most studies evaluate the generation and\ndetection separately, thereby presenting a challenge in unbiased, thorough, and\napplicable evaluations. In this paper, we introduce WaterBench, the first\ncomprehensive benchmark for LLM watermarks, in which we design three crucial\nfactors: (1) For benchmarking procedure, to ensure an apples-to-apples\ncomparison, we first adjust each watermarking method's hyper-parameter to reach\nthe same watermarking strength, then jointly evaluate their generation and\ndetection performance. (2) For task selection, we diversify the input and\noutput length to form a five-category taxonomy, covering $9$ tasks. (3) For\nevaluation metric, we adopt the GPT4-Judge for automatically evaluating the\ndecline of instruction-following abilities after watermarking. We evaluate $4$\nopen-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe\nthe common struggles for current methods on maintaining the generation quality.\nThe code and data are available at https://github.com/THU-KEG/WaterBench.",
    "authors": [
      "Shangqing Tu",
      "Yuliang Sun",
      "Yushi Bai",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "arxiv_id": "2311.07138",
    "url": "https://arxiv.org/abs/2311.07138"
  },
  {
    "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
    "abstract": "Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.",
    "authors": [
      "Hanlin Zhang",
      "Benjamin L. Edelman",
      "Danilo Francati",
      "Daniele Venturi",
      "Giuseppe Ateniese",
      "Boaz Barak"
    ],
    "arxiv_id": "2311.04378",
    "url": "https://arxiv.org/abs/2311.04378"
  },
  {
    "title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models",
    "abstract": "We present REMARK-LLM, a novel efficient, and robust watermarking framework\ndesigned for texts generated by large language models (LLMs). Synthesizing\nhuman-like content using LLMs necessitates vast computational resources and\nextensive datasets, encapsulating critical intellectual property (IP). However,\nthe generated content is prone to malicious exploitation, including spamming\nand plagiarism. To address the challenges, REMARK-LLM proposes three new\ncomponents: (i) a learning-based message encoding module to infuse binary\nsignatures into LLM-generated texts; (ii) a reparameterization module to\ntransform the dense distributions from the message encoding to the sparse\ndistribution of the watermarked textual tokens; (iii) a decoding module\ndedicated for signature extraction; Furthermore, we introduce an optimized beam\nsearch algorithm to guarantee the coherence and consistency of the generated\ncontent. REMARK-LLM is rigorously trained to encourage the preservation of\nsemantic integrity in watermarked content, while ensuring effective watermark\nretrieval. Extensive evaluations on multiple unseen datasets highlight\nREMARK-LLM proficiency and transferability in inserting 2 times more signature\nbits into the same texts when compared to prior art, all while maintaining\nsemantic integrity. Furthermore, REMARK-LLM exhibits better resilience against\na spectrum of watermark detection and removal attacks.",
    "authors": [
      "Ruisi Zhang",
      "Shehzeen Samarah Hussain",
      "Paarth Neekhara",
      "Farinaz Koushanfar"
    ],
    "arxiv_id": "2310.12362",
    "url": "https://arxiv.org/abs/2310.12362"
  },
  {
    "title": "Embarrassingly Simple Text Watermarks",
    "abstract": "We propose Easymark, a family of embarrassingly simple yet effective\nwatermarks. Text watermarking is becoming increasingly important with the\nadvent of Large Language Models (LLM). LLMs can generate texts that cannot be\ndistinguished from human-written texts. This is a serious problem for the\ncredibility of the text. Easymark is a simple yet effective solution to this\nproblem. Easymark can inject a watermark without changing the meaning of the\ntext at all while a validator can detect if a text was generated from a system\nthat adopted Easymark or not with high credibility. Easymark is extremely easy\nto implement so that it only requires a few lines of code. Easymark does not\nrequire access to LLMs, so it can be implemented on the user-side when the LLM\nproviders do not offer watermarked LLMs. In spite of its simplicity, it\nachieves higher detection accuracy and BLEU scores than the state-of-the-art\ntext watermarking methods. We also prove the impossibility theorem of perfect\nwatermarking, which is valuable in its own right. This theorem shows that no\nmatter how sophisticated a watermark is, a malicious user could remove it from\nthe text, which motivate us to use a simple watermark such as Easymark. We\ncarry out experiments with LLM-generated texts and confirm that Easymark can be\ndetected reliably without any degradation of BLEU and perplexity, and\noutperform state-of-the-art watermarks in terms of both quality and\nreliability.",
    "authors": [
      "Ryoma Sato",
      "Yuki Takezawa",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ],
    "arxiv_id": "2310.08920",
    "url": "https://arxiv.org/abs/2310.08920"
  },
  {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "abstract": "In recent years, large language models (LLMs) have achieved remarkable\nperformances in various NLP tasks. They can generate texts that are\nindistinguishable from those written by humans. Such remarkable performance of\nLLMs increases their risk of being used for malicious purposes, such as\ngenerating fake news articles. Therefore, it is necessary to develop methods\nfor distinguishing texts written by LLMs from those written by humans.\nWatermarking is one of the most powerful methods for achieving this. Although\nexisting watermarking methods have successfully detected texts generated by\nLLMs, they significantly degrade the quality of the generated texts. In this\nstudy, we propose the Necessary and Sufficient Watermark (NS-Watermark) for\ninserting watermarks into generated texts without degrading the text quality.\nMore specifically, we derive minimum constraints required to be imposed on the\ngenerated texts to distinguish whether LLMs or humans write the texts. Then, we\nformulate the NS-Watermark as a constrained optimization problem and propose an\nefficient algorithm to solve it. Through the experiments, we demonstrate that\nthe NS-Watermark can generate more natural texts than existing watermarking\nmethods and distinguish more accurately between texts written by LLMs and those\nwritten by humans. Especially in machine translation tasks, the NS-Watermark\ncan outperform the existing watermarking method by up to 30 BLEU scores.",
    "authors": [
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ],
    "arxiv_id": "2310.00833",
    "url": "https://arxiv.org/abs/2310.00833"
  },
  {
    "title": "Functional Invariants to Watermark Large Transformers",
    "abstract": "The rapid growth of transformer-based models increases the concerns about\ntheir integrity and ownership insurance. Watermarking addresses this issue by\nembedding a unique identifier into the model, while preserving its performance.\nHowever, most existing approaches require to optimize the weights to imprint\nthe watermark signal, which is not suitable at scale due to the computational\ncost. This paper explores watermarks with virtually no computational cost,\napplicable to a non-blind white-box setting (assuming access to both the\noriginal and watermarked networks). They generate functionally equivalent\ncopies by leveraging the models' invariance, via operations like dimension\npermutations or scaling/unscaling. This enables to watermark models without any\nchange in their outputs and remains stealthy. Experiments demonstrate the\neffectiveness of the approach and its robustness against various model\ntransformations (fine-tuning, quantization, pruning), making it a practical\nsolution to protect the integrity of large models.",
    "authors": [
      "Pierre Fernandez",
      "Guillaume Couairon",
      "Teddy Furon",
      "Matthijs Douze"
    ],
    "arxiv_id": "2310.11446",
    "url": "https://arxiv.org/abs/2310.11446"
  },
  {
    "title": "Watermarking LLMs with Weight Quantization",
    "abstract": "Abuse of large language models reveals high risks as large language models\nare being deployed at an astonishing speed. It is important to protect the\nmodel weights to avoid malicious usage that violates licenses of open-source\nlarge language models. This paper proposes a novel watermarking strategy that\nplants watermarks in the quantization process of large language models without\npre-defined triggers during inference. The watermark works when the model is\nused in the fp32 mode and remains hidden when the model is quantized to int8,\nin this way, the users can only inference the model without further supervised\nfine-tuning of the model. We successfully plant the watermark into open-source\nlarge language model weights including GPT-Neo and LLaMA. We hope our proposed\nmethod can provide a potential direction for protecting model weights in the\nera of large language model applications.",
    "authors": [
      "Linyang Li",
      "Botian Jiang",
      "Pengyu Wang",
      "Ke Ren",
      "Hang Yan",
      "Xipeng Qiu"
    ],
    "arxiv_id": "2310.11237",
    "url": "https://arxiv.org/abs/2310.11237"
  },
  {
    "title": "A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models",
    "abstract": "Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models. A challenge in the domain lies in preserving the distribution\nof original generated content after watermarking. Our research extends and\nimproves upon existing watermarking framework, placing emphasis on the\nimportance of a \\textbf{Di}stribution-\\textbf{P}reserving (DiP) watermark.\nContrary to the current strategies, our proposed DiPmark simultaneously\npreserves the original token distribution during watermarking\n(distribution-preserving), is detectable without access to the language model\nAPI and prompts (accessible), and is provably robust to moderate changes of\ntokens (resilient). DiPmark operates by selecting a random set of tokens prior\nto the generation of a word, then modifying the token distribution through a\ndistribution-preserving reweight function to enhance the probability of these\nselected tokens during the sampling process. Extensive empirical evaluation on\nvarious language models and tasks demonstrates our approach's\ndistribution-preserving property, accessibility, and resilience, making it a\neffective solution for watermarking tasks that demand impeccable quality\npreservation.",
    "authors": [
      "Yihan Wu",
      "Zhengmian Hu",
      "Junfeng Guo",
      "Hongyang Zhang",
      "Heng Huang"
    ],
    "arxiv_id": "2310.07710",
    "url": "https://arxiv.org/abs/2310.07710"
  },
  {
    "title": "A Semantic Invariant Robust Watermark for Large Language Models",
    "abstract": "Watermark algorithms for large language models (LLMs) have achieved extremely\nhigh accuracy in detecting text generated by LLMs. Such algorithms typically\ninvolve adding extra watermark logits to the LLM's logits at each generation\nstep. However, prior algorithms face a trade-off between attack robustness and\nsecurity robustness. This is because the watermark logits for a token are\ndetermined by a certain number of preceding tokens; a small number leads to low\nsecurity robustness, while a large number results in insufficient attack\nrobustness. In this work, we propose a semantic invariant watermarking method\nfor LLMs that provides both attack robustness and security robustness. The\nwatermark logits in our work are determined by the semantics of all preceding\ntokens. Specifically, we utilize another embedding LLM to generate semantic\nembeddings for all preceding tokens, and then these semantic embeddings are\ntransformed into the watermark logits through our trained watermark model.\nSubsequent analyses and experiments demonstrated the attack robustness of our\nmethod in semantically invariant settings: synonym substitution and text\nparaphrasing settings. Finally, we also show that our watermark possesses\nadequate security robustness. Our code and data are available at\n\\href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\\_Watermark}.\nAdditionally, our algorithm could also be accessed through MarkLLM\n\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.",
    "authors": [
      "Aiwei Liu",
      "Leyi Pan",
      "Xuming Hu",
      "Shiao Meng",
      "Lijie Wen"
    ],
    "arxiv_id": "2310.06356",
    "url": "https://arxiv.org/abs/2310.06356"
  },
  {
    "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
    "abstract": "Existing watermarking algorithms are vulnerable to paraphrase attacks because\nof their token-level design. To address this issue, we propose SemStamp, a\nrobust sentence-level semantic watermarking algorithm based on\nlocality-sensitive hashing (LSH), which partitions the semantic space of\nsentences. The algorithm encodes and LSH-hashes a candidate sentence generated\nby an LLM, and conducts sentence-level rejection sampling until the sampled\nsentence falls in watermarked partitions in the semantic embedding space. A\nmargin-based constraint is used to enhance its robustness. To show the\nadvantages of our algorithm, we propose a \"bigram\" paraphrase attack using the\nparaphrase that has the fewest bigram overlaps with the original sentence. This\nattack is shown to be effective against the existing token-level watermarking\nmethod. Experimental results show that our novel semantic watermark algorithm\nis not only more robust than the previous state-of-the-art method on both\ncommon and bigram paraphrase attacks, but also is better at preserving the\nquality of generation.",
    "authors": [
      "Abe Bohan Hou",
      "Jingyu Zhang",
      "Tianxing He",
      "Yichen Wang",
      "Yung-Sung Chuang",
      "Hongwei Wang",
      "Lingfeng Shen",
      "Benjamin Van Durme",
      "Daniel Khashabi",
      "Yulia Tsvetkov"
    ],
    "arxiv_id": "2310.03991",
    "url": "https://arxiv.org/abs/2310.03991"
  },
  {
    "title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models",
    "abstract": "We show the viability of tackling misuses of large language models beyond the\nidentification of machine-generated text. While existing zero-bit watermark\nmethods focus on detection only, some malicious misuses demand tracing the\nadversary user for counteracting them. To address this, we propose Multi-bit\nWatermark via Position Allocation, embedding traceable multi-bit information\nduring language model generation. Through allocating tokens onto different\nparts of the messages, we embed longer messages in high corruption settings\nwithout added latency. By independently embedding sub-units of messages, the\nproposed method outperforms the existing works in terms of robustness and\nlatency. Leveraging the benefits of zero-bit watermarking, our method enables\nrobust extraction of the watermark without any model access, embedding and\nextraction of long messages ($\\geq$ 32-bit) without finetuning, and maintaining\ntext quality, while allowing zero-bit detection all at the same time. Code is\nreleased here: https://github.com/bangawayoo/mb-lm-watermarking",
    "authors": [
      "KiYoon Yoo",
      "Wonhyuk Ahn",
      "Nojun Kwak"
    ],
    "arxiv_id": "2308.00221",
    "url": "https://arxiv.org/abs/2308.00221"
  },
  {
    "title": "Three Bricks to Consolidate Watermarks for Large Language Models",
    "abstract": "The task of discerning between generated and natural texts is increasingly\nchallenging. In this context, watermarking emerges as a promising technique for\nascribing generated text to a specific model. It alters the sampling generation\nprocess so as to leave an invisible trace in the generated output, facilitating\nlater detection. This research consolidates watermarks for large language\nmodels based on three theoretical and empirical considerations. First, we\nintroduce new statistical tests that offer robust theoretical guarantees which\nremain valid even at low false-positive rates (less than 10$^{\\text{-6}}$).\nSecond, we compare the effectiveness of watermarks using classical benchmarks\nin the field of natural language processing, gaining insights into their\nreal-world applicability. Third, we develop advanced detection schemes for\nscenarios where access to the LLM is available, as well as multi-bit\nwatermarking.",
    "authors": [
      "Pierre Fernandez",
      "Antoine Chaffin",
      "Karim Tit",
      "Vivien Chappelier",
      "Teddy Furon"
    ],
    "arxiv_id": "2308.00113",
    "url": "https://arxiv.org/abs/2308.00113"
  },
  {
    "title": "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs",
    "abstract": "As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden\npatterns. However, we argue that existing LLM watermarking methods are\nencoding-inefficient and cannot flexibly meet the diverse information encoding\nneeds (such as encoding model version, generation time, user id, etc.). In this\nwork, we conduct the first systematic study on the topic of Codable Text\nWatermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\ncustomizable information. First of all, we study the taxonomy of LLM\nwatermarking technologies and give a mathematical formulation for CTWL.\nAdditionally, we provide a comprehensive evaluation system for CTWL: (1)\nwatermarking success rate, (2) robustness against various corruptions, (3)\ncoding rate of payload information, (4) encoding and decoding efficiency, (5)\nimpacts on the quality of the generated text. To meet the requirements of these\nnon-Pareto-improving metrics, we follow the most prominent vocabulary\npartition-based watermarking direction, and devise an advanced CTWL method\nnamed Balance-Marking. The core idea of our method is to use a proxy language\nmodel to split the vocabulary into probability-balanced parts, thereby\neffectively maintaining the quality of the watermarked text. Our code is\navailable at https://github.com/lancopku/codable-watermarking-for-llm.",
    "authors": [
      "Lean Wang",
      "Wenkai Yang",
      "Deli Chen",
      "Hao Zhou",
      "Yankai Lin",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ],
    "arxiv_id": "2307.15992",
    "url": "https://arxiv.org/abs/2307.15992"
  },
  {
    "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
    "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have\nbeen proposed to mitigate the potential harms of text generated by LLMs,\nincluding fake news and copyright issues. However, current watermark detection\nalgorithms require the secret key used in the watermark generation process,\nmaking them susceptible to security breaches and counterfeiting during public\ndetection. To address this limitation, we propose an unforgeable publicly\nverifiable watermark algorithm named UPV that uses two different neural\nnetworks for watermark generation and detection, instead of using the same key\nat both stages. Meanwhile, the token embedding parameters are shared between\nthe generation and detection networks, which makes the detection network\nachieve a high accuracy very efficiently. Experiments demonstrate that our\nalgorithm attains high detection accuracy and computational efficiency through\nneural networks. Subsequent analysis confirms the high complexity involved in\nforging the watermark from the detection network. Our code is available at\n\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}.\nAdditionally, our algorithm could also be accessed through MarkLLM\n\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.",
    "authors": [
      "Aiwei Liu",
      "Leyi Pan",
      "Xuming Hu",
      "Shu'ang Li",
      "Lijie Wen",
      "Irwin King",
      "Philip S. Yu"
    ],
    "arxiv_id": "2307.16230",
    "url": "https://arxiv.org/abs/2307.16230"
  },
  {
    "title": "Robust Distortion-free Watermarks for Language Models",
    "abstract": "We propose a methodology for planting watermarks in text from an\nautoregressive language model that are robust to perturbations without changing\nthe distribution over text up to a certain maximum generation budget. We\ngenerate watermarked text by mapping a sequence of random numbers -- which we\ncompute using a randomized watermark key -- to a sample from the language\nmodel. To detect watermarked text, any party who knows the key can align the\ntext to the random number sequence. We instantiate our watermark methodology\nwith two sampling schemes: inverse transform sampling and exponential minimum\nsampling. We apply these watermarks to three language models -- OPT-1.3B,\nLLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B\nand LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq\n0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens\nvia random edits (i.e., substitutions, insertions or deletions). For the\nAlpaca-7B model, we conduct a case study on the feasibility of watermarking\nresponses to typical user instructions. Due to the lower entropy of the\nresponses, detection is more difficult: around $25\\%$ of the responses -- whose\nmedian length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and\nthe watermark is also less robust to certain automated paraphrasing attacks we\nimplement.",
    "authors": [
      "Rohith Kuditipudi",
      "John Thickstun",
      "Tatsunori Hashimoto",
      "Percy Liang"
    ],
    "arxiv_id": "2307.15593",
    "url": "https://arxiv.org/abs/2307.15593"
  },
  {
    "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
    "abstract": "To mitigate potential risks associated with language models, recent AI\ndetection research proposes incorporating watermarks into machine-generated\ntext through random vocabulary restrictions and utilizing this information for\ndetection. While these watermarks only induce a slight deterioration in\nperplexity, our empirical investigation reveals a significant detriment to the\nperformance of conditional text generation. To address this issue, we introduce\na simple yet effective semantic-aware watermarking algorithm that considers the\ncharacteristics of conditional text generation and the input context.\nExperimental results demonstrate that our proposed method yields substantial\nimprovements across various text generation models, including BART and Flan-T5,\nin tasks such as summarization and data-to-text generation while maintaining\ndetection ability.",
    "authors": [
      "Yu Fu",
      "Deyi Xiong",
      "Yue Dong"
    ],
    "arxiv_id": "2307.13808",
    "url": "https://arxiv.org/abs/2307.13808"
  },
  {
    "title": "Provable Robust Watermarking for AI-Generated Text",
    "abstract": "We study the problem of watermarking large language models (LLMs) generated\ntext -- one of the most promising approaches for addressing the safety\nchallenges of LLM usage. In this paper, we propose a rigorous theoretical\nframework to quantify the effectiveness and robustness of LLM watermarks. We\npropose a robust and high-quality watermark method, Unigram-Watermark, by\nextending an existing approach with a simplified fixed grouping strategy. We\nprove that our watermark method enjoys guaranteed generation quality,\ncorrectness in watermark detection, and is robust against text editing and\nparaphrasing. Experiments on three varying LLMs and two datasets verify that\nour Unigram-Watermark achieves superior detection accuracy and comparable\ngeneration quality in perplexity, thus promoting the responsible use of LLMs.\nCode is available at https://github.com/XuandongZhao/Unigram-Watermark.",
    "authors": [
      "Xuandong Zhao",
      "Prabhanjan Ananth",
      "Lei Li",
      "Yu-Xiang Wang"
    ],
    "arxiv_id": "2306.17439",
    "url": "https://arxiv.org/abs/2306.17439"
  },
  {
    "title": "On the Reliability of Watermarks for Large Language Models",
    "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking\nis a simple and effective strategy for mitigating such harms by enabling the\ndetection and documentation of LLM-generated text. Yet a crucial question\nremains: How reliable is watermarking in realistic settings in the wild? There,\nwatermarked text may be modified to suit a user's needs, or entirely rewritten\nto avoid detection. We study the robustness of watermarked text after it is\nre-written by humans, paraphrased by a non-watermarked LLM, or mixed into a\nlonger hand-written document. We find that watermarks remain detectable even\nafter human and machine paraphrasing. While these attacks dilute the strength\nof the watermark, paraphrases are statistically likely to leak n-grams or even\nlonger fragments of the original text, resulting in high-confidence detections\nwhen enough tokens are observed. For example, after strong human paraphrasing\nthe watermark is detectable after observing 800 tokens on average, when setting\na 1e-5 false positive rate. We also consider a range of new detection schemes\nthat are sensitive to short spans of watermarked text embedded inside a large\ndocument, and we compare the robustness of watermarking to other kinds of\ndetectors.",
    "authors": [
      "John Kirchenbauer",
      "Jonas Geiping",
      "Yuxin Wen",
      "Manli Shu",
      "Khalid Saifullah",
      "Kezhi Kong",
      "Kasun Fernando",
      "Aniruddha Saha",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "arxiv_id": "2306.04634",
    "url": "https://arxiv.org/abs/2306.04634"
  },
  {
    "title": "Undetectable Watermarks for Language Models",
    "abstract": "Recent advances in the capabilities of large language models such as GPT-4\nhave spurred increasing concern about our ability to detect AI-generated text.\nPrior works have suggested methods of embedding watermarks in model outputs, by\nnoticeably altering the output distribution. We ask: Is it possible to\nintroduce a watermark without incurring any detectable change to the output\ndistribution?\n  To this end we introduce a cryptographically-inspired notion of undetectable\nwatermarks for language models. That is, watermarks can be detected only with\nthe knowledge of a secret key; without the secret key, it is computationally\nintractable to distinguish watermarked outputs from those of the original\nmodel. In particular, it is impossible for a user to observe any degradation in\nthe quality of the text. Crucially, watermarks should remain undetectable even\nwhen the user is allowed to adaptively query the model with arbitrarily chosen\nprompts. We construct undetectable watermarks based on the existence of one-way\nfunctions, a standard assumption in cryptography.",
    "authors": [
      "Miranda Christ",
      "Sam Gunn",
      "Or Zamir"
    ],
    "arxiv_id": "2306.09194",
    "url": "https://arxiv.org/abs/2306.09194"
  },
  {
    "title": "Watermarking Text Data on Large Language Models for Dataset Copyright",
    "abstract": "Substantial research works have shown that deep models, e.g., pre-trained\nmodels, on the large corpus can learn universal language representations, which\nare beneficial for downstream NLP tasks. However, these powerful models are\nalso vulnerable to various privacy attacks, while much sensitive information\nexists in the training dataset. The attacker can easily steal sensitive\ninformation from public models, e.g., individuals' email addresses and phone\nnumbers. In an attempt to address these issues, particularly the unauthorized\nuse of private data, we introduce a novel watermarking technique via a\nbackdoor-based membership inference approach named TextMarker, which can\nsafeguard diverse forms of private information embedded in the training text\ndata. Specifically, TextMarker only requires data owners to mark a small number\nof samples for data copyright protection under the black-box access assumption\nto the target model. Through extensive evaluation, we demonstrate the\neffectiveness of TextMarker on various real-world datasets, e.g., marking only\n0.1% of the training dataset is practically sufficient for effective membership\ninference with negligible effect on model utility. We also discuss potential\ncountermeasures and show that TextMarker is stealthy enough to bypass them.",
    "authors": [
      "Yixin Liu",
      "Hongsheng Hu",
      "Xun Chen",
      "Xuyun Zhang",
      "Lichao Sun"
    ],
    "arxiv_id": "2305.13257",
    "url": "https://arxiv.org/abs/2305.13257"
  },
  {
    "title": "Baselines for Identifying Watermarked Large Language Models",
    "abstract": "We consider the emerging problem of identifying the presence and use of\nwatermarking schemes in widely used, publicly hosted, closed source large\nlanguage models (LLMs). We introduce a suite of baseline algorithms for\nidentifying watermarks in LLMs that rely on analyzing distributions of output\ntokens and logits generated by watermarked and unmarked LLMs. Notably,\nwatermarked LLMs tend to produce distributions that diverge qualitatively and\nidentifiably from standard models. Furthermore, we investigate the\nidentifiability of watermarks at varying strengths and consider the tradeoffs\nof each of our identification mechanisms with respect to watermarking scenario.\nAlong the way, we formalize the specific problem of identifying watermarks in\nLLMs, as well as LLM watermarks and watermark detection in general, providing a\nframework and foundations for studying them.",
    "authors": [
      "Leonard Tang",
      "Gavin Uberti",
      "Tom Shlomi"
    ],
    "arxiv_id": "2305.18456",
    "url": "https://arxiv.org/abs/2305.18456"
  },
  {
    "title": "Who Wrote this Code? Watermarking for Code Generation",
    "abstract": "Since the remarkable generation performance of large language models raised\nethical and legal concerns, approaches to detect machine-generated text by\nembedding watermarks are being developed. However, we discover that the\nexisting works fail to function appropriately in code generation tasks due to\nthe task's nature of having low entropy. Extending a logit-modifying watermark\nmethod, we propose Selective WatErmarking via Entropy Thresholding (SWEET),\nwhich enhances detection ability and mitigates code quality degeneration by\nremoving low-entropy segments at generating and detecting watermarks. Our\nexperiments show that SWEET significantly improves code quality preservation\nwhile outperforming all baselines, including post-hoc detection methods, in\ndetecting machine-generated code text. Our code is available in\nhttps://github.com/hongcheki/sweet-watermark.",
    "authors": [
      "Taehyun Lee",
      "Seokhee Hong",
      "Jaewoo Ahn",
      "Ilgee Hong",
      "Hwaran Lee",
      "Sangdoo Yun",
      "Jamin Shin",
      "Gunhee Kim"
    ],
    "arxiv_id": "2305.15060",
    "url": "https://arxiv.org/abs/2305.15060"
  },
  {
    "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
    "abstract": "Recent years have witnessed a proliferation of valuable original natural\nlanguage contents found in subscription-based media outlets, web novel\nplatforms, and outputs of large language models. However, these contents are\nsusceptible to illegal piracy and potential misuse without proper security\nmeasures. This calls for a secure watermarking system to guarantee copyright\nprotection through leakage tracing or ownership identification. To effectively\ncombat piracy and protect copyrights, a multi-bit watermarking framework should\nbe able to embed adequate bits of information and extract the watermarks in a\nrobust manner despite possible corruption. In this work, we explore ways to\nadvance both payload and robustness by following a well-known proposition from\nimage watermarking and identify features in natural language that are invariant\nto minor corruption. Through a systematic analysis of the possible sources of\nerrors, we further propose a corruption-resistant infill model. Our full method\nimproves upon the previous work on robustness by +16.8% point on average on\nfour datasets, three corruption types, and two corruption ratios. Code\navailable at https://github.com/bangawayoo/nlp-watermarking.",
    "authors": [
      "KiYoon Yoo",
      "Wonhyuk Ahn",
      "Jiho Jang",
      "Nojun Kwak"
    ],
    "arxiv_id": "2305.01904",
    "url": "https://arxiv.org/abs/2305.01904"
  },
  {
    "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
    "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.",
    "authors": [
      "Wenjun Peng",
      "Jingwei Yi",
      "Fangzhao Wu",
      "Shangxi Wu",
      "Bin Zhu",
      "Lingjuan Lyu",
      "Binxing Jiao",
      "Tong Xu",
      "Guangzhong Sun",
      "Xing Xie"
    ],
    "arxiv_id": "2305.10036",
    "url": "https://arxiv.org/abs/2305.10036"
  },
  {
    "title": "Watermarking Text Generated by Black-Box Language Models",
    "abstract": "LLMs now exhibit human-like skills in various fields, leading to worries\nabout misuse. Thus, detecting generated text is crucial. However, passive\ndetection methods are stuck in domain specificity and limited adversarial\nrobustness. To achieve reliable detection, a watermark-based method was\nproposed for white-box LLMs, allowing them to embed watermarks during text\ngeneration. The method involves randomly dividing the model vocabulary to\nobtain a special list and adjusting the probability distribution to promote the\nselection of words in the list. A detection algorithm aware of the list can\nidentify the watermarked text. However, this method is not applicable in many\nreal-world scenarios where only black-box language models are available. For\ninstance, third-parties that develop API-based vertical applications cannot\nwatermark text themselves because API providers only supply generated text and\nwithhold probability distributions to shield their commercial interests. To\nallow third-parties to autonomously inject watermarks into generated text, we\ndevelop a watermarking framework for black-box language model usage scenarios.\nSpecifically, we first define a binary encoding function to compute a random\nbinary encoding corresponding to a word. The encodings computed for\nnon-watermarked text conform to a Bernoulli distribution, wherein the\nprobability of a word representing bit-1 being approximately 0.5. To inject a\nwatermark, we alter the distribution by selectively replacing words\nrepresenting bit-0 with context-based synonyms that represent bit-1. A\nstatistical test is then used to identify the watermark. Experiments\ndemonstrate the effectiveness of our method on both Chinese and English\ndatasets. Furthermore, results under re-translation, polishing, word deletion,\nand synonym substitution attacks reveal that it is arduous to remove the\nwatermark without compromising the original semantics.",
    "authors": [
      "Xi Yang",
      "Kejiang Chen",
      "Weiming Zhang",
      "Chang Liu",
      "Yuang Qi",
      "Jie Zhang",
      "Han Fang",
      "Nenghai Yu"
    ],
    "arxiv_id": "2305.08883",
    "url": "https://arxiv.org/abs/2305.08883"
  },
  {
    "title": "Protecting Language Generation Models via Invisible Watermarking",
    "abstract": "Language generation models have been an increasingly powerful enabler for\nmany applications. Many such models offer free or affordable API access, which\nmakes them potentially vulnerable to model extraction attacks through\ndistillation. To protect intellectual property (IP) and ensure fair use of\nthese models, various techniques such as lexical watermarking and synonym\nreplacement have been proposed. However, these methods can be nullified by\nobvious countermeasures such as \"synonym randomization\". To address this issue,\nwe propose GINSEW, a novel method to protect text generation models from being\nstolen through distillation. The key idea of our method is to inject secret\nsignals into the probability vector of the decoding steps for each target\ntoken. We can then detect the secret message by probing a suspect model to tell\nif it is distilled from the protected one. Experimental results show that\nGINSEW can effectively identify instances of IP infringement with minimal\nimpact on the generation quality of protected APIs. Our method demonstrates an\nabsolute improvement of 19 to 29 points on mean average precision (mAP) in\ndetecting suspects compared to previous methods against watermark removal\nattacks.",
    "authors": [
      "Xuandong Zhao",
      "Yu-Xiang Wang",
      "Lei Li"
    ],
    "arxiv_id": "2302.03162",
    "url": "https://arxiv.org/abs/2302.03162"
  },
  {
    "title": "A Watermark for Large Language Models",
    "abstract": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.",
    "authors": [
      "John Kirchenbauer",
      "Jonas Geiping",
      "Yuxin Wen",
      "Jonathan Katz",
      "Ian Miers",
      "Tom Goldstein"
    ],
    "arxiv_id": "2301.10226",
    "url": "https://arxiv.org/abs/2301.10226"
  },
  {
    "title": "Distillation-Resistant Watermarking for Model Protection in NLP",
    "abstract": "How can we protect the intellectual property of trained NLP models? Modern\nNLP models are prone to stealing by querying and distilling from their publicly\nexposed APIs. However, existing protection methods such as watermarking only\nwork for images but are not applicable to text. We propose\nDistillation-Resistant Watermarking (DRW), a novel technique to protect NLP\nmodels from being stolen via distillation. DRW protects a model by injecting\nwatermarks into the victim's prediction probability corresponding to a secret\nkey and is able to detect such a key by probing a suspect model. We prove that\na protected model still retains the original accuracy within a certain bound.\nWe evaluate DRW on a diverse set of NLP tasks including text classification,\npart-of-speech tagging, and named entity recognition. Experiments show that DRW\nprotects the original model and detects stealing suspects at 100% mean average\nprecision for all four tasks while the prior method fails on two.",
    "authors": [
      "Xuandong Zhao",
      "Lei Li",
      "Yu-Xiang Wang"
    ],
    "arxiv_id": "2210.03312",
    "url": "https://arxiv.org/abs/2210.03312"
  },
  {
    "title": "CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks",
    "abstract": "Previous works have validated that text generation APIs can be stolen through\nimitation attacks, causing IP violations. In order to protect the IP of text\ngeneration APIs, a recent work has introduced a watermarking algorithm and\nutilized the null-hypothesis test as a post-hoc ownership verification on the\nimitation models. However, we find that it is possible to detect those\nwatermarks via sufficient statistics of the frequencies of candidate\nwatermarking words. To address this drawback, in this paper, we propose a novel\nConditional wATERmarking framework (CATER) for protecting the IP of text\ngeneration APIs. An optimization method is proposed to decide the watermarking\nrules that can minimize the distortion of overall word distributions while\nmaximizing the change of conditional word selections. Theoretically, we prove\nthat it is infeasible for even the savviest attacker (they know how CATER\nworks) to reveal the used watermarks from a large pool of potential word pairs\nbased on statistical inspection. Empirically, we observe that high-order\nconditions lead to an exponential growth of suspicious (unused) watermarks,\nmaking our crafted watermarks more stealthy. In addition, \\cater can\neffectively identify the IP infringement under architectural mismatch and\ncross-domain imitation attacks, with negligible impairments on the generation\nquality of victim APIs. We envision our work as a milestone for stealthily\nprotecting the IP of text generation APIs.",
    "authors": [
      "Xuanli He",
      "Qiongkai Xu",
      "Yi Zeng",
      "Lingjuan Lyu",
      "Fangzhao Wu",
      "Jiwei Li",
      "Ruoxi Jia"
    ],
    "arxiv_id": "2209.08773",
    "url": "https://arxiv.org/abs/2209.08773"
  },
  {
    "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
    "abstract": "Recent advances in natural language generation have introduced powerful\nlanguage models with high-quality output text. However, this raises concerns\nabout the potential misuse of such models for malicious purposes. In this\npaper, we study natural language watermarking as a defense to help better mark\nand trace the provenance of text. We introduce the Adversarial Watermarking\nTransformer (AWT) with a jointly trained encoder-decoder and adversarial\ntraining that, given an input text and a binary message, generates an output\ntext that is unobtrusively encoded with the given message. We further study\ndifferent training and inference strategies to achieve minimal changes to the\nsemantics and correctness of the input text.\n  AWT is the first end-to-end model to hide data in text by automatically\nlearning -- without ground truth -- word substitutions along with their\nlocations in order to encode the message. We empirically show that our model is\neffective in largely preserving text utility and decoding the watermark while\nhiding its presence against adversaries. Additionally, we demonstrate that our\nmethod is robust against a range of attacks.",
    "authors": [
      "Sahar Abdelnabi",
      "Mario Fritz"
    ],
    "arxiv_id": "2009.03015",
    "url": "https://arxiv.org/abs/2009.03015"
  },
  {
    "title": "Leveraging Optimization for Adaptive Attacks on Image Watermarks",
    "abstract": "Untrustworthy users can misuse image generators to synthesize high-quality\ndeepfakes and engage in unethical activities. Watermarking deters misuse by\nmarking generated content with a hidden message, enabling its detection using a\nsecret watermarking key. A core security property of watermarking is\nrobustness, which states that an attacker can only evade detection by\nsubstantially degrading image quality. Assessing robustness requires designing\nan adaptive attack for the specific watermarking algorithm. When evaluating\nwatermarking algorithms and their (adaptive) attacks, it is challenging to\ndetermine whether an adaptive attack is optimal, i.e., the best possible\nattack. We solve this problem by defining an objective function and then\napproach adaptive attacks as an optimization problem. The core idea of our\nadaptive attacks is to replicate secret watermarking keys locally by creating\nsurrogate keys that are differentiable and can be used to optimize the attack's\nparameters. We demonstrate for Stable Diffusion models that such an attacker\ncan break all five surveyed watermarking methods at no visible degradation in\nimage quality. Optimizing our attacks is efficient and requires less than 1 GPU\nhour to reduce the detection accuracy to 6.3% or less. Our findings emphasize\nthe need for more rigorous robustness testing against adaptive, learnable\nattackers.",
    "authors": [
      "Nils Lukas",
      "Abdulrahman Diaa",
      "Lucas Fenaux",
      "Florian Kerschbaum"
    ],
    "arxiv_id": "2309.16952",
    "url": "https://arxiv.org/abs/2309.16952"
  },
  {
    "title": "Catch You Everything Everywhere: Guarding Textual Inversion via Concept Watermarking",
    "abstract": "AIGC (AI-Generated Content) has achieved tremendous success in many\napplications such as text-to-image tasks, where the model can generate\nhigh-quality images with diverse prompts, namely, different descriptions in\nnatural languages. More surprisingly, the emerging personalization techniques\neven succeed in describing unseen concepts with only a few personal images as\nreferences, and there have been some commercial platforms for sharing the\nvaluable personalized concept. However, such an advanced technique also\nintroduces a severe threat, where malicious users can misuse the target concept\nto generate highly-realistic illegal images. Therefore, it becomes necessary\nfor the platform to trace malicious users and hold them accountable.\n  In this paper, we focus on guarding the most popular lightweight\npersonalization model, ie, Textual Inversion (TI). To achieve it, we propose\nthe novel concept watermarking, where watermark information is embedded into\nthe target concept and then extracted from generated images based on the\nwatermarked concept. Specifically, we jointly train a watermark encoder and a\nwatermark decoder with the sampler in the loop.\n  It shows great resilience to different diffusion sampling processes possibly\nchosen by malicious users, meanwhile preserving utility for normal use. In\npractice, the concept owner can upload his concept with different watermarks\n(ie, serial numbers) to the platform, and the platform allocates different\nusers with different serial numbers for subsequent tracing and forensics.",
    "authors": [
      "Weitao Feng",
      "Jiyan He",
      "Jie Zhang",
      "Tianwei Zhang",
      "Wenbo Zhou",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "arxiv_id": "2309.05940",
    "url": "https://arxiv.org/abs/2309.05940"
  },
  {
    "title": "Hey That's Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs",
    "abstract": "Generative models have seen an explosion in popularity with the release of\nhuge generative Diffusion models like Midjourney and Stable Diffusion to the\npublic. Because of this new ease of access, questions surrounding the automated\ncollection of data and issues regarding content ownership have started to\nbuild. In this paper we present new work which aims to provide ways of\nprotecting content when shared to the public. We show that a generative\nDiffusion model trained on data that has been imperceptibly watermarked will\ngenerate new images with these watermarks present. We further show that if a\ngiven watermark is correlated with a certain feature of the training data, the\ngenerated images will also have this correlation. Using statistical tests we\nshow that we are able to determine whether a model has been trained on marked\ndata, and what data was marked. As a result our system offers a solution to\nprotect intellectual property when sharing content online.",
    "authors": [
      "Luke Ditria",
      "Tom Drummond"
    ],
    "arxiv_id": "2308.11123",
    "url": "https://arxiv.org/abs/2308.11123"
  },
  {
    "title": "Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis",
    "abstract": "Large text-to-image models have shown remarkable performance in synthesizing\nhigh-quality images. In particular, the subject-driven model makes it possible\nto personalize the image synthesis for a specific subject, e.g., a human face\nor an artistic style, by fine-tuning the generic text-to-image model with a few\nimages from that subject. Nevertheless, misuse of subject-driven image\nsynthesis may violate the authority of subject owners. For example, malicious\nusers may use subject-driven synthesis to mimic specific artistic styles or to\ncreate fake facial images without authorization. To protect subject owners\nagainst such misuse, recent attempts have commonly relied on adversarial\nexamples to indiscriminately disrupt subject-driven image synthesis. However,\nthis essentially prevents any benign use of subject-driven synthesis based on\nprotected images.\n  In this paper, we take a different angle and aim at protection without\nsacrificing the utility of protected images for general synthesis purposes.\nSpecifically, we propose GenWatermark, a novel watermark system based on\njointly learning a watermark generator and a detector. In particular, to help\nthe watermark survive the subject-driven synthesis, we incorporate the\nsynthesis process in learning GenWatermark by fine-tuning the detector with\nsynthesized images for a specific subject. This operation is shown to largely\nimprove the watermark detection accuracy and also ensure the uniqueness of the\nwatermark for each individual subject. Extensive experiments validate the\neffectiveness of GenWatermark, especially in practical scenarios with unknown\nmodels and text prompts (74% Acc.), as well as partial data watermarking (80%\nAcc. for 1/4 watermarking). We also demonstrate the robustness of GenWatermark\nto two potential countermeasures that substantially degrade the synthesis\nquality.",
    "authors": [
      "Yihan Ma",
      "Zhengyu Zhao",
      "Xinlei He",
      "Zheng Li",
      "Michael Backes",
      "Yang Zhang"
    ],
    "arxiv_id": "2306.07754",
    "url": "https://arxiv.org/abs/2306.07754"
  },
  {
    "title": "Invisible Image Watermarks Are Provably Removable Using Generative AI",
    "abstract": "Invisible watermarks safeguard images' copyright by embedding hidden messages\nonly detectable by owners. They also prevent people from misusing images,\nespecially those generated by AI models. We propose a family of regeneration\nattacks to remove these invisible watermarks. The proposed attack method first\nadds random noise to an image to destroy the watermark and then reconstructs\nthe image. This approach is flexible and can be instantiated with many existing\nimage-denoising algorithms and pre-trained generative models such as diffusion\nmodels. Through formal proofs and empirical results, we show that all invisible\nwatermarks are vulnerable to the proposed attack. For a particularly resilient\nwatermark, RivaGAN, regeneration attacks remove 93-99% of the invisible\nwatermarks while the baseline attacks remove no more than 3%. However, if we do\nnot require the watermarked image to look the same as the original one,\nwatermarks that keep the image semantically similar can be an alternative\ndefense against our attack. Our finding underscores the need for a shift in\nresearch/industry emphasis from invisible watermarks to semantically similar\nones. Code is available at https://github.com/XuandongZhao/WatermarkAttacker.",
    "authors": [
      "Xuandong Zhao",
      "Kexun Zhang",
      "Zihao Su",
      "Saastha Vasan",
      "Ilya Grishchenko",
      "Christopher Kruegel",
      "Giovanni Vigna",
      "Yu-Xiang Wang",
      "Lei Li"
    ],
    "arxiv_id": "2306.01953",
    "url": "https://arxiv.org/abs/2306.01953"
  },
  {
    "title": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust",
    "abstract": "Watermarking the outputs of generative models is a crucial technique for\ntracing copyright and preventing potential harm from AI-generated content. In\nthis paper, we introduce a novel technique called Tree-Ring Watermarking that\nrobustly fingerprints diffusion model outputs. Unlike existing methods that\nperform post-hoc modifications to images after sampling, Tree-Ring Watermarking\nsubtly influences the entire sampling process, resulting in a model fingerprint\nthat is invisible to humans. The watermark embeds a pattern into the initial\nnoise vector used for sampling. These patterns are structured in Fourier space\nso that they are invariant to convolutions, crops, dilations, flips, and\nrotations. After image generation, the watermark signal is detected by\ninverting the diffusion process to retrieve the noise vector, which is then\nchecked for the embedded signal. We demonstrate that this technique can be\neasily applied to arbitrary diffusion models, including text-conditioned Stable\nDiffusion, as a plug-in with negligible loss in FID. Our watermark is\nsemantically hidden in the image space and is far more robust than watermarking\nalternatives that are currently deployed. Code is available at\nhttps://github.com/YuxinWenRick/tree-ring-watermark.",
    "authors": [
      "Yuxin Wen",
      "John Kirchenbauer",
      "Jonas Geiping",
      "Tom Goldstein"
    ],
    "arxiv_id": "2305.20030",
    "url": "https://arxiv.org/abs/2305.20030"
  },
  {
    "title": "Evading Watermark based Detection of AI-Generated Content",
    "abstract": "A generative AI model can generate extremely realistic-looking content,\nposing growing challenges to the authenticity of information. To address the\nchallenges, watermark has been leveraged to detect AI-generated content.\nSpecifically, a watermark is embedded into an AI-generated content before it is\nreleased. A content is detected as AI-generated if a similar watermark can be\ndecoded from it. In this work, we perform a systematic study on the robustness\nof such watermark-based AI-generated content detection. We focus on\nAI-generated images. Our work shows that an attacker can post-process a\nwatermarked image via adding a small, human-imperceptible perturbation to it,\nsuch that the post-processed image evades detection while maintaining its\nvisual quality. We show the effectiveness of our attack both theoretically and\nempirically. Moreover, to evade detection, our adversarial post-processing\nmethod adds much smaller perturbations to AI-generated images and thus better\nmaintain their visual quality than existing popular post-processing methods\nsuch as JPEG compression, Gaussian blur, and Brightness/Contrast. Our work\nshows the insufficiency of existing watermark-based detection of AI-generated\ncontent, highlighting the urgent needs of new methods. Our code is publicly\navailable: https://github.com/zhengyuan-jiang/WEvade.",
    "authors": [
      "Zhengyuan Jiang",
      "Jinghuai Zhang",
      "Neil Zhenqiang Gong"
    ],
    "arxiv_id": "2305.03807",
    "url": "https://arxiv.org/abs/2305.03807"
  },
  {
    "title": "The Stable Signature: Rooting Watermarks in Latent Diffusion Models",
    "abstract": "Generative image modeling enables a wide range of applications but raises\nethical concerns about responsible deployment. This paper introduces an active\nstrategy combining image watermarking and Latent Diffusion Models. The goal is\nfor all generated images to conceal an invisible watermark allowing for future\ndetection and/or identification. The method quickly fine-tunes the latent\ndecoder of the image generator, conditioned on a binary signature. A\npre-trained watermark extractor recovers the hidden signature from any\ngenerated image and a statistical test then determines whether it comes from\nthe generative model. We evaluate the invisibility and robustness of the\nwatermarks on a variety of generation tasks, showing that Stable Signature\nworks even after the images are modified. For instance, it detects the origin\nof an image generated from a text prompt, then cropped to keep $10\\%$ of the\ncontent, with $90$+$\\%$ accuracy at a false positive rate below 10$^{-6}$.",
    "authors": [
      "Pierre Fernandez",
      "Guillaume Couairon",
      "Hervé Jégou",
      "Matthijs Douze",
      "Teddy Furon"
    ],
    "arxiv_id": "2303.15435",
    "url": "https://arxiv.org/abs/2303.15435"
  },
  {
    "title": "Watermarking Images in Self-Supervised Latent Spaces",
    "abstract": "We revisit watermarking techniques based on pre-trained deep networks, in the\nlight of self-supervised approaches. We present a way to embed both marks and\nbinary messages into their latent spaces, leveraging data augmentation at\nmarking time. Our method can operate at any resolution and creates watermarks\nrobust to a broad range of transformations (rotations, crops, JPEG, contrast,\netc). It significantly outperforms the previous zero-bit methods, and its\nperformance on multi-bit watermarking is on par with state-of-the-art\nencoder-decoder architectures trained end-to-end for watermarking. The code is\navailable at github.com/facebookresearch/ssl_watermarking",
    "authors": [
      "Pierre Fernandez",
      "Alexandre Sablayrolles",
      "Teddy Furon",
      "Hervé Jégou",
      "Matthijs Douze"
    ],
    "arxiv_id": "2112.09581",
    "url": "https://arxiv.org/abs/2112.09581"
  }
]